#!/bin/bash

# Load functions:
source /usr/local/bin/vm-functions

###############################################################################
# Specific procedures:
###############################################################################

#------------------------------------------------------------------------------
# Attempts to stop the container gracefully in case of receive SIGTERM signal from Docker:
#------------------------------------------------------------------------------
stop_container()
{
    echo "############################################################################################################"
    echo "SIGTERM signal received at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))"

    # TODO: Terminate or kill background processes before to exit...

    echo "Container Stopped."
    echo ""
    exit 0
}

#------------------------------------------------------------------------------
# Checks environment variables and return success or fail if everything is in order:
#------------------------------------------------------------------------------
check_variables()
{
    # Starts check assuming failed statuses:
    local variables_exit_code
    local local_backup_path_ok
    local max_backups_per_chain_ok
    local rsync_backup_path_exists
    local rsync_backup_path_ok

    echo "____________________________________________________________________________________________________________"
    echo -e "Checking environment variables...\n"

    # Check MAX_BACKUPS_PER_CHAIN
    #------------------------------------------------------------------------------
    if [[ $MAX_BACKUPS_PER_CHAIN =~ [0-9] && $MAX_BACKUPS_PER_CHAIN -ge 0 ]]; then

        # Is an integer number:
        max_backups_per_chain_ok=true
        echo "MAX_BACKUPS_PER_CHAIN set to: $MAX_BACKUPS_PER_CHAIN"

    else
        # Invalid value:
        echo "ERROR: Incorrect value for MAX_BACKUPS_PER_CHAIN: '$MAX_BACKUPS_PER_CHAIN' <- It must be a natural integer >= 0"
    fi

    # Check LOCAL_BACKUP_PATH:
    #------------------------------------------------------------------------------
    if  [[ -d $LOCAL_BACKUP_PATH ]]; then

        # LOCAL_BACKUP_PATH exists. Check for filesystem permissions:
        if  [[ -r $LOCAL_BACKUP_PATH && -w $LOCAL_BACKUP_PATH ]]; then

            # LOCAL_BACKUP_PATH has read/write permissions.
            echo "LOCAL_BACKUP_PATH set to: $LOCAL_BACKUP_PATH"

            # Check LOCAL_BACKUP_CHAINS_TO_KEEP:
            if [[ $LOCAL_BACKUP_CHAINS_TO_KEEP =~ [0-9] && $LOCAL_BACKUP_CHAINS_TO_KEEP -ge 0 ]]; then

                # Check passed:
                local_backup_path_ok=true
                echo "LOCAL_BACKUP_CHAINS_TO_KEEP set to: $LOCAL_BACKUP_CHAINS_TO_KEEP"

            else
                # Invalid value:
                echo "ERROR: Incorrect value for LOCAL_BACKUP_CHAINS_TO_KEEP: '$LOCAL_BACKUP_CHAINS_TO_KEEP' <- It must be a natural integer >= 0"
            fi

        else
            echo "ERROR: LOCAL_BACKUP_PATH: '$LOCAL_BACKUP_PATH' <- Insufficient filesystem permissions to work into this path"
        fi

    else
        echo "ERROR: LOCAL_BACKUP_PATH: '$LOCAL_BACKUP_PATH'  <- Path was not found inside the container. Ensure you mount this correctly (e.g. '-v /mnt/user/backups/vm-backups:/backups:rw')"
    fi

    # Check RSYNC_BACKUP_PATH:
    #------------------------------------------------------------------------------
    case $RSYNC_BACKUP_PATH in

        '') # Rsync path is not set:

            rsync_backup_path_ok=true
            echo "RSYNC_BACKUP_PATH is not set"
        ;;

        [/]*) # Rsync path is an absolute path into root filesystem:

            if [[ -d $RSYNC_BACKUP_PATH ]]; then

                # RSYNC_BACKUP_PATH exists. Check for filesystem permissions:
                if [[ -r $RSYNC_BACKUP_PATH && -w $RSYNC_BACKUP_PATH ]]; then

                    # RSYNC_BACKUP_PATH has read/write permissions:
                    rsync_backup_path_exists=true
                    echo "RSYNC_BACKUP_PATH is local, and set to: $RSYNC_BACKUP_PATH"

                else
                    echo "ERROR: RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH' <- Insufficient filesystem permissions to work into this path"
                fi

            else
                echo "ERROR: RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH'  <- Path was not found inside the container. Ensure you mount this correctly (e.g. '-v /mnt/remotes/hostname/vm-backups-mirror:/backups-mirror:rw')"
            fi
        ;;

        *[@]*[:/]*)  # Rsync path is at aremote host, pointing an absolute path into its root filesystem:

            # Separate ssh login from remote path:
            local ssh_login=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f1)
            local rsync_path=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f2)

            # Attempts to comunicate with the remote host:
            ssh $SSH_OPTIONS $ssh_login "exit 0"
            local ssh_server_exit_code=$?

            if [[ $ssh_server_exit_code == 0 ]]; then

                # Attempts to perform similar checks as with $LOCAL_BACKUP_PATH, but creating a remote backup directory if doesn't exists:
                local remote_backup_path_status=$(ssh $SSH_OPTIONS $ssh_login "[[ -d $rsync_path && -r $rsync_path && -w $rsync_path ]] && echo 'found' || { mkdir -p $rsync_path; [[ -d $rsync_path ]] && echo 'created' || echo 'failed'; }")

                if [[ $remote_backup_path_status != failed ]]; then

                    # Mark as found (also notifiy if it was created):
                    rsync_backup_path_exists=true
                    echo "RSYNC_BACKUP_PATH is remote, was $remote_backup_path_status and set to: $RSYNC_BACKUP_PATH"

                else
                    echo "ERROR: Remote RSYNC_BACKUP_PATH '$RSYNC_BACKUP_PATH' <- Insufficient filesystem permissions to create or work into this path, or is not a directory"
                fi

            else
                echo "WARNING: Connection to $ssh_login failed (SSH exit code: $ssh_exit_code)"
            fi
        ;;

        *)  # Rsync path is anything else, and considered incorrect:
            echo "ERROR: Incorrect syntax for RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH'. It must be either an absolute path, or an SSH-like endpoint that Rsync can understand (e.g. root@hostname:/path/to/rsync-backup)"
        ;;
    esac

    if [[ $rsync_backup_path_exists ]]; then

        # Verify # of backup chains to keep on Rsync endpoint:
        if [[ $RSYNC_BACKUP_CHAINS_TO_KEEP =~ [0-9] && $RSYNC_BACKUP_CHAINS_TO_KEEP -ge 0 ]]; then

            # Check passed:
            rsync_backup_path_ok=true
            echo "RSYNC_BACKUP_CHAINS_TO_KEEP set to: $RSYNC_BACKUP_CHAINS_TO_KEEP"

        else
            echo "ERROR: Incorrect value RSYNC_BACKUP_CHAINS_TO_KEEP: '$RSYNC_BACKUP_CHAINS_TO_KEEP' <- It must be a natural integer >= 0"
        fi
    fi

    # TODO: Check other ENV variables, and SSH key:

    if [[ $max_backups_per_chain_ok && $local_backup_path_ok && $rsync_backup_path_ok ]]; then

            # When all status variables passed the checks, return success:
            variables_exit_code=0

    else
        # check_variables has found non-recoverable errors.
        variables_exit_code=1

        # Define messages to be prompted:
        local vars_issues_subject="Failed to start container!"
        local vars_issues_message="Issues were found checking environment variables and could not intialize"

        echo -e "\nERROR: $vars_issues_subject\n$vars_issues_message\n"

        [[ $(os_is_unraid) == true ]] \
            && unraid_notify "alert" "VM-Babysitter" "$vars_issues_subject" "$vars_issues_message. Please check container logs, or $(basename $LOGFILE_PATH) for more information."
    fi

    return $variables_exit_code
}

#------------------------------------------------------------------------------
# Populates DOMAINS_LIST, filtering ignored, into process and failed  VMs:
#------------------------------------------------------------------------------
create_domains_list()
{
    local domains_all_list=($(domains_list))
    local domains_skipped_list=(${VM_IGNORED_LIST[@]} ${VMS_TO_DISCARD_LIST[@]} ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ${POWERON_REQUIRED_VMS_LIST[@]} ${CHECK_BACKUPS_LIST[@]} ${CREATE_BACKUP_CHAIN_LIST[@]} ${BACKUPS_TO_SCHEDULE_LIST[@]})

    for domain in ${domains_all_list[@]}; do

        for skipped in ${domains_skipped_list[@]}; do

            [[ $domain == $skipped ]] && continue 2
        done

        DOMAINS_LIST+=($domain)
    done
}

#------------------------------------------------------------------------------
# Check for VMs in DOMAINS_LIST by verifying existence and read/write permissions of its virtual disks,
# also marking VMs for automatic start, when required by the user:
#------------------------------------------------------------------------------
check_domains()
{
    local domain_autostart_list=()
    local domain_success
    local domain_failed_list=()
    local domain_success_list=()
    local vm_has_issues

    echo "____________________________________________________________________________________________________________"
    echo -e "Checking access to virtual disks (and other remarks)..."

    for domain in ${DOMAINS_LIST[@]}; do

        # (Re)initialize used variables:
        vm_has_issues=""

        echo ""

        # Check for virtual disks, if are reachable and possess both read/write permissions:
        if [[ -z $(domain_drives_list $domain) ]]; then

            # Non-transient VM without disks attached. Adds to failed list, notifies and aborts the check:
            domain_failed_list+=($domain)
            vm_has_issues=true
            echo "$domain: Has no virtual disks that can be backed up"
            break

        else
            # Check if existing disk images are reachable inside the container and have correct filesystem permissions:
            local images_list=($(domain_img_paths_list $domain))

            for image in ${images_list[@]}; do

                if [[ ! -f $image ]]; then

                    vm_has_issues=true
                    echo "$domain: Disk image '$image' not found inside the container"

                elif [[ ! -r $image || ! -w $image ]]; then

                    vm_has_issues=true
                    echo "$domain: Insufficient filesystem permissions for disk image '$image'"

                else
                    "$domain: Found, and verified correct filesystem permissions for disk image '$image'"
                fi
            done

            if [[ $vm_has_issues == true ]]; then

                # Check failed. Add into correspondent list:
                domain_failed_list+=($domain)

            else
                # Check passed. Add into correspondent list:
                domain_success_list+=($domain)

                # Search for other remarks:
                for autostart_vm in $VM_AUTOSTART_LIST; do

                    if [[ $domain == $autostart_vm ]]; then

                        # If the VM is in VM_AUTOSTART_LIST, requires to be started:
                        domain_autostart_list+=($domain)
                        echo "$domain: Scheduled for autostart (if not found running by then)"

                        # And stop searching:
                        break
                    fi
                done
            fi
        fi
    done

    # Appends successfully checked VMs for checking backups:
    CHECK_BACKUPS_LIST+=(${domain_success_list[@]})

    # Appends VMs with unexistent, unreachable or virtual disks with permission issues:
    VMS_TO_DISCARD_LIST+=(${domain_failed_list[@]})

    # Appends successfully checked VMs marked for automatic start into the global list:
    POWERON_REQUIRED_VMS_LIST+=(${domain_autostart_list[@]})

    # Export definitive results, so other scripts have updated lists (e.g. future re-scan schedule of previously failed VMs):
    #------------------------------------------------------------------------------

    # Reload the external variables file:
    source $external_vars

    # And Vms with issues to global FAILED_VMS_LIST:
    FAILED_VMS_LIST+=(${VMS_TO_DISCARD_LIST[@]})

    # Sed can't expand arrays correctly. Convert variables to be exported into strings.
    # This will be reverted at next block iteration, by sourcing such external variables as arrays:
    FAILED_VMS_LIST="${FAILED_VMS_LIST[@]}"

    # Export the variables:
    sed -i \
    -e "s/FAILED_VMS_LIST=.*/FAILED_VMS_LIST=($FAILED_VMS_LIST)/" \
    $external_vars

    # Notify about issues and/or errors, if any:
    #------------------------------------------------------------------------------
    if [[ -n ${domain_failed_list[@]} ]]; then

        echo -e "WARNING: Issues found with at least one disk image paths of VM(s): ${domain_failed_list[@]}, and therefore such VMs will be ignored. To solve this, take in mind that:\n - $(basename $0) cannot work with VMs without virtual disks (e.g. transient VMs)\n - Paths including virtual disks must be mounted equally at both host and container (e.g. '/mnt/user/vms:/mnt/user/vms')\n - If additional disks are under different paths, they must be mounted alike, in similar way as the above example\n - All paths and virtual disks must have read/write permissions\n"

        if [[ $(os_is_unraid) == true ]]; then

            local vm_disks_issues_subject="Issues with disk image path(s)"
            local vm_disks_issues_message="Domain(s): ${domain_failed_list[@]} could not be put on backups schedule because issues were found with at least one of the disk image paths"

            unraid_notify "warning" "VM-Babysitter" "$vm_disks_issues_subject" "$vm_disks_issues_message. Please check container logs, or $(basename $LOGFILE_PATH) for more information."
        fi
    fi
}

#------------------------------------------------------------------------------
# Checks VMs in CHECK_BACKUPS_LIST for backup chain's health, permitting to keep using correct ones,
# managing broken ones according the scenario (archiving restorable backups),
# and preparing VMs to create a new backup chains.
# Optionally can verify backups integrity (if enabled by user):
#------------------------------------------------------------------------------
check_backups()
{
    # Local variables used for flow control:
    local backup_chain_integrity
    local backup_folder_integrity
    local checkpoints_found
    local delete_bitmap_exit_code
    local delete_checkpoint_exit_code
    local existing_bitmaps_list
    local existing_images_list
    local folder_checkpoints_list
    local no_backup_chain
    local shutdown_exit_code
    local some_vm_is_running
    local qemu_checkpoints_list
    local recoverable_backup_chain
    local crashed_unraid_scenario
    local verify_integrity_exit_code

    # Lists to add VMs (and summarize at the end):
    local broken_backup_chain_list
    local domain_failed_list
    local domain_shutdown_needed_list
    local domain_shutdown_success_list
    local preserved_backup_chain_list

   echo "____________________________________________________________________________________________________________"
   echo -e "Checking backup chains integrity..."

    if [[ $(os_is_unraid) == true ]]; then

        # OS is Unraid. Check for crashed_unraid_scenario, when comes from a crash and VMs were running before this container started:
        for domain in $(domains_list); do

            # Look for all VMs and its checkpoints, stop if it finds at least one:
            [[ -n $(domain_checkpoints_list $domain) ]] && { checkpoints_found=true; break; } || checkpoints_found=false
        done

        for domain in ${CHECK_BACKUPS_LIST[@]}; do

            # Look for all VMs to be checked, stop if any of them is running:
            [[ $(domain_state $domain) == "running" ]] && { some_vm_is_running=true; break; } || some_vm_is_running=false
        done

        if [[ $checkpoints_found == false && $some_vm_is_running == true && -z $VM_ALLOW_POWERCYCLE ]]; then

            # Set crashed_unraid_scenario flag and notify:
            crashed_unraid_scenario=true
            echo -e "\nNOTICE: Unraid OS detected, no VM has QEMU checkpoints at all, and at least one VM which backup chain needs to be checked is running\nTo prevent the need of user intervention when Unraid comes from a server crash scenario you can, according with your needs, either:\n - Set VM_ALLOW_POWERCYCLE to 'Y', allowing to powercycle domains with issues into backup chains; or\n - Disable autostart of domains you intend to backup automatically, using VM_AUTOSTART_LIST (and/or VM_IGNORED_LIST) instead\n"

        else
            # No crashed_unraid_scenario to mind about:
            crashed_unraid_scenario=false
        fi
    fi

    for domain in ${CHECK_BACKUPS_LIST[@]}; do

        backup_chain_integrity=""
        backup_folder_integrity=""
        delete_bitmap_exit_code=""
        delete_checkpoint_exit_code=""
        existing_images_list=""
        folder_checkpoints_list=()
        no_backup_chain=""
        shutdown_exit_code=""
        recoverable_backup_chain=""
        verify_integrity_exit_code=""

        echo ""
        #------------------------------------------------------------------------------
        # Stage 1: Check backup integrity:
        #------------------------------------------------------------------------------

        # Discard non-existing, and empty folders:
        #------------------------------------------------------------------------------
        if [[ ! -d $LOCAL_BACKUP_PATH/$domain || -z $(ls -A $LOCAL_BACKUP_PATH/$domain) ]]; then

            backup_folder_integrity=false
            echo "$domain: Unexistent or empty backup chain folder '$LOCAL_BACKUP_PATH/$domain'"

        # Interrupted operations (may recover backup chains when has more than one checkpoint):
        #------------------------------------------------------------------------------
        elif [[ -n $(find $LOCAL_BACKUP_PATH/$domain -type f -name "*.partial") ]]; then

            folder_checkpoints_list=($(backup_checkpoints_list $LOCAL_BACKUP_PATH/$domain))

            if [[ ${#folder_checkpoints_list[@]} -le 1 ]]; then

                # Backup is useless:
                backup_folder_integrity=false
                echo "$domain: A full/copy backup operation was previously cancelled. This backup is unrecoverable, therefore will be removed"

            elif [[ -z $VM_ALLOW_BACKUP_CHAIN_FIX || $crashed_unraid_scenario == true ]]; then

                # Backup chain may be recoverable, but either user didn't allow to check, or it's crashed_unraid_scenario:
                recoverable_backup_chain=false
                echo "$domain: A backup operation was previously cancelled. Backup chain is partially recoverable, but will not attempt to fix the backup for further use"

            else
                # Backup chain may be recoverable, and will attempt to fix it:
                recoverable_backup_chain=true
                echo "$domain: A backup operation was previously cancelled. Backup is partially recoverable and it will attempt to fix it and restore the current backup chain for further use"
            fi

        # If backup was made in 'copy' mode, there are no checkpoints or bitmaps in this VM to delete (and no checkpoints can be added anyway):
        #------------------------------------------------------------------------------
        elif [[ ! -f $LOCAL_BACKUP_PATH/$domain/$domain.cpt \
        && $(backup_checkpoints_list $LOCAL_BACKUP_PATH/$domain --num) -eq 0 \
        && -n $(find $LOCAL_BACKUP_PATH/$domain -type f -name "*.copy.data") ]]; then

            no_backup_chain=true
            echo "$domain: Current backup was made in 'copy' mode, and cannot be used for incremental backups"

        else # Backup folder integrity looks 'OK', at a first glance

            # Perform data check integrity when enabled by user:
            #------------------------------------------------------------------------------
            if [[ -n $CHECK_BACKUPS_INTEGRITY ]]; then

                verify_backup_chain $LOCAL_BACKUP_PATH/$domain
                verify_integrity_exit_code=$?
            fi

            # When has passed all tests above (including integrity check, if enabled),
            # then is approved for further checks:
            [[ $verify_integrity_exit_code != 1 ]] && backup_folder_integrity=true
        fi

        #------------------------------------------------------------------------------
        # Stage 2: Verify backup chain integrity on backup folders without issues or limitations:
        #------------------------------------------------------------------------------
        if [[ $backup_folder_integrity == true ]]; then

            echo "$domain: Backup folder integrity of '$LOCAL_BACKUP_PATH/$domain' appears to be OK"

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoints_list $domain))

            # Read backup checkpoints:
            folder_checkpoints_list=($(backup_checkpoints_list $LOCAL_BACKUP_PATH/$domain))

            if [[ $(domain_state $domain) == "shut off" || $crashed_unraid_scenario == true ]]; then

                # Checking in-backup checkpoints vs. disk image bitmaps it's the most accurate way to determine backup chain integrity.
                # Only can be done if VM is shut down (and the only possible way when crashed_unraid_scenario applies):
                local backup_check_message="Performing a full check into its backup chain..."

                [[ $crashed_unraid_scenario == true ]] \
                    && echo "$domain: Is $(domain_state $domain) (Crashed Unraid scenario). $backup_check_message" \
                    || echo "$domain: Is shut down. $backup_check_message"

                # Acquire the list of disk image path(s) contained for this VM:
                existing_images_list=($(domain_img_paths_list $domain))

                for image in ${existing_images_list[@]}; do

                    # Read bitmaps in each virtual image:
                    existing_bitmaps_list=($(disk_image_bitmaps_list $image))

                    if [[ ${existing_bitmaps_list[@]} == ${folder_checkpoints_list[@]} ]]; then

                        # Backup chain is OK, mark as preserved:
                        backup_chain_integrity=true
                        echo "$domain: Backup checkpoints and bitmaps lists on disk $image match (${#existing_bitmaps_list[@]}:${#folder_checkpoints_list[@]})"

                    else
                        backup_chain_integrity=false
                        echo "$domain: Backup checkpoints and bitmaps lists on disk $image do not match! (${#existing_bitmaps_list[@]}:${#folder_checkpoints_list[@]})"

                        # When more than one disk image to check, notify the check has been cancelled:
                        [[ ${#existing_images_list[@]} -gt 1 ]] && echo "$domain: Backup check operation has been cancelled"

                        # Interrupt the current operation. Nothing else to do:
                        break
                    fi
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                # When VM is running, bitmaps created during a backup chain update aren't visible with qemu-img info until VM has not been shut down.
                # The only reliable method it's to compare in-backup vs QEMU checkpoints count (and this doesn't apply on crashed_unraid_scenario):
                echo "$domain: Is running. Comparing QEMU and Backup's checkpoints lists..."

                if [[ ${qemu_checkpoints_list[@]} == ${folder_checkpoints_list[@]} ]]; then

                    # Backup chain integrity is OK:
                    backup_chain_integrity=true
                    echo "$domain: QEMU and backup checkpoints lists match (${#qemu_checkpoints_list[@]}:${#folder_checkpoints_list[@]})"

                else
                    # Mark the backup chain status as broken:
                    backup_chain_integrity=false
                    echo "$domain: QEMU and backup checkpoints lists do not match! ${#qemu_checkpoints_list[@]}:${#folder_checkpoints_list[@]}"
                fi
            fi

        elif [[ $backup_folder_integrity == false ]]; then

            # Delete non-recoverable backup folder:
            rm -rf $LOCAL_BACKUP_PATH/$domain
        fi

        #------------------------------------------------------------------------------
        # Stage 3: Process the results, according with all previous checks (attempting to fix recoverable issues, if possible):
        #------------------------------------------------------------------------------

        if [[ $backup_chain_integrity == true ]]; then

            # At this point, all checks has been successful and backup chain is ready for further use.
            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)

        elif [[ $no_backup_chain == true ]]; then

            # This is a 'copy' backup. As neither checkpoints nor bitmaps are present,
            # and no checkpoints can't be added; doesn't require further comprobations.
            # Add VM to the list of broken backup chain ones:
            broken_backup_chain_list+=($domain)

        elif [[ $recoverable_backup_chain == true ]]; then

            # This usually happens when container is stopped or restarted in the middle of a backup operation,
            # leaving an incomplete -and unusable- backup chain.
            # By deleting last checkpoint & bitmap from VM, and files generated by virtnbdbackup before the interruption,
            # backup chain ends at the latest successful backup state, and can receive more updates.
            # (Under crashed_unraid_scenario, there are basically NO chances to repair a backup chain this way, so this operation won't happen in that case):

            # TODO: Exit this process if issues are found during backup fix.

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoints_list $domain))

            if [[ $(domain_state $domain) == "shut off" ]]; then

                echo "$domain: Removing damaged checkpoint metadata: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoints $domain ${qemu_checkpoints_list[-1]} --metadata

                for image in $(domain_img_paths_list $domain); do

                    # Read bitmaps in each virtual image:
                    existing_bitmaps_list=($(disk_image_bitmaps_list $image))

                    echo "$domain: Deleting insecure bitmap ${existing_bitmaps_list[-1]} on disk image $image ..."
                    disk_image_bitmaps_delete $image ${existing_bitmaps_list[-1]}
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                echo "$domain: Removing damaged checkpoint: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoints $domain ${qemu_checkpoints_list[-1]}
            fi

            echo "$domain: Fixing backup chain data in $LOCAL_BACKUP_PATH/$domain ..."

            # Deletes any related file with the damaged checkpoint:
            find $LOCAL_BACKUP_PATH/$domain -name "*${qemu_checkpoints_list[-1]}*" -type f -delete

            # Deletes the last checkpoint from the list:
            unset qemu_checkpoints_list[-1]

            # Turn the updated list into string to export:
            qemu_checkpoints_list="${qemu_checkpoints_list[@]}"

            # Rebuilds the cpt file with the parsed new list of checkpoints:
            echo "[\"${qemu_checkpoints_list// /\", \"}\"]" > $LOCAL_BACKUP_PATH/$domain/$domain.cpt

            # Backup folder integrity is OK:
            backup_folder_integrity=true

            # Backup chain integrity is assumed as OK:
            backup_chain_integrity=true

            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)
            echo "$domain: Backup chain was fixed by removing latest checkpoint and deleting incomplete data at '$LOCAL_BACKUP_PATH/$domain'"

        else # All other scenarios, where checking backup folder and/or chain failed...

            # Delete checkpoints and/or bitmaps, depending on the case:

            if [[ $(domain_state $domain) == "shut off" ]]; then

                # Deletes checkpoints metadata, if any detected:
                echo "$domain: Pruning any existing checkpoints metadata..."
                domain_delete_checkpoints $domain --all --metadata
                delete_checkpoint_exit_code=$?

                for image in $(domain_img_paths_list $domain); do

                    # Then deletes all bitmaps, in all image disks:
                    echo "$domain: Pruning any existing bitmaps on disk image $image ..."

                    disk_image_bitmaps_delete $image --all
                    delete_bitmap_exit_code=$?

                    # When issues are found deleting bitmaps, cancel the entire operation:
                    [[ $delete_bitmap_exit_code -eq 1 ]] && break
                done

            elif [[ $(domain_state $domain) == "running" && $crashed_unraid_scenario == false ]]; then

                # Deletes checkpoints (and bitmaps) if any detected:
                echo "$domain: Pruning any existing checkpoints..."
                domain_delete_checkpoints $domain --all
                delete_checkpoint_exit_code=$?

            elif [[ $(domain_state $domain) == "running" && $crashed_unraid_scenario == true ]]; then

                # crashed_unraid_scenario. Only does something if is allowed to perform a power cycle on VM:
                if [[ -n $VM_ALLOW_POWERCYCLE ]]; then

                    domain_shutdown $domain --wait $VM_WAIT_TIME
                    shutdown_exit_code=$?

                    if [[ $shutdown_exit_code -eq 0 ]]; then

                        domain_shutdown_success_list+=($domain)

                        for image in $(domain_img_paths_list $domain); do

                            # Then deletes all bitmaps, in all image disks:
                            echo "$domain: Pruning any existing bitmaps on disk image $image ..."

                            disk_image_bitmaps_delete $image --all
                            delete_bitmap_exit_code=$?

                            # When issues are found deleting bitmaps, cancel the entire operation:
                            [[ $delete_bitmap_exit_code -eq 1 ]] && break
                        done
                    fi
                fi
            fi

            # Assign VM and/or backup chain to the correspondent list, according the results:
            case $delete_checkpoint_exit_code in

                0|2)
                    if [[ $(domain_state $domain) == "shut off" ]]; then

                        case $delete_bitmap_exit_code in

                            0|2)    # Checkpoints metadata and bitmaps deleted successfully (or nothing to delete).
                                    # Add to broken backup chain list:
                                    broken_backup_chain_list+=($domain)
                            ;;

                            *)  # There were problems when deleting bitmaps.
                                # This VM has issues and user action is required:
                                domain_failed_list+=($domain)
                            ;;
                        esac

                    elif [[ $(domain_state $domain) == "running" ]]; then

                        # Checkpoints deleted successfully (or nothing to delete).
                        # Add to broken backup chain list:
                        broken_backup_chain_list+=($domain)
                    fi
                ;;

                1)  # There were problems when deleting checkpoints.
                    # This VM has issues and user action is required:
                    domain_failed_list+=($domain)
                ;;

                "") # crashed_unraid_scenario (no checkpoints were ever processed):

                    case $shutdown_exit_code in

                        0)
                            case $delete_bitmap_exit_code in

                                0|2)    # Bitmaps deleted successfully (or nothing to delete).
                                        # Add to broken backup chain list:
                                        broken_backup_chain_list+=($domain)
                                ;;

                                1)
                                    # There were problems when deleting bitmaps.
                                    # This VM has issues and user action is required:
                                    domain_failed_list+=($domain)
                                ;;
                            esac
                        ;;

                        1)  # There were problems when shutting down the VM.
                            # This VM may either have issues, or just taking too long into shut down.
                            # User action is required (but may also shut down itself eventually):
                            domain_shutdown_needed_list+=($domain)
                            echo "WARNING: Shut down of domain '$domain' didn't complete after $VM_WAIT_TIME seconds. If this does not occur eventually, user manual intervention is going to be required"
                        ;;

                        "") # No permissions to power cycle the VM are given (no shutdown was ever attempted):
                            # User action is required:
                            domain_shutdown_needed_list+=($domain)
                            echo "WARNING: Manual shut down of domain '$domain' is required! (VM_ALLOW_POWERCYCLE is not set)"
                        ;;
                    esac
                ;;
            esac
        fi
    done

    # Appends broken backup chains for new backup chain creation:
    CREATE_BACKUP_CHAIN_LIST+=(${broken_backup_chain_list[@]})

    # Appends successfully shut down VMs to the global list in need to be started:
    POWERON_REQUIRED_VMS_LIST+=(${domain_shutdown_success_list[@]})

    # Appends preserved backup chains to scheduled backups directly:
    BACKUPS_TO_SCHEDULE_LIST+=(${preserved_backup_chain_list[@]})

    # Appends VMs failed to shutdown (by user's setting or by fail) to request user action:
    SHUTDOWN_REQUIRED_VMS_LIST+=(${domain_shutdown_needed_list[@]})

    # Appends failed to shut down VMs to failed VMs list:
    VMS_TO_DISCARD_LIST+=(${domain_failed_list[@]})

    # Export definitive results, so other scripts have updated lists (e.g. scheduled backups):
    #------------------------------------------------------------------------------

    # Reload the external variables file:
    source $external_vars

    # Add preserved backup chains to global SCHEDULED_BACKUPS_LIST:
    SCHEDULED_BACKUPS_LIST+=(${BACKUPS_TO_SCHEDULE_LIST[@]})

    # And Vms with issues to global FAILED_VMS_LIST:
    FAILED_VMS_LIST+=(${VMS_TO_DISCARD_LIST[@]})

    # Sed can't expand arrays correctly. Convert variables to be exported into strings.
    # This will be reverted at next block iteration, by sourcing such external variables as arrays:
    FAILED_VMS_LIST="${FAILED_VMS_LIST[@]}"
    SCHEDULED_BACKUPS_LIST="${SCHEDULED_BACKUPS_LIST[@]}"

    # Export the variables:
    sed -i \
    -e "s/FAILED_VMS_LIST=.*/FAILED_VMS_LIST=($FAILED_VMS_LIST)/" \
    -e "s/SCHEDULED_BACKUPS_LIST=.*/SCHEDULED_BACKUPS_LIST=($SCHEDULED_BACKUPS_LIST)/" \
    $external_vars

    # Notify about issues and/or errors, if any:
    #------------------------------------------------------------------------------

    if [[ -n ${domain_shutdown_needed_list[@]} ]]; then

        if [[ $unraid_crashed_scenario == true ]]; then

            # check_backups encountered unraid_crashed_scenario:
            local shutdown_required_subject="Manual Shut Down is Required!"
            local shutdown_required_message="Please power off the following domain(s): ${domain_shutdown_needed_list[@]}. This action is mandatory before to put it into backup schedule. Once performed, the final restart will be done automatically"

        else
            # check_backups gave up powering off vm(s):
            local shutdown_required_subject="A Manual Shut Down Might be Required"
            local shutdown_required_message="Domain(s): ${domain_shutdown_needed_list[@]}, are taking more than expected into shut down completely ($VM_WAIT_TIME secs). This step is mandatory before to put it into backup schedule. If no further notice about success of this is received into the next minutes, consider to examine the domain(s) and if necessary, perform a manual shut down (the final restart will be done automatically)."
        fi

        echo -e "\nNOTICE: $shutdown_required_subject\n$shutdown_required_message\n"

        [[ $(os_is_unraid) == true ]] \
            && unraid_notify "warning" "VM-Babysitter" "$shutdown_required_subject" "$shutdown_required_message. Please check container logs, or $(basename $LOGFILE_PATH) for more information."
    fi

    if [[ -n ${domain_failed_list[@]} ]]; then

        # check_backups found unexpected issues:
        local backup_chain_issues_subject="Something went unexpectedly wrong when checking backup chain integrity"
        local backup_chain_issues_message="Domain(s): ${domain_failed_list[@]} could not be put on backups schedule because something went unexpectedly wrong while checking backup integrity"

        echo -e "\nWARNING: $backup_chain_issues_subject\n$backup_chain_issues_message\n"

        [[ $(os_is_unraid) == true ]] \
            && unraid_notify "warning" "VM-Babysitter" "$backup_chain_issues_subject" "$backup_chain_issues_message. Please check container logs, or $(basename $LOGFILE_PATH) for more information."
    fi
}

###############################################################################
# Main execution:
###############################################################################

# User variables defaults (details on README.md):
#------------------------------------------------------------------------------
BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-"@daily"}
CHECK_BACKUPS_INTEGRITY=${CHECK_BACKUPS_INTEGRITY:-""}
LOCAL_BACKUP_CHAINS_TO_KEEP=${LOCAL_BACKUP_CHAINS_TO_KEEP:-"1"}
LOCAL_BACKUP_PATH=${LOCAL_BACKUP_PATH:-"/backups"}
LOGFILE_PATH=${LOGFILE_PATH:-"/logs/vm-babysitter.log"}
SCHEDULE_LOGFILE_PATH=${SCHEDULE_LOGFILE_PATH:-"/logs/scheduled-tasks.log"}
LOGROTATE_CONFIG_PATH=${LOGROTATE_CONFIG_PATH:-"/tmp/logrotate.d/vm-babysitter"}
LOGROTATE_SCHEDULE=${LOGROTATE_SCHEDULE:-"@daily"}
LOGROTATE_SETTINGS=${LOGROTATE_SETTINGS:-"  compress\n  copytruncate\n  daily\n  dateext\n  dateformat .%Y-%m-%d.%H:%M:%S\n  missingok\n  notifempty\n  rotate 30"}
MAX_BACKUPS_PER_CHAIN=${MAX_BACKUPS_PER_CHAIN:-"30"}
RSYNC_ARGS=${RSYNC_ARGS:-"-a"}
RSYNC_BACKUP_CHAINS_TO_KEEP=${RSYNC_BACKUP_CHAINS_TO_KEEP:-"2"}
RSYNC_BACKUP_PATH=${RSYNC_BACKUP_PATH:-""}
RSYNC_SCHEDULE=${RSYNC_SCHEDULE:-""}
SSH_OPTIONS=${SSH_OPTIONS:-"-q -o IdentityFile=/private/hostname.key -o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"}
[[ -f /usr/share/zoneinfo/$TZ ]] && local_timezone_file="/usr/share/zoneinfo/$TZ"
UNRAID_NOTIFY_HOST=${UNRAID_NOTIFY_HOST:-"localhost"}
VIRTNBDBACKUP_ARGS=${VIRTNBDBACKUP_ARGS:-""}
VM_ALLOW_BACKUP_CHAIN_FIX=${VM_ALLOW_BACKUP_CHAIN_FIX:-""}
VM_ALLOW_POWERCYCLE=${VM_ALLOW_POWERCYCLE:-""}
VM_AUTOSTART_LIST=${VM_AUTOSTART_LIST:-""}
VM_IGNORED_LIST=${VM_IGNORED_LIST:-""}
VM_WAIT_TIME=${VM_WAIT_TIME:-"60"}

# Internal Variables:
#------------------------------------------------------------------------------

# Temporal crontab file (to be loaded for cron)
crontab_file="/tmp/crontab"

# Storing file for bash like lists shared between this script and the scheduler:
external_vars="/tmp/vm-babysit-vars"

# Export variables used by children processes:
export external_vars

# Redirect system stdout and stderr to $LOGFILE_PATH. Note this applies to all other commands and scripts run from this script:
exec &> >(tee -a $LOGFILE_PATH)

# Create internal folders in case don't exist:
[[ ! -d $( dirname $LOGFILE_PATH) ]] && mkdir -p $(dirname $LOGFILE_PATH)

# Sets the local timezone (if ENV TZ was set):
if [[ -n $local_timezone_file ]]; then

    ln -fs $local_timezone_file /etc/localtime
    &> /dev/null dpkg-reconfigure -f noninteractive tzdata
fi

# Catches the signal sent from docker to stop execution:
# The most gracefully way to stop this container is with:
# 'docker kill --signal=SIGTERM <docker-name-or-id>'
trap 'stop_container' SIGTERM

echo "############################################################################################################"
echo "Container started at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))"
echo "############################################################################################################"

#------------------------------------------------------------------------------
# 1. Check input parameters (exits on error)
#------------------------------------------------------------------------------
check_variables
check_variables_exit_code=$?

#------------------------------------------------------------------------------
# 2. Only when input parameters doesn't require to restart the container, it continues the rest of the checks:
#------------------------------------------------------------------------------

if [[ $check_variables_exit_code -eq 0 ]]; then

    # Initializes a file with variables externally stored, to be shared with the scheduler:
    #------------------------------------------------------------------------------
    cat << end_of_external_variables > $external_vars
# Shared values between scripts. DO NOT EDIT!:
FAILED_VMS_LIST=()
SCHEDULED_BACKUPS_LIST=()
end_of_external_variables

    # Create logrotate config:
    #------------------------------------------------------------------------------

    echo -e "\nConfiguring logrotate..."

    # Create directory if doesn't exist:
    mkdir -p $(dirname $LOGROTATE_CONFIG_PATH)

    # Parse logrotate configuration (overwriting, if any previous):
    echo -e "$LOGFILE_PATH \n $SCHEDULE_LOGFILE_PATH {\n$LOGROTATE_SETTINGS\n}" > $LOGROTATE_CONFIG_PATH

    # Create/update Cron task for VMs to be (progressively) included in $scheduled_backups_list:
    #------------------------------------------------------------------------------

    echo -e "Configuring scheduled tasks..."

    # Silently deletes any previous cron task:
    &> /dev/null crontab -r

    # Parses the actual cron task needed to run to $crontab_file
    # (Including ENV vars not being read from cron's environment):
    cat << base_crontab > $crontab_file
# Values below are refreshed upon container (re)start:

# Main environment is bash:
SHELL=/bin/bash

# Search paths for binaries and scripts:
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# Environment variables used by scheduled tasks:
LOCAL_BACKUP_CHAINS_TO_KEEP="$LOCAL_BACKUP_CHAINS_TO_KEEP"
LOCAL_BACKUP_PATH="$LOCAL_BACKUP_PATH"
MAX_BACKUPS_PER_CHAIN="$MAX_BACKUPS_PER_CHAIN"
RSYNC_ARGS="$RSYNC_ARGS"
RSYNC_BACKUP_CHAINS_TO_KEEP="$RSYNC_BACKUP_CHAINS_TO_KEEP"
RSYNC_BACKUP_PATH="$RSYNC_BACKUP_PATH"
RSYNC_SCHEDULE=$RSYNC_SCHEDULE
SSH_OPTIONS="$SSH_OPTIONS"
UNRAID_NOTIFY_HOST="$UNRAID_NOTIFY_HOST"
VIRTNBDBACKUP_ARGS="$VIRTNBDBACKUP_ARGS"
VM_WAIT_TIME=$VM_WAIT_TIME

# Paths for functions and (shared) dynamic variables:
external_vars="$external_vars"

# Schedule for logrotate:
$LOGROTATE_SCHEDULE /usr/sbin/logrotate $LOGROTATE_CONFIG_PATH >> $SCHEDULE_LOGFILE_PATH 2>&1

# Schedule for VM backups:
$BACKUP_SCHEDULE /usr/local/bin/vm-backup --run-schedule >> $SCHEDULE_LOGFILE_PATH 2>&1
base_crontab

    if [[ -n $RSYNC_BACKUP_PATH && -n $RSYNC_SCHEDULE ]]; then

    # Append additional task to run Rsync on a defined schedule:
        cat << optional_crontab >> $crontab_file

# Schedule for Rsync:
$RSYNC_SCHEDULE /usr/local/bin/vm-rsync --run-schedule >> $SCHEDULE_LOGFILE_PATH 2>&1
optional_crontab
    fi

    # Sets the cron task:
    crontab $crontab_file

    # Finally, runs cron and sends to background:
    echo -e "Starting Cron..."
    cron -f -l -L2 &

    # 3. Begin monitorization for VMs in lists, performing operations as required:
    #------------------------------------------------------------------------------
    echo -e "Babysitter mode started...\n"

    # Creates DOMAINS_LIST, a list of non-transient domains that aren't into any of the queues from past iterations:
    create_domains_list

    while true; do

        # Maximum standby period for monitoring should not exceed 10 seconds in any case,
        # because it could ignore SIGTERM from Docker, thus being killed with SIGKILL:
        sleep 1

        if [[ -n ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ]]; then

            # Check for VMs which are in need of shutdown.
            # (This normally happens when the user took the action, or when a VM took more than VM_WAIT_TIME secs to shutdown):
            #------------------------------------------------------------------------------
            i=0
            for domain in ${SHUTDOWN_REQUIRED_VMS_LIST[@]}; do

                if [[ $(domain_state $domain) == "shut off" ]]; then

                    # Add VM to the same list of VMs to be powered on when monitor starts:
                    POWERON_REQUIRED_VMS_LIST+=($domain)

                    # Move to main queue for check:
                    CHECK_BACKUPS_LIST+=($domain)
                    unset SHUTDOWN_REQUIRED_VMS_LIST[$i]

                    # Rebuild indexes into the array:
                    SHUTDOWN_REQUIRED_VMS_LIST=(${SHUTDOWN_REQUIRED_VMS_LIST[@]})

                fi

                # Increases the counter:
                ((i++))
            done

            if [[ $(os_is_unraid) == true ]]; then

                if [[ -z $VM_ALLOW_POWERCYCLE ]]; then

                    shutdown_performed_subject="Manual Powercycle of Domain(s) in Progress"
                    shutdown_performed_message="Domain(s) ${CHECK_BACKUPS_LIST[@]} went shut off successfully by the user, and will be right back as soon as all checks has finished."

                else
                    shutdown_performed_subject="Automatic Powercycle of Domain(s) in Progress"
                    shutdown_performed_message="Domain(s) ${CHECK_BACKUPS_LIST[@]} suddenly went shut off automatically, without user intervention. Will be right back as soon as checks has finished."
                fi

                unraid_notify "warning" "VM-Babysitter" "$shutdown_performed_subject" "$shutdown_performed_message"
            fi
        fi

        if [[ -n ${DOMAINS_LIST[@]} || -n ${CHECK_BACKUPS_LIST[@]} ]]; then

            echo "============================================================================================================"
            echo -e "Changes detected at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))\n"

            # Check for virtual drives status on detected VMs:
            #------------------------------------------------------------------------------
            [[ -n ${DOMAINS_LIST[@]} ]] && check_domains

            # Check for backup chain integrity on all VMs:
            #------------------------------------------------------------------------------
            [[ -n ${CHECK_BACKUPS_LIST[@]} ]] && check_backups

            # Turns on all VMs that was, either declared in VM_AUTOSTART_LIST or previously shutdown for checks:
            #------------------------------------------------------------------------------
            if [[ -n ${POWERON_REQUIRED_VMS_LIST[@]} ]]; then

                echo ""
                for domain in ${POWERON_REQUIRED_VMS_LIST[@]}; do

                    if [[ $(domain_state $domain) != running ]]; then

                        # Turn on the VM. Do not wait for Guest's QEMU agent:
                        domain_start $domain --nowait

                        # Add any VM failed to start to VMS_TO_DISCARD_LIST:
                        [[ $? -ne 0 ]] && VMS_TO_DISCARD_LIST+=($domain)
                    fi
                done
            fi

            # Create new backup chains for VMs that require it:
            #------------------------------------------------------------------------------
            if [[ -n ${CREATE_BACKUP_CHAIN_LIST[@]} ]]; then

                # Create new backup chains with backup script (on-demand mode):
                vm-backup --create-new ${CREATE_BACKUP_CHAIN_LIST[@]}
            fi

            # Show a final summary and notify about user actions, if required:
            #------------------------------------------------------------------------------
            echo "============================================================================================================"
            echo -e "All changes ended to be processed at $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))"
            echo -e "Changed Status Summary:"

            # Good news goes first:
            [[ -n ${DOMAINS_LIST[@]} ]] \
                && { echo -e "\nSubmitted for check: ${#DOMAINS_LIST[@]}\n > ${DOMAINS_LIST[@]}"; unset DOMAINS_LIST; }

            [[ -n ${CHECK_BACKUPS_LIST[@]} ]] \
                && { echo -e "\nPassed the initial check: ${#CHECK_BACKUPS_LIST[@]}\n > ${CHECK_BACKUPS_LIST[@]}"; unset CHECK_BACKUPS_LIST; }

            [[ -n ${BACKUPS_TO_SCHEDULE_LIST[@]} ]] \
                && { echo -e "\nPassed the backup check integrity: ${#BACKUPS_TO_SCHEDULE_LIST[@]}\n > ${BACKUPS_TO_SCHEDULE_LIST[@]}"; unset BACKUPS_TO_SCHEDULE_LIST; }

            [[ -n ${CREATE_BACKUP_CHAIN_LIST[@]} ]] \
                && { echo -e "\nRequired new backup chains: ${#CREATE_BACKUP_CHAIN_LIST[@]}\n > ${CREATE_BACKUP_CHAIN_LIST[@]}"; unset CREATE_BACKUP_CHAIN_LIST; }

            # Then notify about other events (autostart, powercycle, etc.):
            [[ -n ${POWERON_REQUIRED_VMS_LIST[@]} ]] \
                && { echo -e "\nAuto-started or completed powercycle: ${POWERON_REQUIRED_VMS_LIST[@]}\n > ${POWERON_REQUIRED_VMS_LIST[@]}"; unset POWERON_REQUIRED_VMS_LIST; }

            # Then notify about potential, or directly bad news:
            [[ -n ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ]] \
                && echo -e "\nPending shut down (auto or manual): ${#SHUTDOWN_REQUIRED_VMS_LIST[@]}\n > ${SHUTDOWN_REQUIRED_VMS_LIST[@]}"

            [[ -n ${VMS_TO_DISCARD_LIST[@]} ]] \
                && { echo -e "\nFailed during checks: ${#VMS_TO_DISCARD_LIST[@]}\n > ${VMS_TO_DISCARD_LIST[@]}"; unset VMS_TO_DISCARD_LIST; }
        fi

        # Restarts the loop, until SIGTERM or SIGKILL are received from Docker...
        #------------------------------------------------------------------------------
    done

    else
        # Stop the container when check_variables ends with code 1:
        stop_container
fi
