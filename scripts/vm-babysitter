#!/bin/bash

# Load functions:
source /usr/local/bin/vm-functions

###############################################################################
# Specific procedures:
###############################################################################

#------------------------------------------------------------------------------
# Attempts to stop the container gracefully in case of receive SIGTERM signal from Docker:
#------------------------------------------------------------------------------
stop_container()
{
    echo "############################################################################################################"
    echo "$(basename $0) received SIGTERM signal at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))"

    # TODO: Terminate or kill background processes before to exit...

    echo -e "Stopped.\n"
    echo ""
    exit 0
}

#------------------------------------------------------------------------------
# Checks environment variables and return success or fail if everything is in order:
#------------------------------------------------------------------------------
check_variables()
{
    # Starts check assuming failed statuses:
    local variables_exit_code
    local local_backup_path_ok
    local max_backups_per_chain_ok
    local rsync_backup_path_exists
    local rsync_backup_path_ok

    echo "____________________________________________________________________________________________________________"
    echo -e "Checking environment variables...\n"

    # Check MAX_BACKUPS_PER_CHAIN
    #------------------------------------------------------------------------------
    if [[ $MAX_BACKUPS_PER_CHAIN =~ [0-9] && $MAX_BACKUPS_PER_CHAIN -ge 0 ]]; then

        # Is an integer number:
        max_backups_per_chain_ok=true
        echo "MAX_BACKUPS_PER_CHAIN set to: $MAX_BACKUPS_PER_CHAIN"

    else
        # Invalid value:
        echo "ERROR: Incorrect value for MAX_BACKUPS_PER_CHAIN: '$MAX_BACKUPS_PER_CHAIN'. It must be a natural integer >= 0"
    fi

    # Check LOCAL_BACKUP_PATH:
    #------------------------------------------------------------------------------
    if  [[ -d $LOCAL_BACKUP_PATH ]]; then

        # LOCAL_BACKUP_PATH exists. Check for filesystem permissions:
        if  [[ -r $LOCAL_BACKUP_PATH && -w $LOCAL_BACKUP_PATH ]]; then

            # LOCAL_BACKUP_PATH has read/write permissions.
            echo "LOCAL_BACKUP_PATH set to: $LOCAL_BACKUP_PATH"

            # Check LOCAL_BACKUP_CHAINS_TO_KEEP:
            if [[ $LOCAL_BACKUP_CHAINS_TO_KEEP =~ [0-9] && $LOCAL_BACKUP_CHAINS_TO_KEEP -ge 0 ]]; then

                # Check passed:
                local_backup_path_ok=true
                echo "LOCAL_BACKUP_CHAINS_TO_KEEP set to: $LOCAL_BACKUP_CHAINS_TO_KEEP"

            else
                # Invalid value:
                echo "ERROR: Incorrect value for LOCAL_BACKUP_CHAINS_TO_KEEP: '$LOCAL_BACKUP_CHAINS_TO_KEEP'. It must be a natural integer >= 0"
            fi

        else
            echo "ERROR: LOCAL_BACKUP_PATH: '$LOCAL_BACKUP_PATH' <- Insufficient filesystem permissions to work into this path"
        fi

    else
        echo "ERROR: LOCAL_BACKUP_PATH: '$LOCAL_BACKUP_PATH' <- Path not found inside the container. Ensure to mount this correctly (e.g. '-v /mnt/user/backups/vm-backups:/backups:rw')"
    fi

    # Check RSYNC_BACKUP_PATH:
    #------------------------------------------------------------------------------
    case $RSYNC_BACKUP_PATH in

        '') # Rsync path is not set:

            rsync_backup_path_ok=true
            echo "RSYNC_BACKUP_PATH is not set"
        ;;

        [/]*) # Rsync path is an absolute path into root filesystem:

            if [[ -d $RSYNC_BACKUP_PATH ]]; then

                # RSYNC_BACKUP_PATH exists. Check for filesystem permissions:
                if [[ -r $RSYNC_BACKUP_PATH && -w $RSYNC_BACKUP_PATH ]]; then

                    # RSYNC_BACKUP_PATH has read/write permissions:
                    rsync_backup_path_exists=true
                    echo "RSYNC_BACKUP_PATH is a local endpoint, and set to: $RSYNC_BACKUP_PATH"

                else
                    echo "ERROR: RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH'. Insufficient filesystem permissions to work into this path"
                fi

            else
                echo "ERROR: RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH'. Path not found inside the container. Ensure to mount this correctly (e.g. '-v /mnt/remotes/hostname/vm-backups-mirror:/backups-mirror:rw')"
            fi
        ;;

        *[@]*[:/]*)  # Rsync path is at aremote host, pointing an absolute path into its root filesystem:

            # Separate ssh login from remote path:
            local ssh_login=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f1)
            local rsync_path=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f2)

            # Attempts to comunicate with the remote host:
            ssh $SSH_OPTIONS $ssh_login "exit 0"
            local ssh_server_exit_code=$?

            if [[ $ssh_server_exit_code == 0 ]]; then

                # Attempts to perform similar checks as with $LOCAL_BACKUP_PATH, but creating a remote backup directory if doesn't exists:
                local remote_backup_path_status=$(ssh $SSH_OPTIONS $ssh_login "[[ -d $rsync_path && -r $rsync_path && -w $rsync_path ]] && echo 'found' || { mkdir -p $rsync_path; [[ -d $rsync_path ]] && echo 'created' || echo 'failed'; }")

                if [[ $remote_backup_path_status != failed ]]; then

                    # Mark as found (also notifiy if it was created):
                    rsync_backup_path_exists=true
                    echo "RSYNC_BACKUP_PATH is a remote endpoint, was $remote_backup_path_status and set to: $RSYNC_BACKUP_PATH"

                else
                    echo "ERROR: Remote RSYNC_BACKUP_PATH '$RSYNC_BACKUP_PATH'. Insufficient filesystem permissions to create or work into this path, or is not a directory"
                fi

            else
                echo "WARNING: Connection to $ssh_login failed (SSH exit code: $ssh_exit_code)"
            fi
        ;;

        *)  # Rsync path is anything else, and considered incorrect:
            echo "ERROR: Incorrect syntax for RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH'. It must be either an absolute path, or an SSH-like endpoint that Rsync can understand (e.g. root@hostname:/path/to/rsync-backup)"
        ;;
    esac

    if [[ $rsync_backup_path_exists ]]; then

        # Verify # of backup chains to keep on Rsync endpoint:
        if [[ $RSYNC_BACKUP_CHAINS_TO_KEEP =~ [0-9] && $RSYNC_BACKUP_CHAINS_TO_KEEP -ge 0 ]]; then

            # Check passed:
            rsync_backup_path_ok=true
            echo "RSYNC_BACKUP_CHAINS_TO_KEEP set to: $RSYNC_BACKUP_CHAINS_TO_KEEP"

        else
            echo "ERROR: Incorrect value RSYNC_BACKUP_CHAINS_TO_KEEP: '$RSYNC_BACKUP_CHAINS_TO_KEEP'. It must be a natural integer >= 0"
        fi
    fi

    # TODO: Check other ENV variables, and SSH key:

    if [[ $max_backups_per_chain_ok && $local_backup_path_ok && $rsync_backup_path_ok ]]; then

            # When all status variables passed the checks, return success:
            variables_exit_code=0
    else
        # check_variables has found non-recoverable errors.
        variables_exit_code=1

        # Define messages to be prompted:
        local vars_issues_subject="Failed to start container!"
        local vars_issues_message="Issues were found checking environment variables and could not intialize"

        echo -e "\nERROR: $vars_issues_subject\n$vars_issues_message\n"

        [[ $(os_is_unraid) == true ]] \
            && unraid_notify "alert" "VM-Babysitter" "$vars_issues_subject" "$vars_issues_message$unraid_logcheck_message"
    fi

    return $variables_exit_code
}

#------------------------------------------------------------------------------
# Populates DOMAINS_LIST, filtering ignored, and already processed VMs:
#------------------------------------------------------------------------------
create_domains_list()
{
    # Reload the external variables file:
    source $external_vars

    local domains_all_list=($(domains_list))
    local domains_ignored_list=(${VM_IGNORED_LIST[@]} ${SCHEDULED_BACKUPS_LIST[@]} ${FAILED_VMS_LIST[@]})

    for domain in ${domains_all_list[@]}; do

        # When domain is not found at domains_ignored_list, add it DOMAINS_LIST:
        [[ -z $(item_position $domain "domains_ignored_list") ]] && DOMAINS_LIST+=($domain)

    done
}

#------------------------------------------------------------------------------
# Check for VMs in DOMAINS_LIST by verifying existence and read/write permissions of its virtual disks:
#------------------------------------------------------------------------------
check_domains()
{
    local domain_success
    local domain_failed_list=()
    local domain_success_list=()
    local vm_has_issues

    echo "____________________________________________________________________________________________________________"
    echo -e "Checking fileystem permissions for virtual disks..."

    for domain in ${DOMAINS_LIST[@]}; do

        # (Re)initialize used variables:
        vm_has_issues=""

        echo ""

        # Check for virtual disks, if are reachable and possess both read/write permissions:
        if [[ -z $(domain_drives_list $domain) ]]; then

            # Non-transient VM without disks attached. Adds to failed list, notifies and aborts the check:
            domain_failed_list+=($domain)
            vm_has_issues=true
            echo "$domain: Has no virtual disks that can be backed up"
            break

        else
            # Check if existing disk images are reachable inside the container and have correct filesystem permissions:
            local images_list=($(domain_img_paths_list $domain))

            for image in ${images_list[@]}; do

                if [[ ! -f $image ]]; then

                    vm_has_issues=true
                    echo "$domain: Disk image '$image' not found inside the container"

                elif [[ ! -r $image || ! -w $image ]]; then

                    vm_has_issues=true
                    echo "$domain: Insufficient filesystem permissions for disk image '$image'"

                else
                    echo "$domain: Disk image $image found, and verified correct filesystem permissions"
                fi
            done

            if [[ $vm_has_issues == true ]]; then

                # Check failed. Add into correspondent list:
                domain_failed_list+=($domain)

            else
                # Check passed. Add into correspondent list:
                domain_success_list+=($domain)
            fi
        fi
    done

    # Appends successfully checked VMs for checking backups:
    CHECK_BACKUPS_LIST+=(${domain_success_list[@]})

    # Appends VMs with unexistent, unreachable or virtual disks with permission issues:
    VMS_TO_DISCARD_LIST+=(${domain_failed_list[@]})

    # Export definitive results, so other scripts have updated lists (e.g. future re-scan schedule of previously failed VMs):
    #------------------------------------------------------------------------------

    # Reload the external variables file:
    source $external_vars

    # And Vms with issues to global FAILED_VMS_LIST:
    FAILED_VMS_LIST+=(${VMS_TO_DISCARD_LIST[@]})

    # Sed can't expand arrays correctly. Convert variables to be exported into strings.
    # This will be reverted at next block iteration, by sourcing such external variables as arrays:
    FAILED_VMS_LIST="${FAILED_VMS_LIST[@]}"

    # Export the variables:
    sed -i \
    -e "s/FAILED_VMS_LIST=.*/FAILED_VMS_LIST=($FAILED_VMS_LIST)/" \
    $external_vars

    # Notify about issues and/or errors, if any:
    #------------------------------------------------------------------------------
    if [[ -n ${domain_failed_list[@]} ]]; then

        echo -e "\nWARNING: Issues found with at least one disk image paths of VM(s): ${domain_failed_list[@]}, and therefore will be ignored. To solve this, take in mind that:\n - $(basename $0) cannot work with domains without virtual disks (e.g. transient machines)\n - Paths including virtual disks must be mounted equally at both host and container (e.g. '/mnt/user/domains:/mnt/user/domains')\n - If additional disks are under different paths, these has to be mounted in similar way as the above example\n - All paths and virtual disks must have both read and write permissions\n"

        if [[ $(os_is_unraid) == true ]]; then

            local vm_disks_issues_subject="Issues with disk image path(s)"
            local vm_disks_issues_message="Domain(s): ${domain_failed_list[@]} could not be put on backups schedule because issues were found with its disk image path(s)"

            unraid_notify "warning" "VM-Babysitter" "$vm_disks_issues_subject" "$vm_disks_issues_message$unraid_logcheck_message"
        fi
    fi
}

#------------------------------------------------------------------------------
# Checks VMs in CHECK_BACKUPS_LIST for backup chain's health, permitting to keep using correct ones,
# managing broken ones according the scenario (archiving restorable backups),
# and preparing VMs to create a new backup chains.
# Optionally can verify backups integrity (if enabled by user):
#------------------------------------------------------------------------------
check_backups()
{
    # Local variables used for messages and flow control:
    local backup_chain_integrity
    local backup_folder_integrity
    local checkpoints_found
    local crashed_unraid_scenario
    local delete_bitmap_exit_code
    local delete_checkpoint_exit_code
    local existing_bitmaps_list
    local existing_images_list
    local folder_checkpoints_list
    local no_backup_chain
    local shutdown_exit_code
    local domains_running_list
    local qemu_checkpoints_list
    local recoverable_backup_chain
    local resulting_check_backup_list
    local shutdown_required_subject
    local shutdown_required_message
    local verify_integrity_exit_code

    # Lists to add VMs (and summarize at the end):
    local broken_backup_chain_list
    local domain_failed_list
    local domain_shutdown_needed_list
    local domain_shutdown_success_list
    local preserved_backup_chain_list

   echo "____________________________________________________________________________________________________________"
   echo -e "Checking backup chains integrity..."

    # CHECK_BACKUPS_LIST is copied into this list, to be processed instead:
    resulting_check_backup_list=(${CHECK_BACKUPS_LIST[@]})

    if [[ $(os_is_unraid) == true ]]; then

        # OS is Unraid. Check for crashed_unraid_scenario, when comes from a crash and VMs were running before this container started:
        for domain in $(domains_list); do

            # Look for all VMs and its checkpoints, stop if it finds at least one:
            [[ -n $(domain_checkpoints_list $domain) ]] && { checkpoints_found=true; break; } || checkpoints_found=false
        done

        for domain in ${CHECK_BACKUPS_LIST[@]}; do

            # Look for all VMs to be checked, add all those running:
            [[ $(domain_state $domain) == "running" ]] && domains_running_list+=($domain)
        done

        if [[ $checkpoints_found == false && -n ${domains_running_list[@]} ]]; then

            # Set crashed_unraid_scenario flag and notify:
            crashed_unraid_scenario=true

            echo -e "\nNOTICE: This Unraid server has either been restarted recently, or this is $(basename $0)'s 1st run.\n\nAssuming the possibility of this server might have crashed before, all running domains (except listed in env var VM_IGNORED_LIST) will require a controlled powercycle, in order to check and fix any issue found into the backup chain. Refer to documentation for a more detailed explanation."

            if [[ -z $VM_ALLOW_POWERCYCLE ]]; then

                # Nothing can be done until the user shut down all running VMs:
                for domain in ${domains_running_list[@]}; do

                    # Remove from resulting_check_backup_list and rebuild the array:
                    unset resulting_check_backup_list[$(item_position $domain "resulting_check_backup_list")]
                    resulting_check_backup_list=(${resulting_check_backup_list[@]})

                    # Include into VMs where user action is required:
                    domain_shutdown_needed_list+=($domain)
                    echo "$domain: Manual shut down is required"
                done

                local shutdown_required_subject="Manual Shut Down is Required!"
                local shutdown_required_message="Please power off the following domain(s): ${domain_shutdown_needed_list[@]}. This action is mandatory before to put it into backup schedule. Once performed, powercycle will be completed automatically"

                echo -e "\nWARNING: $shutdown_required_subject $shutdown_required_message\n"

                echo -e "\nEnv var VM_ALLOW_POWERCYCLE is not set\nTo prevent the need of user intervention under this scenario in the future, proceed with one of the following solutions:\n - Set env var VM_ALLOW_POWERCYCLE and allow $(basename $0) to powercycle domains automatically\n - Or disable autostart of domains intended to be backed up, and use env vars VM_AUTOSTART_LIST and VM_IGNORED_LIST instead"

                # Notify via Unraid:
                unraid_notify "warning" "VM-Babysitter" "$shutdown_required_subject" "$shutdown_required_message$unraid_logcheck_message"

            else
                # It's allowed to perform a power cycle on VMs:
                echo -e "\nEnv var VM_ALLOW_POWERCYCLE is set, $(basename $0) will attempt to perform a controlled powercycle of the running domain(s)...\n"

                for domain in ${domains_running_list[@]}; do

                    # (Re)initialize used variables:
                    shutdown_exit_code=""

                    echo ""

                    # Shut down the domain and await for it, for an exit code:
                    domain_shutdown $domain --wait $VM_WAIT_TIME
                    shutdown_exit_code=$?

                    if [[ $shutdown_exit_code -eq 0 ]]; then

                        # Schedule domain for start once the upcoming proccess had finished:
                        domain_shutdown_success_list+=($domain)

                    else
                        # This VM may either have issues, or just taking too long into shut down and timed out waiting.
                        # Remove from resulting_check_backup_list and rebuild the array:
                        unset resulting_check_backup_list[$(item_position $domain "resulting_check_backup_list")]
                        resulting_check_backup_list=(${resulting_check_backup_list[@]})

                        # Include into VMs where user action may be required:
                        domain_shutdown_needed_list+=($domain)
                    fi
                done

                if [[ -n ${domain_shutdown_needed_list[@]} ]]; then

                    local shutdown_required_subject="A Manual Shut Down Might be Required"
                    local shutdown_required_message="Domain(s): ${domain_shutdown_needed_list[@]}, are taking more than $VM_WAIT_TIME secs into shut down completely. If no further notice about powercycle success is received into the next minutes, consider to examine and if necessary, perform a manual shut down (powercycle will be completed automatically)"

                    echo -e "\nWARNING: $shutdown_required_subject $shutdown_required_message\n"

                    # Notify via Unraid:
                    unraid_notify "warning" "VM-Babysitter" "$shutdown_required_subject" "$shutdown_required_message$unraid_logcheck_message"
                fi
            fi

        else
            # No crashed_unraid_scenario to mind about:
            crashed_unraid_scenario=false
        fi
    fi

    for domain in ${resulting_check_backup_list[@]}; do

        # (Re)initialize used variables:
        backup_chain_integrity=""
        backup_folder_integrity=""
        delete_bitmap_exit_code=""
        delete_checkpoint_exit_code=""
        existing_images_list=""
        folder_checkpoints_list=()
        no_backup_chain=""
        recoverable_backup_chain=""
        verify_integrity_exit_code=""

        echo ""

        #------------------------------------------------------------------------------
        # Stage 1: Check backup integrity:
        #------------------------------------------------------------------------------

        # Discard non-existing, and empty, and early failed (with only logs) folders:
        #------------------------------------------------------------------------------
        if [[ ! -d $LOCAL_BACKUP_PATH/$domain \
            || -z $(ls -A $LOCAL_BACKUP_PATH/$domain) \
            || -z $(list_extensions $LOCAL_BACKUP_PATH/$domain | grep -iv log) ]]; then

            backup_folder_integrity=false
            echo "$domain: Unexistent or empty backup chain folder '$LOCAL_BACKUP_PATH/$domain'"

        # Interrupted operations (may recover backup chains when has more than one checkpoint):
        #------------------------------------------------------------------------------
        elif [[ -n $(find $LOCAL_BACKUP_PATH/$domain -type f -name "*.partial") ]]; then

            folder_checkpoints_list=($(backup_checkpoints_list $LOCAL_BACKUP_PATH/$domain))

            if [[ ${#folder_checkpoints_list[@]} -le 1 ]]; then

                # Backup is useless:
                backup_folder_integrity=false
                echo "$domain: A full/copy backup operation was previously cancelled. This backup is unrecoverable, therefore will be removed"

            elif [[ -z $VM_ALLOW_BACKUP_CHAIN_FIX || $crashed_unraid_scenario == true ]]; then

                # Backup chain may be recoverable, but either user didn't allow to check, or it's crashed_unraid_scenario:
                recoverable_backup_chain=false
                echo "$domain: A backup operation was previously cancelled. Backup chain is partially recoverable, but will not attempt to fix the backup for further use"

            else
                # Backup chain may be recoverable, and will attempt to fix it:
                recoverable_backup_chain=true
                echo "$domain: A backup operation was previously cancelled. Backup is partially recoverable and it will attempt to fix it and restore the current backup chain for further use"
            fi

        # If backup was made in 'copy' mode, there are no checkpoints or bitmaps in this VM to delete (and no checkpoints can be added anyway):
        #------------------------------------------------------------------------------
        elif [[ ! -f $LOCAL_BACKUP_PATH/$domain/$domain.cpt \
        && $(backup_checkpoints_list $LOCAL_BACKUP_PATH/$domain --num) -eq 0 \
        && -n $(find $LOCAL_BACKUP_PATH/$domain -type f -name "*.copy.data") ]]; then

            no_backup_chain=true
            echo "$domain: Current backup was made in 'copy' mode, and cannot be used for incremental backups"

        else # Backup folder integrity looks 'OK', at a first glance

            # Perform data check integrity when enabled by user:
            #------------------------------------------------------------------------------
            if [[ -n $CHECK_BACKUPS_INTEGRITY ]]; then

                verify_backup_chain $LOCAL_BACKUP_PATH/$domain
                verify_integrity_exit_code=$?
            fi

            # When has passed all tests above (including integrity check, if enabled),
            # then is approved for further checks:
            [[ $verify_integrity_exit_code != 1 ]] && backup_folder_integrity=true
        fi

        #------------------------------------------------------------------------------
        # Stage 2: Verify backup chain integrity on backup folders without issues or limitations:
        #------------------------------------------------------------------------------
        if [[ $backup_folder_integrity == true ]]; then

            echo "$domain: Backup folder integrity of '$LOCAL_BACKUP_PATH/$domain' appears to be OK"

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoints_list $domain))

            # Read backup checkpoints:
            folder_checkpoints_list=($(backup_checkpoints_list $LOCAL_BACKUP_PATH/$domain))

            if [[ $(domain_state $domain) == "shut off" ]]; then

                # Checking in-backup checkpoints vs. disk image bitmaps it's the most accurate way to determine backup chain integrity.
                # Only can be done if VM is shut down (and the only possible way when crashed_unraid_scenario applies):
                echo "$domain: Is shut off. Performing a full check into its backup chain..."

                # Acquire the list of disk image path(s) contained for this VM:
                existing_images_list=($(domain_img_paths_list $domain))

                for image in ${existing_images_list[@]}; do

                    # Read bitmaps in each virtual image:
                    existing_bitmaps_list=($(disk_image_bitmaps_list $image))

                    if [[ ${existing_bitmaps_list[@]} == ${folder_checkpoints_list[@]} ]]; then

                        # Backup chain is OK, mark as preserved:
                        backup_chain_integrity=true
                        echo "$domain: Backup checkpoints and bitmaps lists on disk $image match (${#existing_bitmaps_list[@]} vs ${#folder_checkpoints_list[@]})"

                    else
                        backup_chain_integrity=false
                        echo "$domain: Backup checkpoints and bitmaps lists on disk $image do not match! (${#existing_bitmaps_list[@]} vs ${#folder_checkpoints_list[@]})"

                        # When more than one disk image to check, notify the check has been cancelled:
                        [[ ${#existing_images_list[@]} -gt 1 ]] && echo "$domain: Backup check operation has been cancelled"

                        # Interrupt the current operation. Nothing else to do:
                        break
                    fi
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                # When VM is running, bitmaps created during a backup chain update aren't visible with qemu-img info until VM has not been shut down.
                # The only reliable method it's to compare in-backup vs QEMU checkpoints count (and this can't be done when crashed_unraid_scenario applies):
                echo "$domain: Is running. Comparing QEMU and Backup's checkpoints lists..."

                if [[ ${qemu_checkpoints_list[@]} == ${folder_checkpoints_list[@]} ]]; then

                    # Backup chain integrity is OK:
                    backup_chain_integrity=true
                    echo "$domain: QEMU and backup checkpoints lists match (${#qemu_checkpoints_list[@]} vs ${#folder_checkpoints_list[@]})"

                else
                    # Mark the backup chain status as broken:
                    backup_chain_integrity=false
                    echo "$domain: QEMU and backup checkpoints lists do not match! ${#qemu_checkpoints_list[@]} vs ${#folder_checkpoints_list[@]}"
                fi
            fi

        elif [[ $backup_folder_integrity == false ]]; then

            # Delete non-recoverable backup folder:
            rm -rf $LOCAL_BACKUP_PATH/$domain
        fi

        #------------------------------------------------------------------------------
        # Stage 3: Process the results, according with all previous checks (attempting to fix recoverable issues, if possible):
        #------------------------------------------------------------------------------

        if [[ $backup_chain_integrity == true ]]; then

            # At this point, all checks has been successful and backup chain is ready for further use.
            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)

        elif [[ $no_backup_chain == true ]]; then

            # This is a 'copy' backup. As neither checkpoints nor bitmaps are present,
            # and no checkpoints can't be added; doesn't require further comprobations.
            # Add VM to the list of broken backup chain ones:
            broken_backup_chain_list+=($domain)

        elif [[ $recoverable_backup_chain == true ]]; then

            # This usually happens when container is stopped or restarted in the middle of a backup operation, leaving an incomplete -and unusable- backup chain.
            # By deleting last checkpoint & bitmap from VM, and files generated by virtnbdbackup before the interruption,
            # backup chain ends at the latest successful backup state, and can receive more updates.

            # TODO: Exit this process early if issues are found during backup fix.

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoints_list $domain))

            if [[ $(domain_state $domain) == "shut off" ]]; then

                echo "$domain: Removing damaged checkpoint metadata: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoints $domain ${qemu_checkpoints_list[-1]} --metadata

                for image in $(domain_img_paths_list $domain); do

                    # Read bitmaps in each virtual image:
                    existing_bitmaps_list=($(disk_image_bitmaps_list $image))

                    echo "$domain: Deleting insecure bitmap ${existing_bitmaps_list[-1]} on disk image $image ..."
                    disk_image_bitmaps_delete $image ${existing_bitmaps_list[-1]}
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                echo "$domain: Removing damaged checkpoint: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoints $domain ${qemu_checkpoints_list[-1]}
            fi

            echo "$domain: Fixing backup chain data in $LOCAL_BACKUP_PATH/$domain ..."

            # Deletes any related file with the damaged checkpoint:
            find $LOCAL_BACKUP_PATH/$domain -name "*${qemu_checkpoints_list[-1]}*" -type f -delete

            # Deletes the last checkpoint from the list:
            unset qemu_checkpoints_list[-1]

            # Turn the updated list into string to export:
            qemu_checkpoints_list="${qemu_checkpoints_list[@]}"

            # Rebuilds the cpt file with the parsed new list of checkpoints:
            echo "[\"${qemu_checkpoints_list// /\", \"}\"]" > $LOCAL_BACKUP_PATH/$domain/$domain.cpt

            # Backup folder integrity is OK:
            backup_folder_integrity=true

            # Backup chain integrity is assumed as OK:
            backup_chain_integrity=true

            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)
            echo "$domain: Backup chain was fixed by removing latest checkpoint and deleting incomplete data at '$LOCAL_BACKUP_PATH/$domain'"

        else # All other scenarios, where checking backup folder and/or chain failed...

            # Delete checkpoints and/or bitmaps, depending on the case:
            if [[ $(domain_state $domain) == "shut off" ]]; then

                # Deletes checkpoints metadata, if any detected:
                echo "$domain: Pruning any existing checkpoints metadata..."
                domain_delete_checkpoints $domain --all --metadata
                delete_checkpoint_exit_code=$?

                for image in $(domain_img_paths_list $domain); do

                    # Then deletes all bitmaps, in all image disks:
                    echo "$domain: Pruning any existing bitmaps on disk image $image ..."

                    disk_image_bitmaps_delete $image --all
                    delete_bitmap_exit_code=$?

                    # When issues are found deleting bitmaps, cancel the entire operation:
                    [[ $delete_bitmap_exit_code -eq 1 ]] && break
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                # Deletes checkpoints (and bitmaps) if any detected:
                echo "$domain: Pruning any existing checkpoints..."
                domain_delete_checkpoints $domain --all
                delete_checkpoint_exit_code=$?
            fi

            # Assign VM and/or backup chain to the correspondent list, according the results:
            case $delete_checkpoint_exit_code in

                0|2)
                    if [[ $(domain_state $domain) == "shut off" ]]; then

                        case $delete_bitmap_exit_code in

                            0|2)    # Checkpoints metadata and bitmaps deleted successfully (or nothing to delete).
                                    # Add to broken backup chain list:
                                    broken_backup_chain_list+=($domain)
                            ;;

                            *)  # There were problems when deleting bitmaps.
                                # This VM has issues and user action is required:
                                domain_failed_list+=($domain)
                            ;;
                        esac

                    elif [[ $(domain_state $domain) == "running" ]]; then

                        # Checkpoints deleted successfully (or nothing to delete).
                        # Add to broken backup chain list:
                        broken_backup_chain_list+=($domain)
                    fi
                ;;

                1)  # There were problems when deleting checkpoints.
                    # This VM has issues and user action is required:
                    domain_failed_list+=($domain)
                ;;
            esac
        fi
    done

    # Appends broken backup chains for new backup chain creation:
    CREATE_BACKUP_CHAIN_LIST+=(${broken_backup_chain_list[@]})

    # Appends successfully shut down VMs to the global list in need to be started:
    POWERON_REQUIRED_VMS_LIST+=(${domain_shutdown_success_list[@]})

    # Appends preserved backup chains to scheduled backups directly:
    BACKUPS_TO_SCHEDULE_LIST+=(${preserved_backup_chain_list[@]})

    # Appends VMs failed to shutdown (by user's setting or by fail) to request user action:
    SHUTDOWN_REQUIRED_VMS_LIST+=(${domain_shutdown_needed_list[@]})

    # Appends failed to shut down VMs to failed VMs list:
    VMS_TO_DISCARD_LIST+=(${domain_failed_list[@]})

    # Export definitive results, so other scripts have updated lists (e.g. scheduled backups):
    #------------------------------------------------------------------------------

    # Reload the external variables file:
    source $external_vars

    # Add preserved backup chains to global SCHEDULED_BACKUPS_LIST:
    SCHEDULED_BACKUPS_LIST+=(${BACKUPS_TO_SCHEDULE_LIST[@]})

    # And Vms with issues to global FAILED_VMS_LIST:
    FAILED_VMS_LIST+=(${VMS_TO_DISCARD_LIST[@]})

    # Sed can't expand arrays correctly. Convert variables to be exported into strings.
    # This will be reverted at next block iteration, by sourcing such external variables as arrays:
    FAILED_VMS_LIST="${FAILED_VMS_LIST[@]}"
    SCHEDULED_BACKUPS_LIST="${SCHEDULED_BACKUPS_LIST[@]}"

    # Export the variables:
    sed -i \
    -e "s/FAILED_VMS_LIST=.*/FAILED_VMS_LIST=($FAILED_VMS_LIST)/" \
    -e "s/SCHEDULED_BACKUPS_LIST=.*/SCHEDULED_BACKUPS_LIST=($SCHEDULED_BACKUPS_LIST)/" \
    $external_vars

    # Notify about issues and/or errors, if any:
    #------------------------------------------------------------------------------

    if [[ -n ${domain_failed_list[@]} ]]; then

        # check_backups found unexpected issues:
        local backup_chain_issues_subject="Something went unexpectedly wrong when checking backup chain integrity!"
        local backup_chain_issues_message="Domain(s): ${domain_failed_list[@]} could not be put into backups schedule because something went unexpectedly wrong while checking backup integrity"

        echo -e "\nWARNING: $backup_chain_issues_subject $backup_chain_issues_message\n"

        [[ $(os_is_unraid) == true ]] \
            && unraid_notify "warning" "VM-Babysitter" "$backup_chain_issues_subject" "$backup_chain_issues_message$unraid_logcheck_message"
    fi
}

#------------------------------------------------------------------------------
# Checks domains in POWERON_REQUIRED_VMS_LIST and VM_AUTOSTART_LIST, and attempts to start them.
#------------------------------------------------------------------------------
autostart_domains()
{
    local autostart_success_list=()
    local powercycle_success_list=()
    local start_exit_code
    local start_failed_list=()

    local i=0

    echo "____________________________________________________________________________________________________________"
    echo -e "Starting domains..."

    for domain in ${POWERON_REQUIRED_VMS_LIST[@]}; do

        # (Re)initialize used variables:
        start_exit_code=""

        echo ""

        # Attempt to turn on the VM. Do not wait for Guest's QEMU agent:
        domain_start $domain --nowait

        if [[ $start_exit_code -eq 0 ]]; then

            # Put VM into powercycle success list:
            powercycle_success_list+=($domain)

        else
            # Put VM into failed list:
            start_failed_list+=($domain)

            # Remove from POWERON_REQUIRED_VMS_LIST:
            unset POWERON_REQUIRED_VMS_LIST[$i]

            # Rebuild indexes into the array:
            POWERON_REQUIRED_VMS_LIST=${POWERON_REQUIRED_VMS_LIST[@]}
        fi

        # Increases the counter:
        ((i++))
    done

    if [[ $FIRST_AUTOSTART_COMPLETED != true ]]; then

        for domain in $VM_AUTOSTART_LIST; do

            # (Re)initialize used variables:
            start_exit_code=""

            echo ""

            if [[ -z $(item_position $domain "powercycle_success_list") \
                && -z $(item_position $domain "powercycle_failed_list") \
                && $(domain_state $domain) != "running" ]]; then

                # VM is in VM_AUTOSTART_LIST, has not been marked for powercycle, and is not running.
                # Attempt to turn on the VM. Do not wait for Guest's QEMU agent:
                domain_start $domain --nowait

                if [[ $start_exit_code -eq 0 ]]; then

                    # Put VM into autostart success list:
                    autostart_success_list+=($domain)

                else
                    # Put VM into failed list:
                    start_failed_list+=($domain)
                fi
            fi
        done

        # Ensure the automatic start of VMs is executed once per script run:
        FIRST_AUTOSTART_COMPLETED=true
    fi

    # Add any VM failed to start to VMS_TO_DISCARD_LIST:
    VMS_TO_DISCARD_LIST+=(${start_failed_list[@]})

    # Export definitive results, so other scripts have updated lists (e.g. future re-scan schedule of previously failed VMs):
    #------------------------------------------------------------------------------

    # Reload the external variables file:
    source $external_vars

    # And Vms with issues to global FAILED_VMS_LIST:
    FAILED_VMS_LIST+=(${VMS_TO_DISCARD_LIST[@]})

    # Sed can't expand arrays correctly. Convert variables to be exported into strings.
    # This will be reverted at next block iteration, by sourcing such external variables as arrays:
    FAILED_VMS_LIST="${FAILED_VMS_LIST[@]}"

    # Export the variables:
    sed -i \
    -e "s/FAILED_VMS_LIST=.*/FAILED_VMS_LIST=($FAILED_VMS_LIST)/" \
    $external_vars

    # Notify about events and/or errors, if any:
    #------------------------------------------------------------------------------
    if [[ -n ${autostart_success_list[@]} ]]; then

        autostart_performed_subject="User Configured Autostart Completed"
        autostart_performed_mesage="Domain(s): ${autostart_success_list[@]} started successfully"

        echo -e "\nNOTICE: $autostart_performed_subject\n$autostart_performed_mesage\n"

        [[ $(os_is_unraid) == true ]] \
            && unraid_notify "normal" "VM-Babysitter" "$autostart_performed_subject" "$autostart_performed_mesage"
    fi

    if [[ -n ${powercycle_success_list[@]} ]]; then

        if [[ -z $VM_ALLOW_POWERCYCLE ]]; then

            powercycle_performed_subject="Manual Powercycle Completed"
            powercycle_performed_mesage="Domain(s): ${powercycle_success_list[@]}, that went shut off by the user for backup check, already started successfully"

        else
            powercycle_performed_subject="Automatic Powercycle Completed"
            powercycle_performed_mesage="Domain(s): ${powercycle_success_list[@]}, that went shut off automatically for backup check, already started successfully"
        fi

        echo -e "\nNOTICE: $powercycle_performed_subject\n$powercycle_performed_mesage\n"

        [[ $(os_is_unraid) == true ]] \
            && unraid_notify "normal" "VM-Babysitter" "$powercycle_performed_subject" "$powercycle_performed_mesage"
    fi

    if [[ -n ${start_failed_list[@]} ]]; then

        failed_start_subject="Autostart or Powercycle Failed"
        failed_start_mesage="Something went wrong while performing autostart or powercycle of domain(s): ${start_failed_list[@]}"

        echo -e "\nWARNING: $failed_start_subject\n$failed_start_mesage\n"

        [[ $(os_is_unraid) == true ]] \
            && unraid_notify "warning" "VM-Babysitter" "$failed_start_subject" "$failed_start_mesage$unraid_logcheck_message"
    fi
}

###############################################################################
# Main execution:
###############################################################################

# User variables defaults (details on README.md):
#------------------------------------------------------------------------------
BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-"@daily"}
CHECK_BACKUPS_INTEGRITY=${CHECK_BACKUPS_INTEGRITY:-""}
LOCAL_BACKUP_CHAINS_TO_KEEP=${LOCAL_BACKUP_CHAINS_TO_KEEP:-"1"}
LOCAL_BACKUP_PATH=${LOCAL_BACKUP_PATH:-"/backups"}
LOGFILE_PATH=${LOGFILE_PATH:-"/logs/vm-babysitter.log"}
SCHEDULE_LOGFILE_PATH=${SCHEDULE_LOGFILE_PATH:-"/logs/scheduled-tasks.log"}
LOGROTATE_CONFIG_PATH=${LOGROTATE_CONFIG_PATH:-"/tmp/logrotate.d/vm-babysitter"}
LOGROTATE_SCHEDULE=${LOGROTATE_SCHEDULE:-"@daily"}
LOGROTATE_SETTINGS=${LOGROTATE_SETTINGS:-"  compress\n  copytruncate\n  daily\n  dateext\n  dateformat .%Y-%m-%d.%H:%M:%S\n  missingok\n  notifempty\n  rotate 30"}
MAX_BACKUPS_PER_CHAIN=${MAX_BACKUPS_PER_CHAIN:-"30"}
RSYNC_ARGS=${RSYNC_ARGS:-"-a"}
RSYNC_BACKUP_CHAINS_TO_KEEP=${RSYNC_BACKUP_CHAINS_TO_KEEP:-"2"}
RSYNC_BACKUP_PATH=${RSYNC_BACKUP_PATH:-""}
RSYNC_SCHEDULE=${RSYNC_SCHEDULE:-""}
SSH_OPTIONS=${SSH_OPTIONS:-"-q -o IdentityFile=/private/hostname.key -o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"}
[[ -f /usr/share/zoneinfo/$TZ ]] && local_timezone_file="/usr/share/zoneinfo/$TZ"
UNRAID_NOTIFY_HOST=${UNRAID_NOTIFY_HOST:-"localhost"}
VIRTNBDBACKUP_ARGS=${VIRTNBDBACKUP_ARGS:-""}
VM_ALLOW_BACKUP_CHAIN_FIX=${VM_ALLOW_BACKUP_CHAIN_FIX:-""}
VM_ALLOW_POWERCYCLE=${VM_ALLOW_POWERCYCLE:-""}
VM_AUTOSTART_LIST=${VM_AUTOSTART_LIST:-""}
VM_IGNORED_LIST=${VM_IGNORED_LIST:-""}
VM_WAIT_TIME=${VM_WAIT_TIME:-"60"}

# Internal Variables:
#------------------------------------------------------------------------------

# Temporal crontab file (to be loaded for cron)
crontab_file="/tmp/crontab"

# Storing file for bash like lists shared between this script and the scheduler:
external_vars="/tmp/vm-babysit-vars"

# Export variables used by children processes:
export external_vars

# Redirect system stdout and stderr to $LOGFILE_PATH. Note this applies to all other commands and scripts run from this script:
exec &> >(tee -a $LOGFILE_PATH)

# Create internal folders in case don't exist:
[[ ! -d $( dirname $LOGFILE_PATH) ]] && mkdir -p $(dirname $LOGFILE_PATH)

# Sets the local timezone (if ENV TZ was set):
if [[ -n $local_timezone_file ]]; then

    ln -fs $local_timezone_file /etc/localtime
    &> /dev/null dpkg-reconfigure -f noninteractive tzdata
fi

# This string is appended to some Unraid messages:
unraid_logcheck_message=". Check container logs, or $(basename $LOGFILE_PATH) for details."

# Catches the signal sent from docker to stop execution:
# The most gracefully way to stop this container is with:
# 'docker kill --signal=SIGTERM <docker-name-or-id>'
trap 'stop_container' SIGTERM

echo "############################################################################################################"
echo "$(basename $0) started at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))"
echo "############################################################################################################"

#------------------------------------------------------------------------------
# 1. Check input parameters (exits on error)
#------------------------------------------------------------------------------
check_variables
check_variables_exit_code=$?

#------------------------------------------------------------------------------
# 2. Only when input parameters doesn't require to restart the container, it continues the rest of the checks:
#------------------------------------------------------------------------------

if [[ $check_variables_exit_code -eq 0 ]]; then

    # Initializes a file with variables externally stored, to be shared with the scheduler:
    #------------------------------------------------------------------------------
    cat << end_of_external_variables > $external_vars
# Shared values between scripts. DO NOT EDIT!:
FAILED_VMS_LIST=()
SCHEDULED_BACKUPS_LIST=()
end_of_external_variables

    # Create logrotate config:
    #------------------------------------------------------------------------------

    echo -e "\nConfiguring logrotate..."

    # Create directory if doesn't exist:
    mkdir -p $(dirname $LOGROTATE_CONFIG_PATH)

    # Parse logrotate configuration (overwriting, if any previous):
    echo -e "$LOGFILE_PATH \n $SCHEDULE_LOGFILE_PATH {\n$LOGROTATE_SETTINGS\n}" > $LOGROTATE_CONFIG_PATH

    # Create/update Cron task for VMs to be (progressively) included in $scheduled_backups_list:
    #------------------------------------------------------------------------------

    echo -e "Configuring scheduled tasks..."

    # Silently deletes any previous cron task:
    &> /dev/null crontab -r

    # Parses the actual cron task needed to run to $crontab_file
    # (Including ENV vars not being read from cron's environment):
    cat << base_crontab > $crontab_file
# Values below are refreshed upon container (re)start:

# Main environment is bash:
SHELL=/bin/bash

# Search paths for binaries and scripts:
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# Environment variables used by scheduled tasks:
LOCAL_BACKUP_CHAINS_TO_KEEP="$LOCAL_BACKUP_CHAINS_TO_KEEP"
LOCAL_BACKUP_PATH="$LOCAL_BACKUP_PATH"
LOGFILE_PATH="$LOGFILE_PATH"
MAX_BACKUPS_PER_CHAIN="$MAX_BACKUPS_PER_CHAIN"
RSYNC_ARGS="$RSYNC_ARGS"
RSYNC_BACKUP_CHAINS_TO_KEEP="$RSYNC_BACKUP_CHAINS_TO_KEEP"
RSYNC_BACKUP_PATH="$RSYNC_BACKUP_PATH"
RSYNC_SCHEDULE=$RSYNC_SCHEDULE
SCHEDULE_LOGFILE_PATH="$SCHEDULE_LOGFILE_PATH"
SSH_OPTIONS="$SSH_OPTIONS"
UNRAID_NOTIFY_HOST="$UNRAID_NOTIFY_HOST"
VIRTNBDBACKUP_ARGS="$VIRTNBDBACKUP_ARGS"
VM_WAIT_TIME=$VM_WAIT_TIME

# Paths for functions and (shared) dynamic variables:
external_vars="$external_vars"

# Schedule for logrotate:
$LOGROTATE_SCHEDULE /usr/sbin/logrotate $LOGROTATE_CONFIG_PATH >> $SCHEDULE_LOGFILE_PATH 2>&1

# Schedule for VM backups:
$BACKUP_SCHEDULE /usr/local/bin/vm-backup --run-schedule >> $SCHEDULE_LOGFILE_PATH 2>&1
base_crontab

    if [[ -n $RSYNC_BACKUP_PATH && -n $RSYNC_SCHEDULE ]]; then

    # Append additional task to run Rsync on a defined schedule:
        cat << optional_crontab >> $crontab_file

# Schedule for Rsync:
$RSYNC_SCHEDULE /usr/local/bin/vm-rsync --run-schedule >> $SCHEDULE_LOGFILE_PATH 2>&1
optional_crontab
    fi

    # Sets the cron task:
    crontab $crontab_file

    # Finally, runs cron and sends to background:
    echo -e "Starting Cron..."
    cron -f -l -L2 &

    # 3. Begin monitorization for VMs in lists, performing operations as required:
    #------------------------------------------------------------------------------
    echo -e "Babysitter mode started...\n"

    # Creates DOMAINS_LIST, a list of non-transient domains that will start to be monitored, until fail:
    create_domains_list

    while true; do

        # Maximum standby period for monitoring should not exceed 10 seconds in any case,
        # because it could ignore SIGTERM from Docker, thus being killed with SIGKILL:
        sleep 1

        if [[ -n ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ]]; then

            # Check for VMs which are in need of shutdown.
            # (This normally happens when the user took the action, or when a VM took more than VM_WAIT_TIME secs to shutdown):
            #------------------------------------------------------------------------------
            i=0
            for domain in ${SHUTDOWN_REQUIRED_VMS_LIST[@]}; do

                if [[ $(domain_state $domain) == "shut off" ]]; then

                    # Add VM to the same list of VMs to be powered on when monitor starts:
                    POWERON_REQUIRED_VMS_LIST+=($domain)

                    # Move to main queue for check:
                    CHECK_BACKUPS_LIST+=($domain)
                    unset SHUTDOWN_REQUIRED_VMS_LIST[$i]

                    # Rebuild indexes into the array:
                    SHUTDOWN_REQUIRED_VMS_LIST=(${SHUTDOWN_REQUIRED_VMS_LIST[@]})
                fi

                # Increases the counter:
                ((i++))
            done
        fi

        if [[ -n ${DOMAINS_LIST[@]} || -n ${CHECK_BACKUPS_LIST[@]} ]]; then

            echo "============================================================================================================"
            echo -e "Changes detected at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))\n"

            # Check for virtual drives status on detected VMs:
            #------------------------------------------------------------------------------
            [[ -n ${DOMAINS_LIST[@]} ]] && check_domains

            # Check for backup chain integrity on all VMs:
            #------------------------------------------------------------------------------
            [[ -n ${CHECK_BACKUPS_LIST[@]} ]] && check_backups

            # Start all VMs that was, either declared in VM_AUTOSTART_LIST and have not started yet, or if any is in POWERON_REQUIRED_VMS_LIST:
            #------------------------------------------------------------------------------
            [[ -n ${POWERON_REQUIRED_VMS_LIST[@]} ]] || [[ -n $VM_AUTOSTART_LIST && $FIRST_AUTOSTART_COMPLETED != true ]] && autostart_domains

            # Create new backup chains for VMs that require it:
            #------------------------------------------------------------------------------
            [[ -n ${CREATE_BACKUP_CHAIN_LIST[@]} ]] && vm-backup --create-new ${CREATE_BACKUP_CHAIN_LIST[@]}

            # Reload the external variables file
            source $external_vars

            # Show a summary of changes and global status:
            #------------------------------------------------------------------------------
            echo "============================================================================================================"
            echo -e "Changed Status Summary:\n"

            echo -e "Submitted for checks:      ${DOMAINS_LIST[@]:-"None"}"
            echo -e "Autostarted / powercycled: ${POWERON_REQUIRED_VMS_LIST[@]:-"None"}"
            echo -e "Passed all the checks:     ${BACKUPS_TO_SCHEDULE_LIST[@]:-"None"}"
            echo -e "Required new backup chain: ${CREATE_BACKUP_CHAIN_LIST[@]:-"None"}"
            echo -e "Awaiting for shut down:    ${SHUTDOWN_REQUIRED_VMS_LIST[@]:-"None"}"
            echo -e "Failed during a check:     ${VMS_TO_DISCARD_LIST[@]:-"None"}"

            echo -e "\nGlobal Status (after changes):\n"

            echo -e "Total running into scheduled backups: ${SCHEDULED_BACKUPS_LIST[@]:-"None"}"
            echo -e "Total failed during checks or backup: ${FAILED_VMS_LIST[@]:-"None"}"

            # Unset variables that require a clean state for next iteration (SHUTDOWN_REQUIRED_VMS_LIST is kept, awaiting for results):
            unset DOMAINS_LIST
            unset CHECK_BACKUPS_LIST
            unset BACKUPS_TO_SCHEDULE_LIST
            unset CREATE_BACKUP_CHAIN_LIST
            unset POWERON_REQUIRED_VMS_LIST
            unset VMS_TO_DISCARD_LIST

            echo -e "\nAll changes ended to be processed at $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))\n"
        fi

        # Restarts the loop, until SIGTERM or SIGKILL are received from Docker...
        #------------------------------------------------------------------------------
    done

else
    # Stop the container when check_variables ends with code 1:
    stop_container
fi
