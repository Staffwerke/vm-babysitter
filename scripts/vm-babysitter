#!/bin/bash

# Load functions:
source /usr/local/bin/vm-functions

###############################################################################
# Specific procedures:
###############################################################################

#------------------------------------------------------------------------------
# Attempts to stop the container gracefully in case of receive SIGTERM signal from Docker:
#------------------------------------------------------------------------------
stop_container()
{
    echo "############################################################################################################"
    echo "SIGTERM signal received at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))"
    # TODO: Terminate or kill background processes before to exit.
    echo "Container Stopped."
    echo ""
    exit 0
}

#------------------------------------------------------------------------------
# Checks environment variables and return success or fail if everything is in order:
#------------------------------------------------------------------------------
check_variables()
{
    # Starts check assuming failed statuses:
    local variables_exit_code
    local local_backup_path_ok
    local max_backups_per_chain_ok
    local rsync_backup_path_exists
    local rsync_backup_path_ok

    echo "Checking environment variables..."
    # Check MAX_BACKUPS_PER_CHAIN
    #------------------------------------------------------------------------------
    if [[ $MAX_BACKUPS_PER_CHAIN =~ [0-9] && $MAX_BACKUPS_PER_CHAIN -ge 0 ]]; then

        # Is an integer number:
        max_backups_per_chain_ok=true
        echo "MAX_BACKUPS_PER_CHAIN set to: $MAX_BACKUPS_PER_CHAIN"

    else
        # Invalid value:
        echo "ERROR: Incorrect value for MAX_BACKUPS_PER_CHAIN: '$MAX_BACKUPS_PER_CHAIN' <- It must be a natural integer >= 0"
    fi

    # Check LOCAL_BACKUP_PATH:
    #------------------------------------------------------------------------------
    if  [[ -d $LOCAL_BACKUP_PATH ]]; then

        # LOCAL_BACKUP_PATH exists. Check for filesystem permissions:
        if  [[ -r $LOCAL_BACKUP_PATH && -w $LOCAL_BACKUP_PATH ]]; then

            # LOCAL_BACKUP_PATH has read/write permissions.
            echo "LOCAL_BACKUP_PATH set to: $LOCAL_BACKUP_PATH"

            # Check LOCAL_BACKUP_CHAINS_TO_KEEP:
            if [[ $LOCAL_BACKUP_CHAINS_TO_KEEP =~ [0-9] && $LOCAL_BACKUP_CHAINS_TO_KEEP -ge 0 ]]; then

                # Check passed:
                local_backup_path_ok=true
                echo "LOCAL_BACKUP_CHAINS_TO_KEEP set to: $LOCAL_BACKUP_CHAINS_TO_KEEP"

            else
                # Invalid value:
                echo "ERROR: Incorrect value for LOCAL_BACKUP_CHAINS_TO_KEEP: '$LOCAL_BACKUP_CHAINS_TO_KEEP' <- It must be a natural integer >= 0"
            fi

        else
            echo "ERROR: LOCAL_BACKUP_PATH: '$LOCAL_BACKUP_PATH' <- Insufficient filesystem permissions to work into this path"
        fi

    else
        echo "ERROR: LOCAL_BACKUP_PATH: '$LOCAL_BACKUP_PATH'  <- Path was not found inside the container. Ensure you mount this correctly (e.g. '-v /mnt/user/backups/vm-backups:/backups')"
    fi

    # Check RSYNC_BACKUP_PATH:
    #------------------------------------------------------------------------------

    case $RSYNC_BACKUP_PATH in

        '') # Rsync path is not set:

            rsync_backup_path_ok=true
            echo "RSYNC_BACKUP_PATH is not set"
        ;;

        [/]*) # Rsync path is an absolute path into root filesystem:

            # RSYNC_BACKUP_PATH exists. Check for filesystem permissions:
            if [[ -d $RSYNC_BACKUP_PATH ]]; then

                if [[ -r $RSYNC_BACKUP_PATH && -w $RSYNC_BACKUP_PATH ]]; then

                    # RSYNC_BACKUP_PATH has read/write permissions:
                    rsync_backup_path_exists=true
                    echo "RSYNC_BACKUP_PATH is local, and set to: $RSYNC_BACKUP_PATH"

                else
                    echo "ERROR: RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH' <- Insufficient filesystem permissions to work into this path"
                fi

            else
                echo "ERROR: RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH'  <- Path was not found inside the container. Ensure you mount this correctly (e.g. '-v /mnt/remotes/hostname/vm-backups-mirror:/backups-mirror')"
            fi
        ;;

        *[@]*[:/]*)  # Rsync path is at aremote host, pointing an absolute path into its root filesystem:

            # Apparently includes correct remote login and path. Separates ssh login from remote path:
            local ssh_login=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f1)
            local rsync_path=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f2)

            # Attempts to comunicate with the remote host:
            ssh $SSH_OPTIONS $ssh_login "exit 0"
            local ssh_server_exit_code=$?

            if [[ $ssh_server_exit_code == 0 ]]; then

                # Attempts to perform similar checks as with $LOCAL_BACKUP_PATH, but creating a remote backup directory if doesn't exists:
                local remote_backup_path_status=$(ssh $SSH_OPTIONS $ssh_login "[[ -d $rsync_path && -r $rsync_path && -w $rsync_path ]] && echo 'found' || { mkdir -p $rsync_path; [[ -d $rsync_path ]] && echo 'created' || echo 'failed'; }")

                if [[ $remote_backup_path_status != failed ]]; then

                    # Mark as found (also notifiy if it was created):
                    rsync_backup_path_exists=true
                    echo "RSYNC_BACKUP_PATH is remote, was $remote_backup_path_status and set to: $RSYNC_BACKUP_PATH"

                else
                    echo "ERROR: Remote RSYNC_BACKUP_PATH '$RSYNC_BACKUP_PATH' <- Insufficient filesystem permissions to create or work into this path, or is not a directory"
                fi

            else
                echo "WARNING: Connection to $ssh_login failed (SSH exit code: $ssh_exit_code)"
            fi
        ;;

        *)  # Rsync path is anything else, and considered incorrect:
            echo "ERROR: Incorrect syntax for RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH'. It must be either an absolute path, or an SSH-like endpoint that Rsync can understand (e.g. root@hostname:/path/to/rsync-backup)"
        ;;
    esac

    if [[ $rsync_backup_path_exists ]]; then

        # Verify # of backup chains to keep on Rsync endpoint:
        if [[ $RSYNC_BACKUP_CHAINS_TO_KEEP =~ [0-9] && $RSYNC_BACKUP_CHAINS_TO_KEEP -ge 0 ]]; then

            # Check passed:
            rsync_backup_path_ok=true
            echo "RSYNC_BACKUP_CHAINS_TO_KEEP set to: $RSYNC_BACKUP_CHAINS_TO_KEEP"

        else
            echo "ERROR: Incorrect value RSYNC_BACKUP_CHAINS_TO_KEEP: '$RSYNC_BACKUP_CHAINS_TO_KEEP' <- It must be a natural integer >= 0"
        fi
    fi

    # TODO: Check other ENV variables, and SSH key:

    # When all status variables passed the checks, return success:
    [[ $max_backups_per_chain_ok && $local_backup_path_ok && $rsync_backup_path_ok ]] \
        && variables_exit_code=0 \
        || variables_exit_code=1

    return $variables_exit_code
}

#------------------------------------------------------------------------------
# Populates DOMAINS_LIST, filtering ignored, into process and failed  VMs:
#------------------------------------------------------------------------------
create_domains_list()
{
    local domains_all_list=($(domains_list))
    local domains_skipped_list=(${VM_IGNORED_LIST[@]} ${FAILED_VMS_LIST[@]} ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ${POWEREDOFF_VMS_LIST[@]} ${CHECK_BACKUPS_LIST[@]} ${CREATE_BACKUP_CHAIN_LIST[@]} ${SCHEDULED_BACKUPS_LIST[@]})

    for domain in ${domains_all_list[@]}; do

        for skipped in ${domains_skipped_list[@]}; do

            [[ $domain == $skipped ]] && continue 2
        done

        DOMAINS_LIST+=($domain)
    done
}

#------------------------------------------------------------------------------
# Checks for VMs in DOMAINS_LIST by verifying existence and read/write permissions of its virtual disks,
# also marking VMs for automatic start, when required by the user:
#------------------------------------------------------------------------------
check_domains()
{
    local domain_autostart_list=()
    local domain_failed_list=()
    local domain_success_list=()

    echo ""
    echo "____________________________________________________________________________________________________________"
    echo "Checking for Virtual machines: ${DOMAINS_LIST[@]}"

    for domain in ${DOMAINS_LIST[@]}; do

        # Checks for virtual disks, if are reachable and possess both read/write permissions:
        if [[ -z $(domain_drives_list $domain) ]]; then

            # Non-transient VM without disks attached. Adds to failed list, notifies and aborts the check:
            domain_failed_list+=($domain)
            echo "WARNING: VM $domain has no drives that can be backed up, therefore will be ignored"
            break

        else
            # Does have drives able to be backed up. Checks if such disk images are reachable inside the container:
            local images_list=($(domain_img_paths_list $domain))
            for image in ${images_list[@]}; do

                if [[ ! -f $image ]]; then

                    domain_failed_list+=($domain)
                    echo "ERROR: $domain's disk image: $image not found. Ensure you mount -and mirror- this correctly inside the container (e.g. '-v /common/path/to/vms/disks:/common/path/to/vms/disks')"

                elif [[ ! -r $image || ! -w $image ]]; then

                    domain_failed_list+=($domain)
                    echo "ERROR: $domain's disk image: $image has permission issues (cannot be read or written)"

                else
                    # Check passed:
                    domain_success_list+=($domain)

                    for autostart_vm in $VM_AUTOSTART_LIST; do

                        if [[ $domain == $autostart_vm ]]; then

                            # If the VM is in VM_AUTOSTART_LIST, requires to be started as soon as possible:
                            domain_autostart_list+=($domain)
                            echo "$domain: Into VM_AUTOSTART_LIST, therefore will be started, (after initial check)"
                            break
                        fi
                    done
                fi
            done
        fi
    done

    # Cleanse DOMAINS_LIST, since all VMs were processed, and copied into correspondent lists:
    unset DOMAINS_LIST

    # Appends successfully checked VMs for checking backups:
    CHECK_BACKUPS_LIST+=${domain_success_list[@]}

    # Appends VMs with unexistent, unreachable or virtual disks with permission issues:
    FAILED_VMS_LIST+=${domain_failed_list[@]}

    # Appends successfully checked VMs to the global list in need to be started:
    POWEREDOFF_VMS_LIST+=${domain_autostart_list[@]}

    echo ""
    echo "Virtual Machines Check Summary:"
    echo ""
    echo "Ready for Backup Check Integrity:" ${domain_success_list[@]:-"None"}
    echo "Set for Autostart (after checks):" ${domain_autostart_list[@]:-"None"}
    echo ""
    [[ -z ${domain_failed_list[@]} ]] && \
    echo "All Virtual Machines Checked!" \ ||
    echo "Missing/issues on virtual disks:"  ${domain_failed_list[@]:-"None"}
}

#------------------------------------------------------------------------------
# Checks VMs in CHECK_BACKUPS_LIST for backup chain's health, permitting to keep using correct ones,
# managing broken ones according the scenario (archiving restorable backups),
# and preparing VMs to create a new backup chains.
# Optionally can verify backups integrity (if enabled by user):
#------------------------------------------------------------------------------
check_backups()
{
    # Local variables used for flow control:
    local backup_chain_integrity
    local backup_folder_integrity
    local checkpoints_found
    local delete_bitmap_exit_code
    local delete_checkpoint_exit_code
    local existing_bitmaps_list
    local existing_images_list
    local folder_checkpoints_list
    local no_backup_chain
    local shutdown_exit_code
    local some_vm_is_running
    local qemu_checkpoints_list
    local recoverable_backup_chain
    local unraid_scenario
    local verify_integrity_exit_code

    # Lists to add VMs (and summarize at the end):
    local broken_backup_chain_list
    local domain_failed_list
    local domain_shutdown_needed_list
    local domain_shutdown_success_list
    local preserved_backup_chain_list

    if [[ $(os_is_unraid) == true ]]; then

        # OS is Unraid. Check for unraid_scenario, when comes from a crash and VMs were running before this container started:
        for domain in $(domains_list); do

            # Look for all VMs and its checkpoints, stop if it finds at least one:
            [[ -n $(domain_checkpoints_list $domain) ]] && { checkpoints_found=true; break; } || checkpoints_found=false
        done

        for domain in ${CHECK_BACKUPS_LIST[@]}; do

            # Look for all VMs to be checked, stop if any of them is running:
            [[ $(domain_state $domain) == "running" ]] && { some_vm_is_running=true; break; } || some_vm_is_running=false
        done

        if [[ $checkpoints_found == false && $some_vm_is_running == true ]]; then

            # If no checkpoints were found across the server and at least one of the VMs to be checked is running, set unraid_scenario flag:
            unraid_scenario=true

            echo ""
            echo "NOTICE: Unraid OS detected, no VM in this server has QEMU checkpoints at all, and at least one VM to be checked is running"
            echo
            echo "  - This is the first run of this program after the last server restart"
            echo "  - This Unraid server crashed before this container runs"
            echo
            echo "Due to the possibility of a crash scenario, running VMs need to be checked differently than under normal operation"
            echo "Depending on your env variables, user intervention may be required, such as shutting down VMs or restarting this container manually."

        else
            # No unraid_scenario to mind about:
            unraid_scenario=false
        fi
    fi

    echo ""
    echo "____________________________________________________________________________________________________________"
    echo "Checking backup chain ingtegrity on Virtual machines: ${CHECK_BACKUPS_LIST[@]}"

    for domain in ${CHECK_BACKUPS_LIST[@]}; do

        backup_chain_integrity=""
        backup_folder_integrity=""
        delete_bitmap_exit_code=""
        delete_checkpoint_exit_code=""
        existing_images_list=""
        folder_checkpoints_list=()
        no_backup_chain=""
        shutdown_exit_code=""
        recoverable_backup_chain=""
        verify_integrity_exit_code=""

        echo ""

        #------------------------------------------------------------------------------
        # Stage 1: Check backup integrity:
        #------------------------------------------------------------------------------

        # Discard non-existing, and empty folders:
        #------------------------------------------------------------------------------
        if [[ ! -d $LOCAL_BACKUP_PATH/$domain || -z $(ls -A $LOCAL_BACKUP_PATH/$domain) ]]; then

            backup_folder_integrity=false
            echo "$domain: Unexistent or empty backup chain folder"

        # Interrupted operations (may recover backup chains when has more than one checkpoint):
        #------------------------------------------------------------------------------
        elif [[ -n $(find $LOCAL_BACKUP_PATH/$domain -type f -name "*.partial") ]]; then

            folder_checkpoints_list=($(backup_checkpoints_list $LOCAL_BACKUP_PATH/$domain))

            if [[ ${#folder_checkpoints_list[@]} -le 1 ]]; then

                # Backup is useless:
                backup_folder_integrity=false
                echo "$domain: A full/copy backup operation was previously cancelled. This backup is unrecoverable, therefore will be removed"

            elif [[ -z $VM_ALLOW_BACKUP_CHAIN_FIX || $unraid_scenario == true ]]; then

                # Backup chain may be recoverable, but either user didn't allow to check, or it's unraid_scenario:
                recoverable_backup_chain=false
                echo "$domain: A backup operation was previously cancelled. Backup chain is partially recoverable, but will not attempt to fix the backup for further use"

            else
                # Backup chain may be recoverable, and will attempt to fix it:
                recoverable_backup_chain=true
                echo "$domain: A backup operation was previously cancelled. Backup is partially recoverable and it will attempt to fix it and restore the current backup chain for further use"
            fi

        # If backup was made in 'copy' mode, there are no checkpoints or bitmaps in this VM to delete (and no checkpoints can be added anyway):
        #------------------------------------------------------------------------------
        elif [[ ! -f $LOCAL_BACKUP_PATH/$domain/$domain.cpt \
        && $(backup_checkpoints_list $LOCAL_BACKUP_PATH/$domain --num) -eq 0 \
        && -n $(find $LOCAL_BACKUP_PATH/$domain -type f -name "*.copy.data") ]]; then

            no_backup_chain=true
            echo "$domain: Current backup was made in 'copy' mode, and cannot be used for incremental backups"

        else # Backup folder integrity looks 'OK', at a first glance

            # Perform data check integrity when enabled by user:
            #------------------------------------------------------------------------------
            if [[ -n $CHECK_BACKUPS_INTEGRITY ]]; then

                verify_backup_chain $LOCAL_BACKUP_PATH/$domain
                verify_integrity_exit_code=$?
            fi

            # When has passed all tests above (including integrity check, if enabled),
            # then is approved for further checks:
            [[ $verify_integrity_exit_code != 1 ]] && backup_folder_integrity=true
        fi

        #------------------------------------------------------------------------------
        # Stage 2: Verify backup chain integrity on backup folders without issues or limitations:
        #------------------------------------------------------------------------------
        if [[ $backup_folder_integrity == true ]]; then

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoints_list $domain))

            # Read backup checkpoints:
            folder_checkpoints_list=($(backup_checkpoints_list $LOCAL_BACKUP_PATH/$domain))

            if [[ $(domain_state $domain) == "shut off" || $unraid_scenario == true ]]; then

                # Checking in-backup checkpoints vs. disk image bitmaps it's the most accurate way to determine backup chain integrity.
                # Only can be done if VM is shut down (and the only possible way when unraid_scenario applies):
                local backup_check_message="Performing a full check into its backup chain..."

                [[ $unraid_scenario == true ]] \
                    && echo "$domain: Is running (Unraid OS scenario). $backup_check_message" \
                    || echo "$domain: Is shut down. $backup_check_message"

                # Acquire the list of disk image path(s) contained for this VM:
                existing_images_list=($(domain_img_paths_list $domain))

                for image in ${existing_images_list[@]}; do

                    # Read bitmaps in each virtual image:
                    existing_bitmaps_list=($(disk_image_bitmaps_list $image))

                    if [[ ${existing_bitmaps_list[@]} == ${folder_checkpoints_list[@]} ]]; then

                        # Backup chain is OK, mark as preserved:
                        backup_chain_integrity=true
                        echo "$domain: Backup checkpoints and bitmaps lists on disk $image match (${#existing_bitmaps_list[@]}:${#folder_checkpoints_list[@]})"

                    else
                        backup_chain_integrity=false
                        echo "$domain: Backup checkpoints and bitmaps lists on disk $image do not match! (${#existing_bitmaps_list[@]}:${#folder_checkpoints_list[@]})"

                        # When more than one disk image to check, notify the check has been cancelled:
                        [[ ${#existing_images_list[@]} -gt 1 ]] && echo "$domain: Full check operation has been cancelled"

                        # Interrupt the current operation. Nothing else to do:
                        break
                    fi
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                # When VM is running, bitmaps created during a backup chain update aren't visible with qemu-img info until VM has not been shut down.
                # The only reliable method it's to compare in-backup vs QEMU checkpoints count (and this doesn't apply on unraid_scenario):
                echo "$domain: Is running, comparing QEMU and Backup's checkpoints lists..."

                if [[ ${qemu_checkpoints_list[@]} == ${folder_checkpoints_list[@]} ]]; then

                    # Backup chain integrity is OK:
                    backup_chain_integrity=true
                    echo "$domain: QEMU and backup checkpoints lists match (${#qemu_checkpoints_list[@]}:${#folder_checkpoints_list[@]})"

                else
                    # Mark the backup chain status as broken:
                    backup_chain_integrity=false
                    echo "$domain: QEMU and Backup checkpoints lists do not match! ${#qemu_checkpoints_list[@]}:${#folder_checkpoints_list[@]}"
                fi
            fi

        if [[ $backup_folder_integrity == false ]]; then

            # Delete non-recoverable backup folder:
            rm -rf $LOCAL_BACKUP_PATH/$domain
        fi

        #------------------------------------------------------------------------------
        # Stage 3: Process the results, according with all previous checks (attempting to fix recoverable issues, if possible):
        #------------------------------------------------------------------------------

        if [[ $backup_chain_integrity == true ]]; then

            # At this point, all checks has been successful and backup chain is ready for further use.
            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)

        elif [[ $no_backup_chain == true ]]; then

            # This is a 'copy' backup. As neither checkpoints nor bitmaps are present,
            # and no checkpoints can't be added; doesn't require further comprobations.
            # Add VM to the list of broken backup chain ones:
            broken_backup_chain_list+=($domain)

        elif [[ $recoverable_backup_chain == true ]]; then

            # This usually happens when container is stopped or restarted in the middle of a backup operation,
            # leaving an incomplete -and unusable- backup chain.
            # By deleting last checkpoint & bitmap from VM, and files generated by virtnbdbackup before the interruption,
            # backup chain ends at the latest successful backup state, and can receive more updates.
            # (Under unraid_scenario, there are basically NO chances to repair a backup chain this way, so this operation won't happen in that case):

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoints_list $domain))

            if [[ $(domain_state $domain) == "shut off" ]]; then

                echo "$domain: Removing damaged checkpoint metadata: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoints $domain ${qemu_checkpoints_list[-1]} --metadata

                for image in $(domain_img_paths_list $domain); do

                    # Read bitmaps in each virtual image:
                    existing_bitmaps_list=($(disk_image_bitmaps_list $image))

                    echo "$domain: Deleting insecure bitmap ${existing_bitmaps_list[-1]} on disk image $image..."
                    disk_image_bitmaps_delete $image ${existing_bitmaps_list[-1]}
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                echo "$domain: Removing damaged checkpoint: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoints $domain ${qemu_checkpoints_list[-1]}
            fi

            echo "$domain: Fixing backup chain data in $LOCAL_BACKUP_PATH/$domain ..."

            # Deletes any related file with the damaged checkpoint:
            find $LOCAL_BACKUP_PATH/$domain -name "*${qemu_checkpoints_list[-1]}*" -type f -delete

            # Deletes the last checkpoint from the list:
            unset qemu_checkpoints_list[-1]

            # Turn the updated list into string to export:
            qemu_checkpoints_list="${qemu_checkpoints_list[@]}"

            # Rebuilds the cpt file with the parsed new list of checkpoints:
            echo "[\"${qemu_checkpoints_list// /\", \"}\"]" > $LOCAL_BACKUP_PATH/$domain/$domain.cpt

            # Backup folder integrity is OK:
            backup_folder_integrity=true

            # Backup chain integrity is assumed as OK:
            backup_chain_integrity=true

            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)

        else # All other scenarios, where checking backup folder and/or chain failed...

            # Delete checkpoints and/or bitmaps, depending on the case:

            if [[ $(domain_state $domain) == "shut off" ]]; then

                # Deletes checkpoints metadata, if any detected:
                echo "$domain: Pruning any existing checkpoints metadata..."
                domain_delete_checkpoints $domain --all --metadata
                delete_checkpoint_exit_code=$?

                for image in $(domain_img_paths_list $domain); do

                    # Then deletes all bitmaps, in all image disks:
                    echo "$domain: Pruning any existing bitmaps on disk image $image..."

                    disk_image_bitmaps_delete $image --all
                    delete_bitmap_exit_code=$?

                    # When issues are found deleting bitmaps, cancel the entire operation:
                    [[ $delete_bitmap_exit_code -eq 1 ]] && break
                done

            elif [[ $(domain_state $domain) == "running" && $unraid_scenario == false ]]; then

                # Deletes checkpoints (and bitmaps) if any detected:
                echo "$domain: Pruning any existing checkpoints..."
                domain_delete_checkpoints $domain --all
                delete_checkpoint_exit_code=$?

            elif [[ $(domain_state $domain) == "running" && $unraid_scenario == true ]]; then

                # unraid_scenario. Only does something if is allowed to perform a power cycle on VM:
                if [[ -n $VM_ALLOW_POWERCYCLE ]]; then

                    domain_shutdown $domain --wait $VM_WAIT_TIME
                    shutdown_exit_code=$?

                    if [[ $shutdown_exit_code -eq 0 ]]; then

                        domain_shutdown_success_list+=($domain)

                        for image in $(domain_img_paths_list $domain); do

                            # Then deletes all bitmaps, in all image disks:
                            echo "$domain: Pruning any existing bitmaps on disk image $image..."

                            disk_image_bitmaps_delete $image --all
                            delete_bitmap_exit_code=$?

                            # When issues are found deleting bitmaps, cancel the entire operation:
                            [[ $delete_bitmap_exit_code -eq 1 ]] && break
                        done
                    fi
                fi
            fi

            # Assign VM and/or backup chain to the correspondent list, according the results:

            case $delete_checkpoint_exit_code in

                0|2)
                    if [[ $(domain_state $domain) == "shut off" ]]; then

                        case $delete_bitmap_exit_code in

                            0|2)    # Checkpoints metadata and bitmaps deleted successfully (or nothing to delete).
                                    # Add to broken backup chain list:
                                    broken_backup_chain_list+=($domain)
                            ;;

                            *)  # There were problems when deleting bitmaps.
                                # This VM has issues and user action is required:
                                domain_failed_list+=($domain)
                            ;;
                        esac

                    elif [[ $(domain_state $domain) == "running" ]]; then

                        # Checkpoints deleted successfully (or nothing to delete).
                        # Add to broken backup chain list:
                        broken_backup_chain_list+=($domain)
                    fi
                ;;

                1)  # There were problems when deleting checkpoints.
                    # This VM has issues and user action is required:
                    domain_failed_list+=($domain)
                ;;

                "") # unraid_scenario (no checkpoints were ever processed):

                    case $shutdown_exit_code in

                        0)
                            case $delete_bitmap_exit_code in

                                0|2)    # Bitmaps deleted successfully (or nothing to delete).
                                        # Add to broken backup chain list:
                                        broken_backup_chain_list+=($domain)
                                ;;

                                1)
                                    # There were problems when deleting bitmaps.
                                    # This VM has issues and user action is required:
                                    domain_failed_list+=($domain)
                                ;;
                            esac
                        ;;

                        1)  # There were problems when shutting down the VM.
                            # This VM may either have issues, or just taking too long into shut down.
                            # User action is required (but may also shut down itself eventually):
                            domain_shutdown_needed_list+=($domain)
                            echo "$domain: User action is required"
                        ;;

                        "") # No permissions to power cycle the VM are given (no shutdown was ever attempted):
                            # User action is required:
                            domain_shutdown_needed_list+=($domain)
                            echo "$domain: User action is required (env variable VM_ALLOW_POWERCYCLE not set)"
                        ;;
                    esac
                ;;
            esac
        fi
    done

    # Cleanse CHECK_BACKUPS_LIST, since all VMs were processed, and copied into correspondent lists:
    unset CHECK_BACKUPS_LIST

    # Appends broken backup chains for new backup chain creation:
    CREATE_BACKUP_CHAIN_LIST+=(${broken_backup_chain_list[@]})

    # Appends successfully shut down VMs to the global list in need to be started:
    POWEREDOFF_VMS_LIST+=(${domain_shutdown_success_list[@]})

    # Appends preserved backup chains to scheduled backups directly:
    SCHEDULED_BACKUPS_LIST+=(${preserved_backup_chain_list[@]})

    # Appends VMs failed to shutdown (by user's setting or by fail) to request user action:
    SHUTDOWN_REQUIRED_VMS_LIST+=(${domain_shutdown_needed_list[@]})

    # Appends failed to shut down VMs to failed VMs list:
    FAILED_VMS_LIST+=(${domain_failed_list[@]})

    echo ""
    echo "Backup Chain Integrity Summary:"
    echo ""
    echo "Added to Scheduled Backups:   ${preserved_backup_chain_list[@]:-"None"}"
    echo "In need of new backup chain:  ${broken_backup_chain_list[@]:-"None"}"
    [[ -n $VM_ALLOW_POWERCYCLE ]] && \
    echo "Into automatic power cycle:   ${domain_shutdown_success_list[@]:-"None"}" || \
    echo "Manual shut down is required: ${domain_shutdown_needed_list[@]:-"None"}"
    echo ""
    [[ -z ${domain_failed_list[@]} ]] && \
    echo "All Backup Chains Checked!" \ ||
    echo "Failed Power Cycle:           ${domain_failed_list[@]:-"None"}"
}

###############################################################################

# User variables defaults (details on README.md):
#------------------------------------------------------------------------------

BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-"@daily"}
CHECK_BACKUPS_INTEGRITY=${CHECK_BACKUPS_INTEGRITY:-""}
LOCAL_BACKUP_CHAINS_TO_KEEP=${LOCAL_BACKUP_CHAINS_TO_KEEP:-"1"}
LOCAL_BACKUP_PATH=${LOCAL_BACKUP_PATH:-"/backups"}
LOGFILE_PATH=${LOGFILE_PATH:-"/logs/vm-babysitter.log"}
SCHEDULE_LOGFILE_PATH=${SCHEDULE_LOGFILE_PATH:-"/logs/scheduled-tasks.log"}
LOGROTATE_CONFIG_PATH=${LOGROTATE_CONFIG_PATH:-"/tmp/logrotate.d/vm-babysitter"}
LOGROTATE_SCHEDULE=${LOGROTATE_SCHEDULE:-"@daily"}
LOGROTATE_SETTINGS=${LOGROTATE_SETTINGS:-"  compress\n  copytruncate\n  daily\n  dateext\n  dateformat .%Y-%m-%d.%H:%M:%S\n  missingok\n  notifempty\n  rotate 30"}
MAX_BACKUPS_PER_CHAIN=${MAX_BACKUPS_PER_CHAIN:-"30"}
RSYNC_ARGS=${RSYNC_ARGS:-"-a"}
RSYNC_BACKUP_CHAINS_TO_KEEP=${RSYNC_BACKUP_CHAINS_TO_KEEP:-"2"}
RSYNC_BACKUP_PATH=${RSYNC_BACKUP_PATH:-""}
RSYNC_SCHEDULE=${RSYNC_SCHEDULE:-""}
SSH_OPTIONS=${SSH_OPTIONS:-"-q -o IdentityFile=/private/hostname.key -o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"}
[[ -f /usr/share/zoneinfo/$TZ ]] && local_timezone_file="/usr/share/zoneinfo/$TZ"
UNRAID_NOTIFY_HOST=${UNRAID_NOTIFY_HOST:-"localhost"}
VIRTNBDBACKUP_ARGS=${VIRTNBDBACKUP_ARGS:-""}
VM_ALLOW_BACKUP_CHAIN_FIX=${VM_ALLOW_BACKUP_CHAIN_FIX:-""}
VM_ALLOW_POWERCYCLE=${VM_ALLOW_POWERCYCLE:-""}
VM_AUTOSTART_LIST=${VM_AUTOSTART_LIST:-""}
VM_IGNORED_LIST=${VM_IGNORED_LIST:-""}
VM_WAIT_TIME=${VM_WAIT_TIME:-"60"}

# Internal Variables:
#------------------------------------------------------------------------------

# Temporal crontab file (to be loaded for cron)
crontab_file="/tmp/crontab"

# Storing file for bash like lists shared between this script and the scheduler:
external_vars="/tmp/vm-babysit-vars"

# Export variables used by children processes:
export external_vars

# Main execution:
###############################################################################

# Redirect system stdout and stderr to $LOGFILE_PATH. Note this applies to all other commands and scripts run from this script:
exec &> >(tee -a $LOGFILE_PATH)

# Create internal folders in case don't exist:
[[ ! -d $( dirname $LOGFILE_PATH) ]] && mkdir -p $(dirname $LOGFILE_PATH)

# Sets the local timezone (if ENV TZ was set):
if [[ -n $local_timezone_file ]]; then

    ln -fs $local_timezone_file /etc/localtime
    &> /dev/null dpkg-reconfigure -f noninteractive tzdata
fi

# Catches the signal sent from docker to stop execution:
# The most gracefully way to stop this container is with:
# 'docker kill --signal=SIGTERM <docker-name-or-id>'
trap 'stop_container' SIGTERM


echo "############################################################################################################"
echo "Container started at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))"
echo "############################################################################################################"

#------------------------------------------------------------------------------
# 1. Check input parameters (exits on error)
#------------------------------------------------------------------------------
check_variables
check_variables_exit_code=$?

#------------------------------------------------------------------------------
# 2. Only when input parameters doesn't require to restart the container, it continues the rest of the checks:
#------------------------------------------------------------------------------

if [[ $check_variables_exit_code -eq 0 ]]; then

    # Initializes a file with variables externally stored, to be shared with the scheduler:
    #------------------------------------------------------------------------------
    cat << end_of_external_variables > $external_vars
# Shared values between scripts. DO NOT EDIT!:
FAILED_VMS_LIST=()
SCHEDULED_BACKUPS_LIST=()
end_of_external_variables

    # Create logrotate config:
    #------------------------------------------------------------------------------

    echo "Configuring logrotate..."

    # Create directory if doesn't exist:
    mkdir -p $(dirname $LOGROTATE_CONFIG_PATH)

    # Parse logrotate configuration (overwriting, if any previous):
    echo -e "$LOGFILE_PATH \n $SCHEDULE_LOGFILE_PATH {\n$LOGROTATE_SETTINGS\n}" > $LOGROTATE_CONFIG_PATH

    # Create/update Cron task for VMs to be (progressively) included in $scheduled_backups_list:
    #------------------------------------------------------------------------------

    echo "Configuring scheduled tasks..."

    # Silently deletes any previous cron task:
    &> /dev/null crontab -r

    # Parses the actual cron task needed to run to $crontab_file
    # (Including ENV vars not being read from cron's environment):
    cat << base_crontab > $crontab_file
# Values below are refreshed upon container (re)start:

# Main environment is bash:
SHELL=/bin/bash

# Search paths for binaries and scripts:
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# Environment variables used by scheduled tasks:
LOCAL_BACKUP_CHAINS_TO_KEEP="$LOCAL_BACKUP_CHAINS_TO_KEEP"
LOCAL_BACKUP_PATH="$LOCAL_BACKUP_PATH"
MAX_BACKUPS_PER_CHAIN="$MAX_BACKUPS_PER_CHAIN"
RSYNC_ARGS="$RSYNC_ARGS"
RSYNC_BACKUP_CHAINS_TO_KEEP="$RSYNC_BACKUP_CHAINS_TO_KEEP"
RSYNC_BACKUP_PATH="$RSYNC_BACKUP_PATH"
RSYNC_SCHEDULE=$RSYNC_SCHEDULE
SSH_OPTIONS="$SSH_OPTIONS"
UNRAID_NOTIFY_HOST="$UNRAID_NOTIFY_HOST"
VIRTNBDBACKUP_ARGS="$VIRTNBDBACKUP_ARGS"
VM_WAIT_TIME=$VM_WAIT_TIME

# Paths for functions and (shared) dynamic variables:
external_vars="$external_vars"

# Schedule for logrotate:
$LOGROTATE_SCHEDULE /usr/sbin/logrotate $LOGROTATE_CONFIG_PATH >> $SCHEDULE_LOGFILE_PATH 2>&1

# Schedule for VM backups:
$BACKUP_SCHEDULE /usr/local/bin/vm-backup --run-schedule >> $SCHEDULE_LOGFILE_PATH 2>&1
base_crontab

    if [[ -n $RSYNC_BACKUP_PATH && -n $RSYNC_SCHEDULE ]]; then

    # Append additional task to run Rsync on a defined schedule:
        cat << optional_crontab >> $crontab_file

# Schedule for Rsync:
$RSYNC_SCHEDULE /usr/local/bin/vm-rsync --run-schedule >> $SCHEDULE_LOGFILE_PATH 2>&1
optional_crontab
    fi

    # Sets the cron task:
    crontab $crontab_file

    # Finally, runs cron and sends to background:
    echo "Starting Cron..."
    cron -f -l -L2 &

    # 3. Begin monitorization for VMs in lists, performing operations as required:
    #------------------------------------------------------------------------------

    # Creates DOMAINS_LIST, a list of non-transient domains that aren't into any of the queues from past iterations:
    create_domains_list

    echo ""
    echo "############################################################################################################"
    echo "Monitoring mode started"

    while true; do

        # Maximum standby period for monitoring should not exceed 10 seconds in any case,
        # because it could ignore SIGTERM from Docker, thus being killed with SIGKILL:
        sleep 1

        # (Re)reads all external variables:
        #------------------------------------------------------------------------------
        source $external_vars

        if [[ -n ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ]]; then

            # Check for VMs which are in need of shutdown.
            # (This normally happens when the user took the action, or when a VM took more than VM_WAIT_TIME secs to shutdown):
            #------------------------------------------------------------------------------
            i=0
            for domain in ${SHUTDOWN_REQUIRED_VMS_LIST[@]}; do

                if [[ $(domain_state $domain) == "shut off" ]]; then

                    # Add VM to the same list of VMs to be powered on when monitor starts:
                    POWEREDOFF_VMS_LIST+=($domain)

                    # Move to main queue for check:
                    CHECK_BACKUPS_LIST+=($domain)
                    unset SHUTDOWN_REQUIRED_VMS_LIST[$i]

                    # Rebuild indexes into the array:
                    SHUTDOWN_REQUIRED_VMS_LIST=(${SHUTDOWN_REQUIRED_VMS_LIST[@]})

                    [[ $(os_is_unraid) == true ]] && unraid_notify "normal" "VM-Babysitter" "Required shut down detected" "Resuming check/backup for $domain"
                fi
            done
        fi

        if [[ -n ${DOMAINS_LIST[@]} || -n ${CHECK_BACKUPS_LIST[@]} ]]; then

            # Create the global list of VMs to be displayed:
            ONGOING_CHECK_LIST=(${DOMAINS_LIST[@]} ${CHECK_BACKUPS_LIST[@]})

            echo ""
            echo "____________________________________________________________________________________________________________"
            echo "Automatic check for VM(s) ${ONGOING_CHECK_LIST[@]} in progress..."


            # Check for virtual drives status on detected VMs:
            #------------------------------------------------------------------------------
            [[ -n ${DOMAINS_LIST[@]} ]] && check_domains

            # Check for backup chain integrity on all VMs:
            #------------------------------------------------------------------------------
            [[ -n ${CHECK_BACKUPS_LIST[@]} ]] && check_backups

            # Turns on all VMs that was, either declared in VM_AUTOSTART_LIST or previously shutdown for checks:
            #------------------------------------------------------------------------------
            if [[ -n ${POWEREDOFF_VMS_LIST[@]} ]]; then

                echo ""
                i=0
                for domain in ${POWEREDOFF_VMS_LIST[@]}; do

                    if [[ $(domain_state $domain) != running ]]; then

                        # Turn on the VM. Do not wait for Guest's QEMU agent:
                        domain_start $domain --nowait

                        # Add any VM failed to start to FAILED_VMS_LIST:
                        [[ $? -ne 0 ]] && FAILED_VMS_LIST+=($domain)
                    fi

                    # Remove the VM from the list is being read:
                    unset POWEREDOFF_VMS_LIST[$i]

                    # Rebuild indexes into the array:
                    POWEREDOFF_VMS_LIST=(${POWEREDOFF_VMS_LIST[@]})

                    # Increases the counter:
                    ((i++))
                done
            fi

            # Create new backup chains for VMs that require it:
            #------------------------------------------------------------------------------
            if [[ -n ${CREATE_BACKUP_CHAIN_LIST[@]} ]]; then

                # Create new backup chains with backup script (on-demand mode):
                vm-backup --create-new ${CREATE_BACKUP_CHAIN_LIST[@]}

                # Reload external variables to gather changes on lists:
                source $external_vars
            fi

            # 3.4 Show status report and notify about user actions, if required:
            #------------------------------------------------------------------------------
            echo ""
            echo "############################################################################################################"
            echo "All VMs with status changed finished to be processed at $(date "+%Y-%m-%d %H:%M:%S")"
            echo ""
            echo "VM-Babysitter Global Summary:"
            echo ""
            echo "Current Scheduled Backups:    ${SCHEDULED_BACKUPS_LIST[@]:-"None"}"
            echo "Manual Shut Down Required:    ${SHUTDOWN_REQUIRED_VMS_LIST[@]:-"None"}"
            echo "Misbehaving Virtual Machines: ${FAILED_VMS_LIST[@]:-"None"}"

            if [[ -n ${SCHEDULED_BACKUPS_LIST[@]} && -z ${SHUTDOWN_REQUIRED_VMS_LIST[@]} && -z ${FAILED_VMS_LIST[@]} ]]; then

                echo ""
                echo "All Virtual Machines Running Under Incremental Backups!"

            elif [[ -n ${SHUTDOWN_REQUIRED_VMS_LIST[@]} || -n ${FAILED_VMS_LIST[@]} ]]; then

                user_action_message="USER ACTION IS REQUIRED!"

                echo ""
                echo $user_action_message

                if [[ -n ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ]]; then

                    shutdown_required_message="As env variable 'VM_ALLOW_POWERCYCLE' is not set, a manual Shut Down is required for VM(s): ${SHUTDOWN_REQUIRED_VMS_LIST[@]} 'As Soon As Possible'. This is required prior to perform backup integrity checks and cannot procced without that. Once shut down, the program will perform the checks and will restart involved VM(s) automatically."

                    echo "WARNING: $shutdown_required_message"
                    [[ $(os_is_unraid) == true ]] && unraid_notify "warning" "VM-Babysitter" "$user_action_message" "$shutdown_required_message"
                fi

                if [[ -n ${FAILED_VMS_LIST[@]} ]]; then

                    failed_vms_message="Unexpected failure during check/backup operation involving VM(s): ${FAILED_VMS_LIST[@]}. Human attention is required 'As Soon As Possible' to solve the situation. Any scheduled backup or further alert for involved VM(s) is suspended until this container had been restarted. Check $(basename $LOGFILE_PATH) in this host for detailed logs."


                    echo  "ERROR: $failed_vms_message"
                    [[ $(os_is_unraid) == true ]] && unraid_notify "alert" "VM-Babysitter" "$user_action_message" "$failed_vms_message"
                fi
            fi

            # 3.5 Marks ongoing check as finished and exports variables to be used on scheduled backups (entering in 'silent' mode):
            #------------------------------------------------------------------------------

            # Sed can't expand arrays correctly. Convert variables to be exported into strings.
            # This will be reverted at next block iteration, by sourcing such external variables as arrays:
            FAILED_VMS_LIST="${FAILED_VMS_LIST[@]}"
            SCHEDULED_BACKUPS_LIST="${SCHEDULED_BACKUPS_LIST[@]}"

            # Export the variables:
            sed -i \
            -e "s/FAILED_VMS_LIST=.*/FAILED_VMS_LIST=($FAILED_VMS_LIST)/" \
            -e "s/SCHEDULED_BACKUPS_LIST=.*/SCHEDULED_BACKUPS_LIST=($SCHEDULED_BACKUPS_LIST)/" \
            $external_vars
        fi

        # Restarts the loop, until SIGTERM or SIGKILL are received from Docker...
        #------------------------------------------------------------------------------
    done

else
    # Initial checks have proven non-recoverable errors:
    failed_initialize_message="Could not initialize due to errors! Check $(basename $LOGFILE_PATH) in this host for detailed logs."

    echo "ERROR: $failed_initialize_message"
    [[ $(os_is_unraid) == true ]] && unraid_notify "alert" "VM-Babysitter" "Failed to start container" "$failed_initialize_message"

    stop_container
fi
