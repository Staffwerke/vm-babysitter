#!/bin/bash

###############################################################################
# Specific procedures:
###############################################################################

#------------------------------------------------------------------------------
# Attempts to stop the container gracefully in case of receive SIGTERM signal from Docker:
#------------------------------------------------------------------------------
stop_container()
{
    echo "############################################################################################################"
    echo "SIGTERM signal received at: $(date "+%Y-%m-%d %H:%M:%S")"
    # To DO: Terminate or kill background processes before to exit.
    echo "Container Stopped."
    echo ""
    exit 0
}

#------------------------------------------------------------------------------
# Checks environment variables and return success or fail if everything is in order:
#------------------------------------------------------------------------------
check_variables()
{
    # Starts check assuming failed statuses:
    local check_variables_status=1
    local max_backups_per_chain_status=1
    local local_backup_path_status=1
    local rsync_backup_path_status=1

    # Check $MAX_BACKUPS_PER_CHAIN
    #------------------------------------------------------------------------------
    if [[ $MAX_BACKUPS_PER_CHAIN =~ [0-9] && $MAX_BACKUPS_PER_CHAIN -ge 0 ]]; then

        # Is an integer number:
        max_backups_per_chain_status=0
        echo "Max backups to save per chain: $MAX_BACKUPS_PER_CHAIN"

    else
        # Invalid value:
        echo "ERROR: Incorrect value for environment variable MAX_BACKUPS_PER_CHAIN: '$MAX_BACKUPS_PER_CHAIN' <- It must be a natural integer and equal/greater than zero"
    fi

    # Check LOCAL_BACKUP_PATH:
    #------------------------------------------------------------------------------
    if  [[ -d $LOCAL_BACKUP_PATH ]]; then

        # $LOCAL_BACKUP_PATH found
        if  [[ -r $LOCAL_BACKUP_PATH && -w $LOCAL_BACKUP_PATH ]]; then

            # $LOCAL_BACKUP_PATH has read/write permissions.
            echo "Backups main path internally set to: $LOCAL_BACKUP_PATH"

            # Check $LOCAL_BACKUP_CHAINS_TO_KEEP:
            if [[ $LOCAL_BACKUP_CHAINS_TO_KEEP =~ [0-9] && $LOCAL_BACKUP_CHAINS_TO_KEEP -ge 0 ]]; then

                # Check passed:
                local_backup_path_status=0
                echo "Max backup chains per VM to keep locally: $LOCAL_BACKUP_CHAINS_TO_KEEP"

            else
                # Invalid value:
                echo "ERROR: Incorrect value for environment variable LOCAL_BACKUP_CHAINS_TO_KEEP: '$LOCAL_BACKUP_CHAINS_TO_KEEP' <- It must be a natural integer"
            fi

        else
            echo "ERROR: Backups main path: $LOCAL_BACKUP_PATH <- Permission issues (cannot be read or written)"
        fi

    else
        echo "ERROR: Backups main path: $LOCAL_BACKUP_PATH  <- Not found inside the container. Ensure you mount this correctly (e.g. '-v /path/to/backups-main-path:/$LOCAL_BACKUP_PATH')"
    fi

    # Check RSYNC_BACKUP_PATH
    #------------------------------------------------------------------------------
    if [[ -z $RSYNC_BACKUP_PATH ]]; then

        rsync_backup_path_status=0
        echo ""
        echo "Environment variable RSYNC_BACKUP_PATH not set. No remote backup endpoint will be used"

    elif [[ $RSYNC_BACKUP_PATH == *@*:/* ]]; then

        # Apparently includes correct remote login and path. Separates ssh login from remote path:
        local rsync_server=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f1)
        local rsync_path=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f2)

        # Attempts to comunicate with the remote host:
        ssh $SSH_OPTIONS $rsync_server "exit 0"
        local rsync_server_status=$?

        if [[ $rsync_server_status == 0 ]]; then

            # Attempts to perform similar checks as with $LOCAL_BACKUP_PATH, but creating a remote backup directory if doesn't exists:
            local rsync_check_backup_path_status=$(ssh $SSH_OPTIONS $rsync_server "[[ -d $rsync_path && -r $rsync_path && -w $rsync_path ]] && echo 'found' || { mkdir -p $rsync_path; [[ -d $rsync_path ]] && echo 'created' || echo 'failed'; }")


            if [[ $rsync_check_backup_path_status != failed ]]; then

                # Notifiy about good status of $rsync_path
                echo "$rsync_server: $rsync_path was $rsync_check_backup_path_status sucessfully"

                # Verify # of backup chains to keep:
                if [[ $RSYNC_BACKUP_CHAINS_TO_KEEP =~ [0-9] && $RSYNC_BACKUP_CHAINS_TO_KEEP -ge 0 ]]; then

                    # Check passed:
                    rsync_backup_path_status=0
                    echo "Max backup chains per VM to keep remotely: $RSYNC_BACKUP_CHAINS_TO_KEEP"

                else
                    echo "ERROR: Incorrect value for environment variable RSYNC_BACKUP_CHAINS_TO_KEEP: '$RSYNC_BACKUP_CHAINS_TO_KEEP' it must be a positive integer and equal/greater than zero"
                fi

            else
                echo "ERROR: remote mirror $rsync_path cannot be read and/or written (or is not a directory)"
            fi

        else
            echo "ERROR: Connection with $rsync_server failed with status $rsync_server_status"
        fi

    else
        echo "ERROR: Incorrect syntax for RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH' it must be SSH-like absolute path (e.g. user@host:/path-to-mirror)"
    fi

    # TO DO: Check other ENV variables, and SSH key:

    # When all status variables passed the checks, return success:
    [[ $local_backup_path_status -eq 0 \
    && $rsync_backup_path_status -eq 0 ]] \
        && check_variables_status=0

    return $check_variables_status
}

#------------------------------------------------------------------------------
# Populates DOMAINS_LIST, filtering ignored, into process and failed  VMs:
#------------------------------------------------------------------------------
create_domains_list()
{
    local domains_all_list=($(domains_list))
    local domains_skipped_list=(${VM_IGNORED_LIST[@]} ${FAILED_VMS_LIST[@]} ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ${POWEREDOFF_VMS_LIST[@]} ${CHECK_BACKUPS_LIST[@]} ${CREATE_BACKUP_CHAIN_LIST[@]} ${SCHEDULED_BACKUPS_LIST[@]})

    for domain in ${domains_all_list[@]}; do

        for skipped in ${domains_skipped_list[@]}; do

            [[ $domain == $skipped ]] && continue 2
        done

        DOMAINS_LIST+=($domain)
    done
}

#------------------------------------------------------------------------------
# Checks for VMs in DOMAINS_LIST by verifying existence and read/write permissions of its virtual disks,
# also marking VMs for automatic start, when required by the user:
#------------------------------------------------------------------------------
check_domains()
{

    local domain_autostart_list=()
    local domain_failed_list=()
    local domain_success_list=()

    echo ""
    echo "____________________________________________________________________________________________________________"
    echo "Checking for Virtual machines: ${DOMAINS_LIST[@]}"

    for domain in ${DOMAINS_LIST[@]}; do

        # Checks for virtual disks, if are reachable and possess both read/write permissions:

        if [[ -z $(domain_drives_list $domain) ]]; then

            # Non-transient VM without disks attached. Adds to failed list, notifies and aborts the check:
            domain_failed_list+=($domain)
            echo "WARNING: VM $domain has no drives that can be backed up, therefore will be ignored"
            break

        else
            # Does have drives able to be backed up. Checks if such disk images are reachable inside the container:
            local images_list=($(domain_img_paths_list $domain))
            for image in ${images_list[@]}; do

                if [[ ! -f $image ]]; then

                    domain_failed_list+=($domain)
                    echo "ERROR: $domain's disk image: $image not found. Ensure you mount -and mirror- this correctly inside the container (e.g. '-v /common/path/to/vms/disks:/common/path/to/vms/disks')"

                elif [[ ! -r $image || ! -w $image ]]; then

                    domain_failed_list+=($domain)
                    echo "ERROR: $domain's disk image: $image has permission issues (cannot be read or written)"
                else
                    # Check passed:
                    domain_success_list+=($domain)

                    for autostart_vm in $VM_AUTOSTART_LIST; do

                        if [[ $domain == $autostart_vm ]]; then

                            # If the VM is in VM_AUTOSTART_LIST, requires to be started as soon as possible:
                            domain_autostart_list+=($domain)
                            echo "$domain: Into VM_AUTOSTART_LIST, therefore will be started, (after initial check)"
                            break
                        fi
                    done
                fi
            done
        fi
    done

    # Cleanse DOMAINS_LIST, since all VMs were processed, and copied into correspondent lists:
    unset DOMAINS_LIST

    # Appends successfully checked VMs for checking backups:
    CHECK_BACKUPS_LIST+=${domain_success_list[@]}

    # Appends VMs with unexistent, unreachable or virtual disks with permission issues:
    FAILED_VMS_LIST+=${domain_failed_list[@]}

    # Appends successfully checked VMs to the global list in need to be started:
    POWEREDOFF_VMS_LIST+=${domain_autostart_list[@]}

    echo ""
    echo "Virtual Machines Check Summary:"
    echo ""
    echo "Ready for Backup Check Integrity:" ${domain_success_list[@]:-"None"}
    echo "Set for Autostart (after checks):" ${domain_autostart_list[@]:-"None"}
    echo ""
    [[ -z ${domain_failed_list[@]} ]] && \
    echo "All Backup Chains Checked!" \ ||
    echo "Missing/issues on virtual disks:"  ${domain_failed_list[@]:-"None"}
}

#------------------------------------------------------------------------------
# Checks VMs in CHECK_BACKUPS_LIST for backup chain's health, permitting to keep using correct ones,
# managing broken ones according the scenario (archiving restorable backups),
# and preparing VMs to create a new backup chains.
# Optionally can verify backups integrity (if enabled by user):
#------------------------------------------------------------------------------
check_backups()
{
    # Local variables used for flow control:
    local backup_chain_integrity
    local backup_folder_integrity
    local delete_bitmap_status
    local delete_checkpoint_status
    local existing_bitmaps_list
    local folder_checkpoints_list
    local no_backup_chain
    local shutdown_status
    local qemu_checkpoints_list
    local recoverable_backup_chain
    local verify_integrity_status

    # Lists to add VMs (and summarize at the end):
    local broken_backup_chain_list
    local domain_failed_list
    local domain_shutdown_needed_list
    local domain_shutdown_success_list
    local preserved_backup_chain_list

    if [[ $(os_is_unraid) == yes ]]; then

        # OS is Unraid. Check for unraid_scenario, when comes from a crash and VMs were running before this container started:
        for domain in $(domains_list); do

            # Look for all VMs and its checkpoints, stop if it finds at least one:
            [[ -n $(domain_checkpoint_list $domain) ]] && { local checkpoints_found="true"; break; }

        done

        for domain in ${CHECK_BACKUPS_LIST[@]}; do

            # Look for all VMs to be checked, stop if any of them is running:
            [[ $(domain_state $domain) == "running" ]] && { local vms_are_running="true"; break; }
        done

        if [[ $checkpoints_found == false && $vms_are_running == true ]]; then

            # If no checkpoints were found across the server and at least one of the VMs to be checked is running, set unraid_scenario flag:
            local unraid_scenario="true"

            echo ""
            echo "NOTICE: Unraid OS detected, no VM in this server has QEMU checkpoints at all, and at least one VM to be checked is running"
            echo
            echo "  - This is the first run of this program after the last server restart"
            echo "  - This Unraid server crashed before this container runs"
            echo
            echo "Due to the possibility of a crash scenario, running VMs need to be checked differently than under normal operation"
            echo "Depending on your env variables, user intervention may be required, such as shutting down VMs or restarting this container manually."
        fi
    fi

    echo ""
    echo "____________________________________________________________________________________________________________"
    echo "Checking backup chain ingtegrity on Virtual machines: ${CHECK_BACKUPS_LIST[@]}"

    local i=0
    for domain in ${CHECK_BACKUPS_LIST[@]}; do

        backup_chain_integrity=""
        backup_folder_integrity=""
        delete_bitmap_status=""
        delete_checkpoint_status=""
        folder_checkpoints_list=()
        no_backup_chain=""
        shutdown_status=""
        recoverable_backup_chain=""
        verify_integrity_status=""

        echo ""

        #------------------------------------------------------------------------------
        # Stage 1: Check backup integrity:
        #------------------------------------------------------------------------------

        # Discard non-existing, and empty folders:
        #------------------------------------------------------------------------------
        if [[ ! -d $LOCAL_BACKUP_PATH/$domain || $(dir_is_empty $LOCAL_BACKUP_PATH/$domain) == yes ]]; then

            backup_folder_integrity="false"
            echo "$domain: Unexistent or empty backup chain folder"

        # Interrupted operations (may recover backup chains when has more than one checkpoint):
        #------------------------------------------------------------------------------
        elif [[ -n $(find $LOCAL_BACKUP_PATH/$domain -type f -name "*.partial") ]]; then

            folder_checkpoints_list=($(backup_checkpoint_list $LOCAL_BACKUP_PATH/$domain))

            if [[ ${#folder_checkpoints_list[@]} -le 1 ]]; then

                # Backup is useless:
                backup_folder_integrity="false"
                echo "$domain: A full/copy backup operation was previously cancelled. This backup is unrecoverable, therefore will be removed"

            elif [[ -z $VM_ALLOW_BACKUP_CHAIN_FIX || $unraid_scenario = true ]]; then

                # Backup chain may be recoverable, but either user didn't allow to check, or it's unraid_scenario:
                recoverable_backup_chain="false"
                echo "$domain: A backup operation was previously cancelled. Backup chain is partially recoverable, but will not attempt to fix the backup for further use"

            else
                # Backup chain may be recoverable, and will attempt to fix it:
                recoverable_backup_chain="true"
                echo "$domain: A backup operation was previously cancelled. Backup is partially recoverable and it will attempt to fix it and restore the current backup chain for further use"
            fi

        # If backup was made in 'copy' mode, there are no checkpoints or bitmaps in this VM to delete (and no checkpoints can be added anyway):
        #------------------------------------------------------------------------------
        elif [[ ! -f $LOCAL_BACKUP_PATH/$domain/$domain.cpt \
        && $(backup_checkpoint_list $LOCAL_BACKUP_PATH/$domain --num) -eq 0 \
        && -n $(find $LOCAL_BACKUP_PATH/$domain -type f -name "*.copy.data") ]]; then

            no_backup_chain="true"
            echo "$domain: Current backup was made in 'copy' mode, and cannot be used for incremental backups"

        else # Backup folder integrity looks 'OK', at a first glance

            # Perform data check integrity when enabled by user:
            #------------------------------------------------------------------------------
            if [[ -n $CHECK_BACKUPS_INTEGRITY ]]; then

                verify_backup_chain $LOCAL_BACKUP_PATH/$domain
                verify_integrity_status=$?
            fi

            # When has passed all tests above (including integrity check, if enabled),
            # then is approved for further checks:
            [[ $verify_integrity_status != 1 ]] && backup_folder_integrity="true"
        fi

        #------------------------------------------------------------------------------
        # Stage 2: Verify backup chain integrity on backup folders without issues or limitations:
        #------------------------------------------------------------------------------
        if [[ $backup_folder_integrity = true ]]; then

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoint_list $domain))

            # Read backup checkpoints:
            folder_checkpoints_list=($(backup_checkpoint_list $LOCAL_BACKUP_PATH/$domain))

            if [[ $(domain_state $domain) == "shut off" || $unraid_scenario == true ]]; then

                # Checking in-backup checkpoints vs. disk image bitmaps it's the most accurate way to determine backup chain integrity.
                # Only can be done if VM is shut down (and the only possible way when unraid_scenario applies):
                local backup_check_message="Performing a full check into its backup chain..."

                [[ $unraid_scenario == true ]] \
                    && echo "$domain: Is running (Unraid OS scenario). $backup_check_message" \
                    || echo "$domain: Is shut down. $backup_check_message"

                for image in $(domain_img_paths_list $domain); do

                    # Read bitmaps in each virtual image:
                    existing_bitmaps_list=($(disk_image_bitmap_list $image))

                    if [[ ${existing_bitmaps_list[@]} != ${folder_checkpoints_list[@]} ]]; then

                        backup_chain_integrity="false"
                        echo "$domain: Checkpoints and bitmaps lists on disk $image MISMATCH"

                        # Interrupt the current operation. Nothing else to do:
                        break

                    elif [[ -n ${existing_bitmaps_list[@]} ]]; then

                        # It is assumed bitmaps = checkpoints, and none is a empty list.

                        # Backup chain is OK, mark as preserved:
                        backup_chain_integrity="true"
                        echo "$domain: Checkpoints and bitmaps lists on disk $image MATCH"

                    else
                        backup_chain_integrity="false"
                        echo "$domain: ${#folder_checkpoints_list[@]} checkpoints found, and ${#existing_bitmaps_list[@]} bitmaps found on disk $image (backup chain is broken)"

                        # Interrupt the current operation. Nothing else to do:
                        break
                    fi
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                # When VM is running, bitmaps created during a backup chain update aren't visible with qemu-img info until VM has not been shut down.
                # The only reliable method it's to compare in-backup vs QEMU checkpoints count (and this doesn't apply on unraid_scenario):
                echo "$domain: Is running, comparing QEMU and Backup's checkpoint lists..."

                if [[ ${qemu_checkpoints_list[@]} == ${folder_checkpoints_list[@]} ]]; then

                    # Backup chain integrity is OK:
                    backup_chain_integrity="true"
                    echo "$domain: QEMU and Backup checkpoints lists MATCH"

                else
                    # Mark the backup chain status as broken:
                    backup_chain_integrity="false"
                    echo "$domain: QEMU and Backup's checkpoint lists MISMATCH"
                fi
            fi
        fi

        #------------------------------------------------------------------------------
        # Stage 3: Process the results, according with all previous checks (attempting to fix recoverable issues, if possible):
        #------------------------------------------------------------------------------

        if [[ $backup_chain_integrity == true ]]; then

            # At this point, all checks has been successful and backup chain is ready for further use.
            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)

        elif [[ $no_backup_chain == true ]]; then

            # This is a 'copy' backup. As neither checkpoints nor bitmaps are present,
            # and no checkpoints can't be added; doesn't require further comprobations.
            # Add VM to the list of broken backup chain ones:
            broken_backup_chain_list+=($domain)

        elif [[ $recoverable_backup_chain == true ]]; then

            # This usually happens when container is stopped or restarted in the middle of a backup operation,
            # leaving an incomplete -and unusable- backup chain.
            # By deleting last checkpoint & bitmap from VM, and files generated by virtnbdbackup before the interruption,
            # backup chain ends at the latest successful backup state, and can receive more updates.
            # (Under unraid_scenario, there are basically NO chances to repair a backup chain this way, so this operation won't happen in that case):

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoints_list $domain))

            if [[ $(domain_state $domain) == "shut off" ]]; then

                echo "$domain: Removing damaged checkpoint metadata: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoint $domain ${qemu_checkpoints_list[-1]} --metadata

                for image in $(domain_img_paths_list $domain); do

                    # Read bitmaps in each virtual image:
                    existing_bitmaps_list=($(disk_image_bitmap_list $image))

                    echo "$domain: Deleting insecure bitmap ${existing_bitmaps_list[-1]} on disk image $image..."
                    disk_image_bitmap_delete $image ${existing_bitmaps_list[-1]}
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                echo "$domain: Removing damaged checkpoint: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoint $domain ${qemu_checkpoints_list[-1]}
            fi

            echo "$domain: Fixing backup chain data in $LOCAL_BACKUP_PATH/$domain ..."

            # Deletes any related file with the damaged checkpoint:
            find $LOCAL_BACKUP_PATH/$domain -name "*${qemu_checkpoints_list[-1]}*" -type f -delete

            # Deletes the last checkpoint from the list:
            unset qemu_checkpoints_list[-1]

            # Turn the updated list into string to export:
            qemu_checkpoints_list="${qemu_checkpoints_list[@]}"

            # Rebuilds the cpt file with the parsed new list of checkpoints:
            echo "[\"${qemu_checkpoints_list// /\", \"}\"]" > $LOCAL_BACKUP_PATH/$domain/$domain.cpt

            # Backup folder integrity is OK:
            backup_folder_integrity="true"

            # Backup chain integrity is assumed as OK:
            backup_chain_integrity="true"

            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)

        else # All other scenarios, where checking backup folder and/or chain failed...

            # Delete checkpoints and/or bitmaps, depending on the case:

            if [[ $(domain_state $domain) == "shut off" ]]; then

                # Deletes checkpoints metadata, if any detected:
                echo "$domain: Pruning any existing checkpoints metadata..."
                domain_delete_checkpoint $domain --all --metadata
                delete_checkpoint_status=$?

                for image in $(domain_img_paths_list $domain); do

                    # Then deletes all bitmaps, in all image disks:
                    echo "$domain: Pruning any existing bitmaps on disk image $image..."

                    disk_image_bitmap_delete $image --all
                    delete_bitmap_status=$?

                    # When issues are found deleting bitmaps, cancel the entire operation:
                    [[ $delete_bitmap_status -eq 1 ]] && break
                done

            elif [[ $(domain_state $domain) == "running" && $unraid_scenario != true ]]; then

                # Deletes checkpoints (and bitmaps) if any detected:
                echo "$domain: Pruning any existing checkpoints..."
                domain_delete_checkpoint $domain --all
                delete_checkpoint_status=$?

            elif [[ $(domain_state $domain) == "running" && $unraid_scenario == true ]]; then

                # unraid_scenario. Only does something if is allowed to perform a power cycle on VM:

                if [[ ! -z $VM_ALLOW_POWERCYCLE ]]; then

                    domain_shutdown $domain --wait $VM_WAIT_TIME
                    shutdown_status=$?

                    if [[ $shutdown_status -eq 0 ]]; then

                        domain_shutdown_success_list+=($domain)

                        for image in $(domain_img_paths_list $domain); do

                            # Then deletes all bitmaps, in all image disks:
                            echo "$domain: Pruning any existing bitmaps on disk image $image..."

                            disk_image_bitmap_delete $image --all
                            delete_bitmap_status=$?

                            # When issues are found deleting bitmaps, cancel the entire operation:
                            [[ $delete_bitmap_status -eq 1 ]] && break
                        done
                    fi
                fi
            fi

            # Assign VM and/or backup chain to the correspondent list, according the results:

            case $delete_checkpoint_status in

                0|2)
                    if [[ $(domain_state $domain) == "shut off" ]]; then

                        case $delete_bitmap_status in

                            0|2)    # Checkpoints metadata and bitmaps deleted successfully (or nothing to delete).
                                    # Add to broken backup chain list:
                                    broken_backup_chain_list+=($domain)
                            ;;

                            *)  # There were problems when deleting bitmaps.
                                # This VM has issues and user action is required:
                                domain_failed_list+=($domain)
                            ;;
                        esac

                    elif [[ $(domain_state $domain) == "running" ]]; then

                        # Checkpoints deleted successfully (or nothing to delete).
                        # Add to broken backup chain list:
                        broken_backup_chain_list+=($domain)
                    fi
                ;;

                1)  # There were problems when deleting checkpoints.
                    # This VM has issues and user action is required:
                    domain_failed_list+=($domain)
                ;;

                "") # unraid_scenario (no checkpoints were ever processed):

                    case $shutdown_status in

                        0)
                            case $delete_bitmap_status in

                                0|2)    # Bitmaps deleted successfully (or nothing to delete).
                                        # Add to broken backup chain list:
                                        broken_backup_chain_list+=($domain)
                                ;;

                                1)
                                    # There were problems when deleting bitmaps.
                                    # This VM has issues and user action is required:
                                    domain_failed_list+=($domain)
                                ;;
                            esac
                        ;;

                        1)  # There were problems when shutting down the VM.
                            # This VM may either have issues, or just taking too long into shut down.
                            # User action is required (but may also shut down itself eventually):
                            domain_shutdown_needed_list+=($domain)
                            echo "$domain: User action is required"
                        ;;

                        "") # No permissions to power cycle the VM are given (no shutdown was ever attempted):
                            # User action is required:
                            domain_shutdown_needed_list+=($domain)
                            echo "$domain: User action is required (env variable VM_ALLOW_POWERCYCLE not set)"
                        ;;
                    esac
                ;;
            esac
        fi

        #------------------------------------------------------------------------------
        # Stage 4: Process backup folders that cannot be used anymore by archiving or deleting, accordingly:
        #------------------------------------------------------------------------------
        if [[ $backup_chain_integrity != true ]]; then

            if [[ $backup_folder_integrity == false ]]; then

                # Delete non-recoverable backup folder:
                rm -rf $LOCAL_BACKUP_PATH/$domain

            else
                # Archive any total or partially restorable backup:
                archive_backup $LOCAL_BACKUP_PATH/$domain
            fi

            # If a rsync path is set, archive any existing existing remote backup:
            [[ ! -z $RSYNC_BACKUP_PATH ]] && archive_remote_backup $RSYNC_BACKUP_PATH/$domain
        fi

        # Increases the index to check the next VM:
        ((i++))
    done

    # Cleanse CHECK_BACKUPS_LIST, since all VMs were processed, and copied into correspondent lists:
    unset CHECK_BACKUPS_LIST

    # Appends broken backup chains for new backup chain creation:
    CREATE_BACKUP_CHAIN_LIST+=(${broken_backup_chain_list[@]})

    # Appends successfully shut down VMs to the global list in need to be started:
    POWEREDOFF_VMS_LIST+=(${domain_shutdown_success_list[@]})

    # Appends preserved backup chains to scheduled backups directly:
    SCHEDULED_BACKUPS_LIST+=(${preserved_backup_chain_list[@]})

    # Appends VMs failed to shutdown (by user's setting or by fail) to request user action:
    SHUTDOWN_REQUIRED_VMS_LIST+=(${domain_shutdown_needed_list[@]})

    # Appends failed to shut down VMs to failed VMs list:
    FAILED_VMS_LIST+=(${domain_failed_list[@]})

    echo ""
    echo "Backup Chain Integrity Summary:"
    echo ""
    echo "Added to Scheduled Backups:   ${preserved_backup_chain_list[@]:-"None"}"
    echo "In need of new backup chain:  ${broken_backup_chain_list[@]:-"None"}"
    [[ ! -z $VM_ALLOW_POWERCYCLE ]] && \
    echo "Into automatic power cycle:   ${domain_shutdown_success_list[@]:-"None"}" || \
    echo "Manual shut down is required: ${domain_shutdown_needed_list[@]:-"None"}"
    echo ""
    [[ -z ${domain_failed_list[@]} ]] && \
    echo "All Backup Chains Checked!" \ ||
    echo "Failed Power Cycle:           ${domain_failed_list[@]:-"None"}"
}

###############################################################################

# User variables defaults (details on README.md):
#------------------------------------------------------------------------------

BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-"@daily"}
CHECK_BACKUPS_INTEGRITY=${CHECK_BACKUPS_INTEGRITY:-""}
LOCAL_BACKUP_CHAINS_TO_KEEP=${LOCAL_BACKUP_CHAINS_TO_KEEP:-"1"}
LOCAL_BACKUP_PATH=${LOCAL_BACKUP_PATH:-"/backups"}
LOGFILE_PATH=${LOGFILE_PATH:-"/logs/vm-babysitter.log"}
SCHEDULE_LOGFILE_PATH=${SCHEDULE_LOGFILE_PATH:-"/logs/scheduled-tasks.log"}
LOGROTATE_CONFIG_PATH=${LOGROTATE_CONFIG_PATH:-"/tmp/logrotate.d/vm-babysitter"}
LOGROTATE_SCHEDULE=${LOGROTATE_SCHEDULE:-"@daily"}
LOGROTATE_SETTINGS=${LOGROTATE_SETTINGS:-"  compress\n  copytruncate\n  daily\n  dateext\n  dateformat .%Y-%m-%d.%H:%M:%S\n  missingok\n  notifempty\n  rotate 30"}
MAX_BACKUPS_PER_CHAIN=${MAX_BACKUPS_PER_CHAIN:-"30"}
RSYNC_ARGS=${RSYNC_ARGS:-"-a"}
RSYNC_BACKUP_CHAINS_TO_KEEP=${RSYNC_BACKUP_CHAINS_TO_KEEP:-"2"}
RSYNC_BACKUP_PATH=${RSYNC_BACKUP_PATH:-""}
SSH_OPTIONS=${SSH_OPTIONS:-"-q -o IdentityFile=/private/hostname.key -o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"}
[[ -f /usr/share/zoneinfo/$TZ ]] && local_timezone_file="/usr/share/zoneinfo/$TZ"
UNRAID_NOTIFY_HOST=${UNRAID_NOTIFY_HOST:-"localhost"}
VIRTNBDBACKUP_ARGS=${VIRTNBDBACKUP_ARGS:-""}
VM_ALLOW_BACKUP_CHAIN_FIX=${VM_ALLOW_BACKUP_CHAIN_FIX:-""}
VM_ALLOW_POWERCYCLE=${VM_ALLOW_POWERCYCLE:-""}
VM_AUTOSTART_LIST=${VM_AUTOSTART_LIST:-""}
VM_IGNORED_LIST=${VM_IGNORED_LIST:-""}
[[ ! -z $VM_RAM_LIMIT ]] && VM_RAM_LIMIT=$(numfmt --from=iec --to-unit=1Ki ${VM_RAM_LIMIT^^})
VM_WAIT_TIME=${VM_WAIT_TIME:-"60"}

# Internal Variables:
#------------------------------------------------------------------------------

# Temporal crontab file (to be loaded for cron)
crontab_file="/tmp/crontab"

# Storing file for bash like lists shared between this script and the scheduler:
external_vars="/tmp/vm-babysit-vars"

# Main library where common functions and some variables are loaded:
functions_path="/usr/local/bin/vm-functions"

# Path of the scheduler script (run by cron):
backup_script_path="/usr/local/bin/vm-backup"

# Main execution:
###############################################################################

# Redirect system stdout and stderr to $LOGFILE_PATH:
exec > >(tee -a $LOGFILE_PATH) 2> >(tee -a $LOGFILE_PATH >&2)

# Load functions:
source $functions_path

# Create internal folders in case don't exist:
[[ ! -d $( dirname $LOGFILE_PATH) ]] && mkdir -p $(dirname $LOGFILE_PATH)

# Sets the local timezone (if ENV TZ was set):
if [[ ! -z $local_timezone_file ]]; then

    ln -fs $local_timezone_file /etc/localtime
    &> /dev/null dpkg-reconfigure -f noninteractive tzdata
fi

# Catches the signal sent from docker to stop execution:
# The most gracefully way to stop this container is with:
# 'docker kill --signal=SIGTERM <docker-name-or-id>'
trap 'stop_container' SIGTERM


echo "############################################################################################################"
echo "Container started at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))"
echo "############################################################################################################"

#------------------------------------------------------------------------------
# 1. Check input parameters (exits on error)
#------------------------------------------------------------------------------
check_variables
variables_status=$?

#------------------------------------------------------------------------------
# 2. Only when input parameters doesn't require to restart the container, it continues the rest of the checks:
#------------------------------------------------------------------------------

if [[ $variables_status -eq 0 ]]; then

    # Create logrotate config:
    #------------------------------------------------------------------------------

    echo "Configuring logrotate..."

    # Create directory if doesn't exist:
    mkdir -p $(dirname $LOGROTATE_CONFIG_PATH)

    # Parse logrotate configuration (overwriting, if any previous):
    echo -e "$LOGFILE_PATH \n $SCHEDULE_LOGFILE_PATH {\n$LOGROTATE_SETTINGS\n}" > $LOGROTATE_CONFIG_PATH

    # Create/update Cron task for VMs to be (progressively) included in $scheduled_backups_list:
    #------------------------------------------------------------------------------

    echo "Configuring $(basename $backup_script_path)..."

    # Silently deletes any previous cron task:
    &> /dev/null crontab -r

    # Parses the actual cron task needed to run to $crontab_file
    # (Including ENV vars not being read from cron's environment):
    cat << end_of_crontab > $crontab_file
# Values below are refreshed upon container (re)start:

# Main environment is bash:
SHELL=/bin/bash

# Search paths for binaries and scripts:
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# Environment variables passed through Docker:
LOCAL_BACKUP_PATH="$LOCAL_BACKUP_PATH"
MAX_BACKUPS_PER_CHAIN="$MAX_BACKUPS_PER_CHAIN"
RSYNC_BACKUP_PATH="$RSYNC_BACKUP_PATH"
RSYNC_ARGS="$RSYNC_ARGS"
SCHEDULE_LOGFILE_PATH="$SCHEDULE_LOGFILE_PATH"
SSH_OPTIONS="$SSH_OPTIONS"
VIRTNBDBACKUP_ARGS="$VIRTNBDBACKUP_ARGS"
UNRAID_NOTIFY_HOST="$UNRAID_NOTIFY_HOST"

# Paths for functions and (shared) dynamic variables:
functions_path="$functions_path"
external_vars="$external_vars"

# Schedule for $(basename $backup_script_path):
$BACKUP_SCHEDULE $backup_script_path

# Schedule for logrotate:
$LOGROTATE_SCHEDULE /usr/sbin/logrotate $LOGROTATE_CONFIG_PATH
end_of_crontab

    # Sets the cron task:
    crontab $crontab_file

    # Finally, runs cron and sends to background:
    echo "Starting Cron..."
    cron -f -l -L2 &

    # 2.3 Initializes a file with variables externally stored, to be shared with the scheduler:
    #------------------------------------------------------------------------------
    cat << end_of_external_variables > $external_vars
# Shared values between $(basename $0) and $backup_script_path. DO NOT EDIT!:

# Dynamic arrays:
FAILED_VMS_LIST=()
SCHEDULED_BACKUPS_LIST=()
end_of_external_variables

    # 3. Begin monitorization for VMs in lists, performing operations as required:
    #------------------------------------------------------------------------------

    echo ""
    echo "############################################################################################################"
    echo "Monitoring mode started"

    while true; do

        # Maximum standby period for monitoring should not exceed 10 seconds in any case,
        # because it could ignore SIGTERM from Docker, thus being killed with SIGKILL:
        sleep 1

        # (Re)reads all external variables (presumably modified by $backup_script_path):
        #------------------------------------------------------------------------------
        source $external_vars

        if [[ -n ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ]]; then

            # Check for VMs which are in need of shutdown.
            # (This normally happens when the user took the action, or when a VM took more than $VM_WAIT_TIME secs to shutdown):
            #------------------------------------------------------------------------------
            i=0
            for domain in ${SHUTDOWN_REQUIRED_VMS_LIST[@]}; do

                if [[ $(domain_state $domain) == "shut off" ]]; then

                    # Add VM to the same list of VMs to be powered on when monitor starts:
                    POWEREDOFF_VMS_LIST+=($domain)

                    # Move to main queue for check:
                    CHECK_BACKUPS_LIST+=($domain)
                    unset SHUTDOWN_REQUIRED_VMS_LIST[$i]

                    # Rebuild indexes into the array:
                    SHUTDOWN_REQUIRED_VMS_LIST=(${SHUTDOWN_REQUIRED_VMS_LIST[@]})

                    [[ $(os_is_unraid) == yes ]] && unraid_notify "normal" "VM-Babysitter" "Required shut down detected" "Resuming check/backup for $domain"
                fi
            done
        fi

        # (Re)creates DOMAINS_LIST, a list of non-transient domains that aren't into any of the queues from past iterations:
        create_domains_list

        if [[ -n ${DOMAINS_LIST[@]} || -n ${CHECK_BACKUPS_LIST[@]} ]]; then

            # Create the global list of VMs to be displayed:
            ONGOING_CHECK_LIST=(${DOMAINS_LIST[@]} ${CHECK_BACKUPS_LIST[@]})

            echo ""
            echo "____________________________________________________________________________________________________________"
            echo "Automatic check for VM(s) ${ONGOING_CHECK_LIST[@]} in progress..."


            # Check for virtual drives status on detected VMs:
            #------------------------------------------------------------------------------
            [[ -n ${DOMAINS_LIST[@]} ]] && check_domains

            # Check for backup chain integrity on all VMs:
            #------------------------------------------------------------------------------
            [[ -n ${CHECK_BACKUPS_LIST[@]} ]] && check_backups

            # Turns on all VMs that was, either declared in VM_AUTOSTART_LIST or previously shutdown for checks:
            #------------------------------------------------------------------------------
            if [[ -n ${POWEREDOFF_VMS_LIST[@]} ]]; then

                echo ""
                i=0
                for domain in ${POWEREDOFF_VMS_LIST[@]}; do

                    if [[ $(domain_state $domain) != running ]]; then

                        # Turn on the VM. Do not wait for Guest's QEMU agent:
                        domain_start $domain --nowait
                    fi

                    # Remove the VM from the list is being read:
                    unset POWEREDOFF_VMS_LIST[$i]

                    # Rebuild indexes into the array:
                    POWEREDOFF_VMS_LIST=(${POWEREDOFF_VMS_LIST[@]})

                    # Increases the counter:
                    ((i++))
                done
            fi

            # Create new backup chains for VMs that require it:
            #------------------------------------------------------------------------------
            if [[ -n ${CREATE_BACKUP_CHAIN_LIST[@]} ]]; then

                # Create new backup chains with backup script (on-demand mode):
                vm-backup ${CREATE_BACKUP_CHAIN_LIST[@]}

                # Reload external variables to gather changes on lists:
                source $external_vars
            fi

            # 3.4 Show status report and notify about user actions, if required:
            #------------------------------------------------------------------------------
            echo ""
            echo "############################################################################################################"
            echo "All VMs with status changed finished to be processed at $(date "+%Y-%m-%d %H:%M:%S")"
            echo ""
            echo "VM-Babysitter Global Summary:"
            echo ""
            echo "Current Scheduled Backups:    ${SCHEDULED_BACKUPS_LIST[@]:-"None"}"
            echo "Manual Shut Down Required:    ${SHUTDOWN_REQUIRED_VMS_LIST[@]:-"None"}"
            echo "Misbehaving Virtual Machines: ${FAILED_VMS_LIST[@]:-"None"}"

            if [[ -n ${SCHEDULED_BACKUPS_LIST[@]} && -z ${SHUTDOWN_REQUIRED_VMS_LIST[@]} && -z ${FAILED_VMS_LIST[@]} ]]; then

                echo ""
                echo "All Virtual Machines Running Under Incremental Backups!"

            elif [[ ! -z ${SHUTDOWN_REQUIRED_VMS_LIST[@]} || ! -z ${FAILED_VMS_LIST[@]} ]]; then

                user_action_message="USER ACTION IS REQUIRED!"

                echo ""
                echo $user_action_message

                if [[ ! -z ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ]]; then

                    shutdown_required_message="As env variable 'VM_ALLOW_POWERCYCLE' is not set, a manual Shut Down is required for VM(s): ${SHUTDOWN_REQUIRED_VMS_LIST[@]} 'As Soon As Possible'. This is required prior to perform backup integrity checks and cannot procced without that. Once shut down, the program will perform the checks and will restart involved VM(s) automatically."

                    echo "WARNING: $shutdown_required_message"
                    [[ $(os_is_unraid) == yes ]] && unraid_notify "warning" "VM-Babysitter" "$user_action_message" "$shutdown_required_message"
                fi

                if [[ ! -z ${FAILED_VMS_LIST[@]} ]]; then

                    failed_vms_message="Unexpected failure during check/backup operation involving VM(s): ${FAILED_VMS_LIST[@]}. Human attention is required 'As Soon As Possible' to solve the situation. Any scheduled backup or further alert for involved VM(s) is suspended until this container had been restarted. Check $(basename $LOGFILE_PATH) in this host for detailed logs."


                    echo  "ERROR: $failed_vms_message"
                    [[ $(os_is_unraid) == yes ]] && unraid_notify "alert" "VM-Babysitter" "$user_action_message" "$failed_vms_message"
                fi
            fi

            # 3.5 Marks ongoing check as finished and exports variables to be used on scheduled backups (entering in 'silent' mode):
            #------------------------------------------------------------------------------

            # Sed can't expand arrays correctly. Convert variables to be exported into strings.
            # This will be reverted at next block iteration, by sourcing such external variables as arrays:
            FAILED_VMS_LIST="${FAILED_VMS_LIST[@]}"
            SCHEDULED_BACKUPS_LIST="${SCHEDULED_BACKUPS_LIST[@]}"

            # Export the variables:
            sed -i \
            -e "s/FAILED_VMS_LIST=.*/FAILED_VMS_LIST=($FAILED_VMS_LIST)/" \
            -e "s/SCHEDULED_BACKUPS_LIST=.*/SCHEDULED_BACKUPS_LIST=($SCHEDULED_BACKUPS_LIST)/" \
            $external_vars
        fi

        # Restarts the loop, until SIGTERM or SIGKILL are received from Docker...
        #------------------------------------------------------------------------------
    done

else
    # Initial checks have proven non-recoverable errors:
    failed_initialize_message="Could not initialize due to errors! Check $(basename $LOGFILE_PATH) in this host for detailed logs."

    echo "ERROR: $failed_initialize_message"
    [[ $(os_is_unraid) == yes ]] && unraid_notify "alert" "VM-Babysitter" "Failed to start container" "$failed_initialize_message"

    stop_container
fi
