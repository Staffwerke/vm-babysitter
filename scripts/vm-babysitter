#!/bin/bash

###############################################################################
# Specific procedures:
###############################################################################

#------------------------------------------------------------------------------
# Attempts to stop the container gracefully in case of receive SIGTERM signal from Docker:
#------------------------------------------------------------------------------
stop_container()
{
    echo "############################################################################################################"
    echo "SIGTERM signal received at: $(date "+%Y-%m-%d %H:%M:%S")"
    # To DO: Terminate or kill background processes before to exit.
    echo "Container Stopped."
    echo ""
    exit 0
}

#------------------------------------------------------------------------------
# Checks VMs in CHECK_BACKUPS_LIST for backup chain integrity (and puts VMs at point of create new backup chains, if possible):
#------------------------------------------------------------------------------
check_backups()
{
    # Local variables used for flow control:
    local backup_chain_integrity
    local folder_checkpoints_list
    local backup_chain_full
    local backup_folder_integrity
    local existing_bitmaps_list
    local powercycle_status
    local qemu_checkpoints_list
    local recoverable_backup_chain
    local virtnbdrestore_status

    # Lists to add VMs (and summarize at the end):
    local broken_or_full_backup_chain_list
    local domain_powercycle_failed_list
    local domain_powercycle_needed_list
    local preserved_backup_chain_list

    if [[ $(os_is_unraid) == yes ]]; then

        for domain in $(domains_list); do

            # When OS is Unraid, look for checkpoints in all VMs, only stopping if it finds something:
            [[ ! -z $(domain_checkpoint_list $domain) ]] && { local checkpoints_found="yes"; break; }

        done

        # If no checkpoints were found across the server, set unraid_scenario flag:
        [[ -z $checkpoints_found ]] && { local unraid_scenario="true"; echo "NOTICE: Unraid OS detected with no QEMU checkpoints at all. Very likely, this server has just been restarted (or this is the first run and no VMs has been backed up yet)"; }
    fi

    echo ""
    echo "____________________________________________________________________________________________________________"
    echo "Checking backup chain ingtegrity on Virtual machines: ${CHECK_BACKUPS_LIST[@]}"

    local i=0
    for domain in ${CHECK_BACKUPS_LIST[@]}; do

        backup_chain_integrity=""
        backup_chain_full=""
        backup_folder_integrity=""
        powercycle_status=""
        recoverable_backup_chain=""
        virtnbdrestore_status=""

        echo ""

        #------------------------------------------------------------------------------
        # Stage 1: Check backup integrity:
        #------------------------------------------------------------------------------

        # Discard non existing, and incomplete folders:
        #------------------------------------------------------------------------------
        if [[ ! -d $LOCAL_BACKUP_PATH/$domain || ! -f $LOCAL_BACKUP_PATH/$domain/$domain.cpt || ! -d $LOCAL_BACKUP_PATH/$domain/checkpoints ]]; then

            backup_folder_integrity="false"
            echo "$domain: No backup chain folder detected, or internal structure is inconsistent"

        # Interrupted operations (may recover backup chain in some cases):
        #------------------------------------------------------------------------------
        elif [[ ! -z $(find $LOCAL_BACKUP_PATH/$domain -type f -name "*.partial") ]]; then

            folder_checkpoints_list=($(backup_checkpoint_list $LOCAL_BACKUP_PATH/$domain))

            if [[ ${#folder_checkpoints_list[@]} -eq 1 ]]; then

                # Backup chain is useless:
                backup_folder_integrity="false"
                echo "$domain: A backup operation was previously cancelled. This backup chain is unrecoverable, therefore will be removed"

            else

                if [[ -z $VM_ALLOW_BACKUP_CHAIN_FIX || $unraid_scenario = true ]]; then

                    # Backup chain may be recoverable, but either user didn't allow to check, or it's unraid_scenario:
                    recoverable_backup_chain="false"
                    echo "$domain: A backup operation was previously cancelled. Backup chain is partially recoverable, but will not attempt to fix the backup for further usage. A new backup chain will be created instead."

                else

                    # Backup chain may be recoverable, and will attempt to fix it:
                    recoverable_backup_chain="true"
                    echo "$domain: An backup operation was previously cancelled. Backup is partially recoverable and it will attempt to fix it and restore the current backup chain for further use"
                fi
            fi

        # Backup folder integrity looks 'OK', at a first glance:
        #------------------------------------------------------------------------------
        else

            # Apply the 1st part of retention policy, by retireving the number of existing checkpoints in the backup chain:
            #------------------------------------------------------------------------------
            local num_of_backup_checkpoints=$(($(backup_checkpoint_list $LOCAL_BACKUP_PATH/$domain --num) - 1))

            if [[ ! -z $MAX_BACKUPS_PER_CHAIN && $num_of_backup_checkpoints -ge $MAX_BACKUPS_PER_CHAIN ]]; then

                # Backup chain is full, a new one will be created:
                backup_chain_full="true"
                echo "$domain: Backup chain contains $num_of_backup_checkpoints checkpoint(s), and the limit set in environment variable MAX_BACKUPS_PER_CHAIN is $MAX_BACKUPS_PER_CHAIN. A new backup chain will be created"

            else
                # Display checkpoints limits, if set:
                [[ ! -z $MAX_BACKUPS_PER_CHAIN ]] && local display_checkpoint_limit=" out of $MAX_BACKUPS_PER_CHAIN"

                echo "$domain: Detected $num_of_backup_checkpoints$display_checkpoint_limit checkpoints"
            fi

            # Unless backup chain is full, perform data check integrity if enabled by user (and if backup includes checksum files):
            #------------------------------------------------------------------------------
            if [[ ! -z $CHECK_BACKUPS_INTEGRITY && $backup_chain_full != true ]]; then

                # It only has sense to scan for backup data integrity on backups that has implemented checksum feature
                if [[ ! -z $(find $LOCAL_BACKUP_PATH/$domain -name "*.chksum") ]]; then

                    # Cheksums files are available. Check for backup's data integrity:
                    echo "$domain: Checking backup data integrity..."
                    virtnbdrestore -i $LOCAL_BACKUP_PATH/$domain -o verify

                    # Get the last status of virtnbdrestore:
                    virtnbdrestore_status=$?

                    if [[ $virtnbdrestore_status -ne 0 ]]; then

                        # Backup chain is broken, a new one will be created:
                        backup_chain_full="true"

                        echo "$domain: Virtnbdrestore failed verifying the integrity of backup chain with status $virtnbdrestore_status. A new backup chain will be created"

                    else
                        echo "$domain: Backup data integrity is OK"
                    fi

                else
                    # No checksum files to compare:
                    echo "$domain: Data integrity check cancelled: No checksum files were found (saved with an old version of Virtnbdbackup?)"
                fi
            fi

            # Backup folder integrity is OK, always is not full or broken:
            [[ $backup_chain_full != true ]] && backup_folder_integrity="true"
        fi

        #------------------------------------------------------------------------------
        # Stage 2: Verify backup chain integrity:
        #------------------------------------------------------------------------------
        if [[ $backup_folder_integrity = true ]]; then

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoint_list $domain))

            # Read backup checkpoints:
            folder_checkpoints_list=($(backup_checkpoint_list $LOCAL_BACKUP_PATH/$domain))

            if [[ $(domain_state $domain) == "shut off" || $unraid_scenario == true ]]; then

                local backup_check_message="Performing a full check into its backup chain..."

                [[ $unraid_scenario == true ]] \
                    && echo "$domain: Is running (Unraid OS scenario). $backup_check_message" \
                    || echo "$domain: Is shut down. $backup_check_message"

                for image in $(domain_img_paths_list $domain); do

                    # Read bitmaps in each virtual image:
                    existing_bitmaps_list=($(disk_image_bitmap_list $image))

                    if [[ ${existing_bitmaps_list[@]} != ${folder_checkpoints_list[@]} ]]; then

                        backup_chain_integrity="false"
                        echo "$domain: Checkpoints and bitmaps lists on disk $image MISMATCH"

                        # Interrupt the current operation. Nothing else to do:
                        break

                    elif [[ ! -z ${existing_bitmaps_list[@]} ]]; then

                        # It is assumed bitmaps = checkpoints, and none is a empty list.

                        # Backup chain is OK, mark as preserved:
                        backup_chain_integrity="true"
                        echo "$domain: Checkpoints and bitmaps lists on disk $image MATCH"

                    else
                        backup_chain_integrity="false"
                        echo "$domain: ${#folder_checkpoints_list[@]} checkpoints found, and ${#existing_bitmaps_list[@]} bitmaps found on disk $image (backup chain is broken)"

                        # Interrupt the current operation. Nothing else to do:
                        break
                    fi
                done
            fi

            if [[ $(domain_state $domain) == "running" ]]; then

                echo "$domain: Is running, comparing QEMU and Backup's checkpoint lists..."

                if [[ ${qemu_checkpoints_list[@]} == ${folder_checkpoints_list[@]} ]]; then

                    # Backup chain integrity is OK:
                    backup_chain_integrity="true"
                    echo "$domain: QEMU and Backup checkpoints lists MATCH"

                else
                    # Mark the backup chain status as broken:
                    backup_chain_integrity="false"
                    echo "$domain: QEMU and Backup's checkpoint lists MISMATCH"
                fi
            fi
        fi

        #------------------------------------------------------------------------------
        # Stage 3: Process backup chain and files according with the scenarios and previous results:
        #------------------------------------------------------------------------------

        if [[ $backup_chain_integrity == true ]]; then

            # At this point, all checks has been successful.
            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)

        elif [[ $recoverable_backup_chain == true ]]; then

            # Read qemu checkpoints:
            qemu_checkpoints_list=($(domain_checkpoints_list $domain))

            if [[ $(domain_state $domain) == "shut off" ]]; then

                echo "$domain: Removing damaged checkpoint metadata: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoint $domain ${qemu_checkpoints_list[-1]} --metadata

                for image in $(domain_img_paths_list $domain); do

                    echo "$domain: Deleting insecure bitmap ${existing_bitmaps_list[-1]} on disk image $image..."
                    disk_image_delete_bitmap $image ${existing_bitmaps_list[-1]}
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                echo "$domain: Removing damaged checkpoint: ${qemu_checkpoints_list[-1]} ..."
                domain_delete_checkpoint $domain ${qemu_checkpoints_list[-1]}
            fi

            echo "$domain: Fixing backup chain data in $LOCAL_BACKUP_PATH/$domain ..."

            # Deletes any related file with the damaged checkpoint:
            find $LOCAL_BACKUP_PATH/$domain -name "*${qemu_checkpoints_list[-1]}*" -type f -delete

            # Deletes the last checkpoint from the list:
            unset qemu_checkpoints_list[-1]

            # Turn the updated list into string to export:
            qemu_checkpoints_list="${qemu_checkpoints_list[@]}"

            # Rebuilds the cpt file with the parsed new list of checkpoints:
            echo "[\"${qemu_checkpoints_list// /\", \"}\"]" > $LOCAL_BACKUP_PATH/$domain/$domain.cpt

            # Backup folder integrity is OK:
            backup_folder_integrity="true"

            # Backup chain integrity is assumed as OK:
            backup_chain_integrity="true"

            # Add VM to the list of preserved backup chain ones:
            preserved_backup_chain_list+=($domain)

        else # All other scenarios where checking backup folder/chain failed, or when backup chin is full...

            if [[ $(domain_state $domain) == "shut off" || $unraid_scenario == true ]]; then

                if [[ ! -z ${qemu_checkpoints_list[@]} ]]; then

                    # Deletes checkpoints metadata if any detected:
                    echo "$domain: Pruning existing checkpoints in QEMU..."
                    domain_delete_checkpoint $domain --all --metadata
                fi

                for image in $(domain_img_paths_list $domain); do

                    # Then deletes all bitmaps, in all image disks:
                    echo "$domain: Deleting bitmaps on disk image $image..."

                    for bitmap in $(disk_image_bitmap_list $image); do

                        disk_image_delete_bitmap $image $bitmap
                    done
                done

            elif [[ $(domain_state $domain) == "running" ]]; then

                # Deletes checkpoints (and bitmaps) if any detected:
                echo "$domain: Pruning existing checkpoints in QEMU..."
                domain_delete_checkpoint $domain --all
            fi

            if [[ $backup_folder_integrity == false ]]; then

                # Useless or unexistent backup chain folder. Delete it:
                rm -rf $LOCAL_BACKUP_PATH/$domain

                # if remote path is set, keep existing remote ones without apply any retention policy:
                [[ ! -z $RSYNC_BACKUP_PATH ]] && archive_remote_backup $RSYNC_BACKUP_PATH/$domain

            else # All scenarios where backup chain is still able to be partial or totally restored...

                # Processes backup for being archived locally (and remotely, if set) applying 2nd part of retention policy:
                archive_backup $LOCAL_BACKUP_PATH/$domain $LOCAL_BACKUP_CHAINS_TO_KEEP
                [[ ! -z $RSYNC_BACKUP_PATH ]] && archive_remote_backup $RSYNC_BACKUP_PATH/$domain $RSYNC_BACKUP_CHAINS_TO_KEEP
            fi

            if [[ $unraid_scenario == true && ! -z $VM_ALLOW_POWERCYCLE && $(domain_state $domain) == "running" ]]; then

                # unraid_scenario where user allows automatic power cycle and VM is running,
                # perform full power cycle of VM:
                echo "$domain: Applying changes onto VM..."
                domain_powercycle $domain $VM_WAIT_TIME
                powercycle_status=$?

                # Depending on powercycle operation result,
                # Add VM to the list of broken/failed backup chain ones,
                # Or notify the user about VM misbehavior:
                [[ $powercycle_status -eq 0 ]] \
                    && broken_or_full_backup_chain_list+=($domain) \
                    || domain_powercycle_failed_list+=($domain)

            elif [[ $unraid_scenario == true && -z $VM_ALLOW_POWERCYCLE && $(domain_state $domain) == "running" ]]; then

                # unraid_scenario where user forbids automatic power cycle and VM is running,
                # add to the list of VMs in need to be shut down manually:
                echo "$domain: User action is required (env variable VM_ALLOW_POWERCYCLE not set)"
                domain_powercycle_needed_list+=($domain)

            else # Any other scenario

                # Add shutdown VM to the list of broken/failed backup chain ones:
                broken_or_full_backup_chain_list[$i]=$domain
            fi
        fi

        # Increases the index to check the next VM:
        ((i++))
    done

    # Cleanse CHECK_BACKUPS_LIST, since all VMs were processed, and copied into correspondent lists:
    unset CHECK_BACKUPS_LIST

    # Appends preserved backup chains to scheduled backups directly:
    SCHEDULED_BACKUPS_LIST+=(${preserved_backup_chain_list[@]})

    # Appends broken and/or full backup chains for new backup chain creation:
    CREATE_BACKUP_CHAIN_LIST+=(${broken_or_full_backup_chain_list[@]})

    # Appends VMs failed to shutdown (by user's setting or by fail) to request user action:
    SHUTDOWN_REQUIRED_VMS_LIST+=(${domain_powercycle_needed_list[@]})

    # Appends failed to shut down VMs to failed VMs list:
    FAILED_VMS_LIST+=(${domain_powercycle_failed_list[@]})

    echo ""
    echo "Backup Chain Integrity Summary:"
    echo ""
    echo "Added to Scheduled Backups:   ${preserved_backup_chain_list[@]:-"None"}"
    echo "In need of new backup chain:  ${broken_or_full_backup_chain_list[@]:-"None"}"
    [[ ! -z $VM_ALLOW_POWERCYCLE ]] \
        && echo "Failed Power Cycle:           ${domain_powercycle_failed_list[@]:-"None"}" \
        || echo "Manual shut down is required: ${domain_powercycle_needed_list[@]:-"None"}"

    if [[ -z ${domain_powercycle_failed_list[@]} ]]; then

        echo ""
        echo "All Backup Chains Checked!"
    fi
}

#------------------------------------------------------------------------------
# Creates backup chains for VMs in CREATE_BACKUP_CHAIN_LIST, managing temporal RAM limits (when set) and powering on/off as necessary:
#------------------------------------------------------------------------------
create_backup_chain()
{
    # Local variables used for flow control:
    local original_ram_size
    local memlimit_active
    local rsync_status
    local virsh_status
    local virtnbdbackup_status

    # Lists to add VMs (and summarize at the end):
    local backup_chain_failed_list
    local backup_chain_success_list
    local domain_poweron_failed_list
    local domain_poweron_success_list
    local remote_sync_failed_list
    local remote_sync_success_list

    echo ""
    echo "___________________________________________________________________________________________________"
    echo "Creating Backup chains for Virtual machines: ${CREATE_BACKUP_CHAIN_LIST[@]}"

    [[ $(os_is_unraid) == yes ]] && unraid_notify "normal" "VM-Babysitter" "Backup chain creation" "In progress for ${CREATE_BACKUP_CHAIN_LIST[@]}. Avoid to stop/restart this container until further notice"

    local i=0
    for domain in ${CREATE_BACKUP_CHAIN_LIST[@]}; do

        original_ram_size=""
        memlimit_active="no"

        echo ""

        while true; do

            if [[ $(domain_state $domain) == running ]]; then

                # Only when VM is running, attempts to create a new backup chain:
                do_backup_chain $domain "full" $LOCAL_BACKUP_PATH $VIRTNBDBACKUP_ARGS

                # Get last status for virtnbdbackup:
                virtnbdbackup_status=$?

                if [[ $virtnbdbackup_status -eq 0 ]]; then

                    # Backup chain creation was successful!
                    backup_chain_success_list[$i]=$domain

                else
                    # Failed to create a new backup chain:
                    backup_chain_failed_list[$i]=$domain

                    # Delete partial files (unusable garbage):
                    rm -rf $LOCAL_BACKUP_PATH/$domain
                fi

                if [[ ${domain_poweron_success_list[$i]} == $domain ]]; then

                    # VM was previously shut off, revert to its previous state:
                    domain_shutdown $domain --nowait

                    if [[ $memlimit_active == yes ]]; then

                        # RAM was previously throttled. Reverting to its original values:
                        domain_setmem "$domain" "$original_ram_size"

                        [[ $? -eq 0 ]] \
                            && echo "$domain: Reverted RAM size to its original setting of ${original_ram_size// / \/ } KiB" \
                            || echo "WARNING: Failed to revert $domain RAM size to its original setting!"
                    fi
                fi

                if [[ ${backup_chain_success_list[$i]} == $domain && ! -z $RSYNC_BACKUP_PATH ]]; then

                    # Backup chain creation was successful and remote endpoint is (correctly) set,
                    # attempts to transfer changes via rsync:

                    echo "$domain: Copying new backup chain to configured remote mirror..."

                    # Transfer backup using SSH_OPTIONS and RSYNC_ARGS:
                    rsync $RSYNC_ARGS -e "ssh $SSH_OPTIONS" $LOCAL_BACKUP_PATH/$domain/ $RSYNC_BACKUP_PATH/$domain/

                    # Get last status for rsync:
                    rsync_status=$?

                    # Depending on success with rsync, will add the VM to $remote_success or $remote_fail
                    if [[ $rsync_status -eq 0 ]]; then

                        # When rsync exited normally, adds the VM to $remote_sync_success_list:
                        remote_sync_success_list[$i]+=$domain
                        echo "$domain: New backup chain was copied successfuly to configured remote mirror"

                    else
                        # Adds the VM to $remote_sync_failed_list list:
                        remote_sync_failed_list[$i]+=$domain
                        echo "$domain: Failed to copy new backup chain to configured remote mirror with status $rsync_status"
                    fi
                fi

                # Exits the loop:
                break

            else
                # VM is presumably shut off. Will attempts to start it:
                if [[ ! -z $VM_RAM_LIMIT ]]; then

                    # And needs to check how much memory uses by default, throttling it if limits are established:
                    if [[ $(domain_getmem $domain --max) -gt $VM_RAM_LIMIT ]]; then

                        # If max value is greater than the established limit, sets the VM RAM temporarily to such limit:
                        memlimit_active="yes"

                        # Saves apart original RAM sizes:
                        original_ram_size=$(domain_getmem $domain)

                        # Applies temporal values (according user input, but values are grabbed from virsh in KiB):
                        domain_setmem "$domain" "$VM_RAM_LIMIT"

                        [[ $? -eq 0 ]] \
                            && echo "$domain: RAM size temporarily throttled from ${original_ram_size// / \/ } to $VM_RAM_LIMIT KiB for this task" \
                            || echo "WARNING: Failed to set $domain RAM size to user defined limit of $VM_RAM_LIMIT KiB"
                    fi
                fi

                # Attempts to start the VM (awaits for VM's QEMU agent):
                domain_start $domain --wait $VM_WAIT_TIME

                # Get last status for virsh:
                virsh_status=$?

                if [[ $? -eq 0 ]]; then

                    domain_poweron_success_list[$i]=$domain

                    # Restarts the loop to chech VM under changed conditions:
                    continue

                else
                    # VM failed to power on. This is an abnormal situation:
                    echo "$domain: Failed to start with virsh status: $virsh_status (no backup chain could be created)"

                    domain_poweron_failed_list[$i]=$domain

                    # Breaks the loop:
                    break
                fi
            fi
        done

        # Increases the index to check the next VM:
        ((i++))
    done

    # Cleanse CREATE_BACKUP_CHAIN_LIST, since all VMs were processed, and copied into correspondent lists:
    unset CREATE_BACKUP_CHAIN_LIST

    # Appends successfuly created backup chains to scheduled backups:
    SCHEDULED_BACKUPS_LIST+=(${backup_chain_success_list[@]})

    # Appends failed backup chains to request manual check by the user:
    FAILED_VMS_LIST+=(${domain_poweron_failed_list[@]})
    FAILED_VMS_LIST+=(${backup_chain_failed_list[@]})

    # And shows the summary at the very end:

    echo ""
    echo "Backup Chain Creation Summary:"
    echo ""
    echo "Added to Scheduled Backups:        ${backup_chain_success_list[@]:-"None"}"
    echo "Could not create new backup chain: ${backup_chain_failed_list[@]:-"None"}"
    echo "Could not power on:                ${domain_poweron_failed_list[@]:-"None"}"

    if [[ ! -z $RSYNC_BACKUP_PATH ]]; then

        # Only show remote stats when remote endppoint is set:
        echo ""
        echo "Success copying to remote mirror: ${remote_sync_success_list[@]:-"None"}"
        echo "Could not copy to remote mirror:  ${remote_sync_failed_list[@]:-"None"}"
    fi

    if [[ -z ${backup_chain_failed_list[@]} && -z ${domain_poweron_failed_list[@]} && -z ${remote_sync_failed_list[@]} ]]; then

        echo ""
        echo "All Backup Chains Created!"

        [[ $(os_is_unraid) == yes ]] && unraid_notify "normal" "VM-Babysitter" "Backup chain creation" "Success"

    elif [[ $(os_is_unraid) == yes ]]; then

        [[ ! -z ${domain_poweron_failed_list[@]} ]] && unraid_notify "warning" "VM-Babysitter" "Virsh failed (last status: $virsh_status)" "Could not power on: ${domain_poweron_failed_list[@]}"

        [[ ! -z ${backup_chain_failed_list[@]} ]] && unraid_notify "warning" "VM-Babysitter" "Virtnbdbackup failed (last status: $virtnbdbackup_status)" "Could not create backup chain(s) for: ${backup_chain_failed_list[@]}"

        [[ ! -z ${remote_sync_failed_list[@]} ]] && unraid_notify "warning" "VM-Babysitter" "Rsync failed (last status: $rsync_status)" "Could not sync backup chain(s) of: ${remote_sync_failed_list[@]} (Will be re-attempted on next schedule)"
    fi
}

###############################################################################

# User variables defaults (details on README.md):
#------------------------------------------------------------------------------

BACKUP_SCHEDULE=${BACKUP_SCHEDULE:-"@daily"}
CHECK_BACKUPS_INTEGRITY=${CHECK_BACKUPS_INTEGRITY:-""}
LOCAL_BACKUP_CHAINS_TO_KEEP=${LOCAL_BACKUP_CHAINS_TO_KEEP:-""}
LOCAL_BACKUP_PATH=${LOCAL_BACKUP_PATH:-"/backups"}
LOGFILE_PATH=${LOGFILE_PATH:-"/logs/vm-babysitter.log"}
LOGROTATE_CONFIG_PATH=${LOGROTATE_CONFIG_PATH:-"/tmp/logrotate.d/vm-babysitter"}
LOGROTATE_SCHEDULE=${LOGROTATE_SCHEDULE:-"@daily"}
LOGROTATE_SETTINGS=${LOGROTATE_SETTINGS:-"  compress\n  copytruncate\n  daily\n  dateext\n  dateformat .%Y-%m-%d.%H:%M:%S\n  missingok\n  notifempty\n  rotate 30"}
MAX_BACKUPS_PER_CHAIN=${MAX_BACKUPS_PER_CHAIN:-"30"}
RSYNC_ARGS=${RSYNC_ARGS:-"-a"}
RSYNC_BACKUP_CHAINS_TO_KEEP=${RSYNC_BACKUP_CHAINS_TO_KEEP:-""}
RSYNC_BACKUP_PATH=${RSYNC_BACKUP_PATH:-""}
SSH_OPTIONS=${SSH_OPTIONS:-"-q -o IdentityFile=/private/hostname.key -o BatchMode=yes -o StrictHostKeyChecking=no -o ConnectTimeout=10"}
[[ -f /usr/share/zoneinfo/$TZ ]] && local_timezone_file="/usr/share/zoneinfo/$TZ"
UNRAID_NOTIFY_HOST=${UNRAID_NOTIFY_HOST:-"localhost"}
VIRTNBDBACKUP_ARGS=${VIRTNBDBACKUP_ARGS:-""}
VM_ALLOW_BACKUP_CHAIN_FIX=${VM_ALLOW_BACKUP_CHAIN_FIX:-""}
VM_ALLOW_POWERCYCLE=${VM_ALLOW_POWERCYCLE:-""}
VM_AUTOSTART_LIST=${VM_AUTOSTART_LIST:-""}
VM_IGNORED_LIST=${VM_IGNORED_LIST:-""}
[[ ! -z $VM_RAM_LIMIT ]] && VM_RAM_LIMIT=$(numfmt --from=iec --to-unit=1Ki ${VM_RAM_LIMIT^^})
VM_WAIT_TIME=${VM_WAIT_TIME:-"60"}

# Internal Variables:
#------------------------------------------------------------------------------

# Temporal crontab file (to be loaded for cron)
crontab_file="/tmp/crontab"

# Storing file for bash like lists shared between this script and the scheduler:
external_vars="/tmp/vm-babysit-vars"

# Main library where common functions and some variables are loaded:
functions_path="/usr/local/bin/vm-functions"

# Path of the scheduler script (run by cron):
scheduled_backup_script="/usr/local/bin/update_backup_chain"

# Main execution:
###############################################################################

# Redirect system stdout and stderr to $LOGFILE_PATH:
exec > >(tee -a $LOGFILE_PATH) 2> >(tee -a $LOGFILE_PATH >&2)

# Load functions:
source $functions_path

# Create internal folders in case don't exist:
[[ ! -d $( dirname $LOGFILE_PATH) ]] && mkdir -p $(dirname $LOGFILE_PATH)

# Sets the local timezone (if ENV TZ was set):
if [[ ! -z $local_timezone_file ]]; then

    ln -fs $local_timezone_file /etc/localtime
    &> /dev/null dpkg-reconfigure -f noninteractive tzdata
fi

# Catches the signal sent from docker to stop execution:
# The most gracefully way to stop this container is with:
# 'docker kill --signal=SIGTERM <docker-name-or-id>'
trap 'stop_container' SIGTERM

#------------------------------------------------------------------------------
# 1. Check input parameters (exits on error)
#------------------------------------------------------------------------------

echo "############################################################################################################"
echo "Container started at: $(date "+%Y-%m-%d %H:%M:%S") ($(cat /etc/timezone))"
echo "############################################################################################################"

# 1.1 Check DOMAINS_LIST:
#------------------------------------------------------------------------------
echo ""
echo "Querying for persistent Virtual machines from libvirt..."

# The initial list of VMs to work:
DOMAINS_LIST=($(domains_list))

if [[ ! -z ${DOMAINS_LIST[@]} ]]; then

    # 1.1.1 Check VM_IGNORED_LIST:
    #------------------------------------------------------------------------------
    if [[ ! -z $VM_IGNORED_LIST ]]; then

    echo ""

        for domain in $VM_IGNORED_LIST; do

            if [[ $(domain_exists $domain) == yes ]]; then

                # Remove the VM from DOMAINS_LIST
                unset DOMAINS_LIST[$(item_position $domain "DOMAINS_LIST")]

                # Rebuild indexes into the array:
                DOMAINS_LIST=(${DOMAINS_LIST[@]})

                echo "$domain: Into VM_IGNORED_LIST, therefore ignored"

            else
                echo "WARNING: VM '$domain' in VM_IGNORED_LIST not found!"
            fi
        done
    fi

    # 1.1.2 Check VM_AUTOSTART_LIST:
    #------------------------------------------------------------------------------
    if [[ ! -z $VM_AUTOSTART_LIST ]]; then

    echo ""

    i=0
        for domain in $VM_AUTOSTART_LIST; do

            if [[ $(domain_exists $domain) == yes ]]; then

                # Add VM to the list of VMs to be powered on when monitor starts:
                POWEREDOFF_VMS_LIST+=($domain)
                echo "$domain: Into VM_AUTOSTART_LIST, therefore will be started (after initial check)"

            else
                echo "WARNING: VM $domain in VM_AUTOSTART_LIST not found!"
            fi
            ((i++))
        done
    fi

    # 1.1.3 Check DOMAINS_LIST VM's disk images:
    #------------------------------------------------------------------------------

    echo ""

    i=0
    for domain in ${DOMAINS_LIST[@]}; do

        drives_list=($(domain_drives_list $domain))
        if [[ ! -z ${drives_list[@]} ]]; then

            # Does have drives able to be backed up. Checks if such disk images are reachable inside the container:
            images_list=($(domain_img_paths_list $domain))
            for image in ${images_list[@]}; do

                if [[ ! -f $image ]]; then

                    FAILED_VMS_LIST+=($domain)

                    echo "ERROR: $domain's disk image: $image not found. Ensure you mount -and mirror- this correctly inside the container (e.g. '-v /common/path/to/vms/disks:/common/path/to/vms/disks')"

                elif [[ ! -r $image && ! -w $image ]]; then

                    FAILED_VMS_LIST+=($domain)

                    echo "ERROR: $domain's disk image: $image has permission issues (cannot be read or written)"
                fi
            done
        else

            # Exclude VMs without disks images that can be backed up:
            unset DOMAINS_LIST[$i]

            # Rebuild indexes into the array:
            DOMAINS_LIST=(${DOMAINS_LIST[@]})

            echo "WARNING: VM $domain has no drives that can be backed up, therefore will be ignored"
        fi
        ((i++))
    done

    # Re-check DOMAINS_LIST after the above procedure (ignored and left aside by other issues):
    if [[ ! -z ${DOMAINS_LIST[@]} ]]; then

        # VMs are left after initial checks, then domain_list_status is correct:
        domains_list_status="OK"

        echo "Persistent Virtual machine(s) able to be backed up: ${DOMAINS_LIST[@]}"

    elif [[ ! -z ${FAILED_VMS_LIST[@]} ]]; then

        # No VMs are left, but one or more failed the initial checks:
        echo "ERROR: Issues detected with '${FAILED_VMS_LIST[@]}' that need to be solved before to run this container again"

    else
        echo "WARNING: After initial check, no persistent Virtual machines are left available for backup"
    fi
else
    # No VMs were found from the beginning:
    echo "ERROR: No persistent Virtual machines were found!"
fi

# 1.2 Check MAX_BACKUPS_PER_CHAIN
#------------------------------------------------------------------------------
# Check for LOCAL_BACKUP_CHAINS_TO_KEEP:
if [[ $MAX_BACKUPS_PER_CHAIN =~ [0-9] ]]; then

    # Is an integer number:
    max_backups_per_chain_status="OK"
    echo "Max backups to save per chain: $MAX_BACKUPS_PER_CHAIN"

elif [[ -z $MAX_BACKUPS_PER_CHAIN ]]; then

    # Was not set, warn the user about the potential issue:
    max_backups_per_chain_status="UNUSED"
    echo "WARNING: Environment variable MAX_BACKUPS_PER_CHAIN is not set. You should set a reasonable limit for the number of backups, or use the default value. Otherwise the backup chain will grow without any control, filling up your storage!"

else
    # Invalid value:
    max_backups_per_chain_status="FAILED"
    echo "ERROR: Incorrect value for environment variable MAX_BACKUPS_PER_CHAIN: '$MAX_BACKUPS_PER_CHAIN' <- It must be a natural integer"
fi

# 1.3 Check LOCAL_BACKUP_PATH
#------------------------------------------------------------------------------

echo ""

if  [[ -d $LOCAL_BACKUP_PATH ]]; then

    # $LOCAL_BACKUP_PATH found
    if  [[ -r $LOCAL_BACKUP_PATH && -w $LOCAL_BACKUP_PATH ]]; then

        # $LOCAL_BACKUP_PATH has read/write permissions.
        local_backup_path_status="OK"
        echo "Backups main path internally set to: $LOCAL_BACKUP_PATH"

        # Check for LOCAL_BACKUP_CHAINS_TO_KEEP:
        if [[ $LOCAL_BACKUP_CHAINS_TO_KEEP =~ [0-9] ]]; then

            # Is an integer number:
            echo "Max backup chains per VM to keep locally: $LOCAL_BACKUP_CHAINS_TO_KEEP"

        elif [[ -z $LOCAL_BACKUP_CHAINS_TO_KEEP ]]; then

            # Was not set:
            echo "Environment variable LOCAL_BACKUP_CHAINS_TO_KEEP not set. ALL recoverable backup chains will be archived locally"

        else
            # Invalid value (unsets the 'OK' status):
            unset local_backup_path_status
            echo "ERROR: Incorrect value for environment variable LOCAL_BACKUP_CHAINS_TO_KEEP: '$LOCAL_BACKUP_CHAINS_TO_KEEP' <- It must be a natural integer"
        fi

    else
        echo "ERROR: Backups main path: $LOCAL_BACKUP_PATH <- Permission issues (cannot be read or written)"
    fi

else
    echo "ERROR: Backups main path: $LOCAL_BACKUP_PATH  <- Not found inside the container. Ensure you mount this correctly (e.g. '-v /path/to/backups-main-path:/$LOCAL_BACKUP_PATH')"
fi

# 1.4 Check RSYNC_BACKUP_PATH
#------------------------------------------------------------------------------

if [[ -z $RSYNC_BACKUP_PATH ]]; then

    rsync_backup_path_status="UNUSED"
    echo ""
    echo "Environment variable RSYNC_BACKUP_PATH not set. No remote backup endpoint will be used"

elif [[ $RSYNC_BACKUP_PATH == *@*:/* ]]; then

    echo ""

    # Apparently includes correct remote login and path. Separates ssh login from remote path:
    rsync_server=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f1)
    rsync_path=$(echo $RSYNC_BACKUP_PATH | cut -d':' -f2)

    # Attempts to comunicate with the remote host:
    ssh $SSH_OPTIONS $rsync_server "exit 0"
    rsync_server_status=$?

    if [[ $rsync_server_status == 0 ]]; then

        # Attempts to perform similar checks as with $LOCAL_BACKUP_PATH:
        rsync_backup_path_status=$(ssh $SSH_OPTIONS $rsync_server "[[ -d $rsync_path && -r $rsync_path && -w $rsync_path ]] && echo 'OK' || { mkdir -p $rsync_path; [[ -d $rsync_path ]] && echo 'CREATED' || echo 'FAILED'; }")


        if [[ $rsync_backup_path_status != FAILED ]]; then

            # Notifiy about good status of $rsync_path
            echo "$rsync_server: $rsync_path exists and status is '$rsync_backup_path_status'"

            # Verify # of backup chains to keep:
            if [[ $RSYNC_BACKUP_CHAINS_TO_KEEP =~ [0-9] ]]; then

                # Is an integer number:
                echo "Max backup chains per VM to keep remotely: $RSYNC_BACKUP_CHAINS_TO_KEEP"

            elif [[ -z $RSYNC_BACKUP_CHAINS_TO_KEEP ]]; then

                # Was not set:
                echo "All recoverable backup chains will be archived remotely"

            else
                echo "ERROR: Incorrect value for environment variable RSYNC_BACKUP_CHAINS_TO_KEEP: '$RSYNC_BACKUP_CHAINS_TO_KEEP' it must be a positive integer"
                rsync_backup_path_status="FAILED"
            fi

        else
            echo "ERROR: remote mirror $rsync_path cannot be read and/or written (or is not a directory)"
        fi

    else
        echo "ERROR: Connection with $rsync_server failed with status $rsync_server_status"
        rsync_backup_path_status="FAILED"
    fi
else
    echo "ERROR: Incorrect syntax for RSYNC_BACKUP_PATH: '$RSYNC_BACKUP_PATH' it must be SSH-like absolute path (e.g. user@host:/path-to-mirror)"
    rsync_backup_path_status="FAILED"
fi

# 1.5 TO DO: Check other ENV variables, and SSH key:
#------------------------------------------------------------------------------
#------------------------------------------------------------------------------
# 2. Only when input parameters doesn't require to restart the container, it continues the rest of the checks:
#------------------------------------------------------------------------------

if [[ $domains_list_status == OK && $max_backups_per_chain_status != FAILED && $local_backup_path_status == OK && $rsync_backup_path_status != FAILED ]]; then

    # 2.0 Create logrotate config:
    #------------------------------------------------------------------------------

    if [[ -z $DISABLE_LOGROTATE ]]; then

        echo "Configuring logrotate..."

        # Create directory if doesn't exist:
        mkdir -p $(dirname $LOGROTATE_CONFIG_PATH)

        # Parse logrotate configuration (overwriting, if any previous):
        echo -e "$LOGFILE_PATH {\n$LOGROTATE_SETTINGS\n}" > $LOGROTATE_CONFIG_PATH

    else
        echo "Logrotate disabled by user"
        # Convert variable into a comment, to be added to crontab
        DISABLE_LOGROTATE="# "
    fi

    # 2.1 Create/update Cron task for VMs to be (progressively) included in $scheduled_backups_list:
    #------------------------------------------------------------------------------

    echo "Configuring $(basename $scheduled_backup_script)..."

    # Silently deletes any previous cron task:
    &> /dev/null crontab -r

    # Parses the actual cron task needed to run to $crontab_file
    # (Including ENV vars not being read from cron's environment):
    cat << end_of_crontab > $crontab_file
# Values below are refreshed upon container (re)start:

# Main environment is bash:
SHELL=/bin/bash

# Search paths for binaries and scripts:
PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin

# Environment variables passed through Docker:
LOCAL_BACKUP_PATH="$LOCAL_BACKUP_PATH"
MAX_BACKUPS_PER_CHAIN="$MAX_BACKUPS_PER_CHAIN"
RSYNC_BACKUP_PATH="$RSYNC_BACKUP_PATH"
RSYNC_ARGS="$RSYNC_ARGS"
SSH_OPTIONS="$SSH_OPTIONS"
VIRTNBDBACKUP_ARGS="$VIRTNBDBACKUP_ARGS"
UNRAID_NOTIFY_HOST="$UNRAID_NOTIFY_HOST"

# Paths for functions and (shared) dynamic variables:
functions_path="$functions_path"
external_vars="$external_vars"

# Schedule for $(basename $scheduled_backup_script):
$BACKUP_SCHEDULE $scheduled_backup_script > /proc/1/fd/1 2>&1

# Schedule for logrotate:
$DISABLE_LOGROTATE$LOGROTATE_SCHEDULE /usr/sbin/logrotate $LOGROTATE_CONFIG_PATH > /proc/1/fd/1 2>&1
end_of_crontab

    # Sets the cron task:
    crontab $crontab_file

    # Finally, runs cron and sends to background:
    echo "Starting Cron..."
    cron -f -l -L2 &

    # 2.2 Add all remaining VMs in DOMAINS_LIST to this queue:
    #------------------------------------------------------------------------------

    CHECK_BACKUPS_LIST=(${DOMAINS_LIST[@]})

    # 2.3 Initializes a file with variables externally stored, to be shared with the scheduler:
    #------------------------------------------------------------------------------
    cat << end_of_external_variables > $external_vars
# Shared values between $(basename $0) and $scheduled_backup_script. DO NOT EDIT!:

# Dynamic arrays:
CHECK_BACKUPS_LIST=(${CHECK_BACKUPS_LIST[@]})
FAILED_VMS_LIST=()
SCHEDULED_BACKUPS_LIST=()

# Dynamic strings:
ONGOING_BACKUP="false"
ONGOING_CHECK="false"
RESTARTED_SERVER=$RESTARTED_SERVER
end_of_external_variables

    # 3. Begin monitorization for VMs in lists, performing operations as required:
    #------------------------------------------------------------------------------

    echo ""
    echo "############################################################################################################"
    echo "Monitoring mode started"

    while true; do

        # Maximum standby period for monitoring should not exceed 10 seconds in any case,
        # because it could ignore SIGTERM from Docker, thus being killed with SIGKILL:
        sleep 1

        # 3.1 (Re)reads all external variables (presumably modified by $scheduled_backup_script):
        #------------------------------------------------------------------------------
        source $external_vars

        if [[ $ONGOING_BACKUP == false ]]; then

            if [[ ! -z ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ]]; then

                # 3.2 Check first for VMs which are in need of shutdown.
                # (This normally happens when the user took the action, or when a VM took long time to shutdown):
                #------------------------------------------------------------------------------
                i=0
                for domain in ${SHUTDOWN_REQUIRED_VMS_LIST[@]}; do

                    if [[ $(domain_state $domain) == "shut off" ]]; then

                        # Add VM to the same list of VMs to be powered on when monitor starts:
                        POWEREDOFF_VMS_LIST+=($domain)

                        # Move to main queue for check:
                        CHECK_BACKUPS_LIST+=($domain)
                        unset SHUTDOWN_REQUIRED_VMS_LIST[$i]

                        # Rebuild indexes into the array:
                        SHUTDOWN_REQUIRED_VMS_LIST=(${SHUTDOWN_REQUIRED_VMS_LIST[@]})

                        [[ $(os_is_unraid) == yes ]] && unraid_notify "normal" "VM-Babysitter" "Required shut down detected" "Resuming check/backup for $domain"
                    fi
                done
            fi

            if [[ ! -z ${CHECK_BACKUPS_LIST[@]} || ! -z ${CREATE_BACKUP_CHAIN_LIST[@]} ]]; then

                # 3.3 Fresh start, or status of at least on VM has changed, and sent to one of the queues. Marks an ongoing check starting:
                #------------------------------------------------------------------------------
                ONGOING_CHECK="true"
                sed -i \
                -e "s/ONGOING_CHECK=.*/ONGOING_CHECK=\"$ONGOING_CHECK\"/" $external_vars

                # Create the global list of VMs to be displayed:
                ONGOING_CHECK_LIST=(${CHECK_BACKUPS_LIST[@]} ${CREATE_BACKUP_CHAIN_LIST[@]})

                echo ""
                echo "____________________________________________________________________________________________________________"
                echo "Automatic check for VM(s) ${ONGOING_CHECK_LIST[@]} in progress..."


                # 3.3.1 Check for backup state (and apply retention policy) on all VMs:
                #------------------------------------------------------------------------------
                [[ ! -z ${CHECK_BACKUPS_LIST[@]} ]] && check_backups

                # Both sub-routines invoked above require to shut down VMs in order to proceed
                # They do this automatically if env variable VM_ALLOW_POWERCYCLE is set.
                # After they finish, VMs are started if in the global list below:
                if [[ ! -z ${POWEREDOFF_VMS_LIST[@]} ]]; then

                    # 3.3.2 Turns on all VMs that was previously shutdown for checks:
                    #------------------------------------------------------------------------------
                    echo ""
                    i=0
                    for domain in ${POWEREDOFF_VMS_LIST[@]}; do

                        if [[ $(domain_state $domain) != running ]]; then

                            # Turn on the VM. Do not wait for Guest's QEMU agent:
                            domain_start $domain --nowait
                        fi

                        # Remove the VM from the list is being read:
                        unset POWEREDOFF_VMS_LIST[$i]

                        # Rebuild indexes into the array:
                        POWEREDOFF_VMS_LIST=(${POWEREDOFF_VMS_LIST[@]})

                        # Increases the counter:
                        ((i++))
                    done
                fi

               # 3.3.3 Create new backup chains for VMs that require it:
               #------------------------------------------------------------------------------
               [[ ! -z ${CREATE_BACKUP_CHAIN_LIST[@]} ]] && create_backup_chain

                # 3.4 Show status report and notify about user actions, if required:
                #------------------------------------------------------------------------------
                echo ""
                echo "############################################################################################################"
                echo "All VMs with status changed finished to be processed at $(date "+%Y-%m-%d %H:%M:%S")"
                echo ""
                echo "VM-Babysitter Global Summary:"
                echo ""
                echo "Current Scheduled Backups:    ${SCHEDULED_BACKUPS_LIST[@]:-"None"}"
                echo "Manual Shut Down Required:    ${SHUTDOWN_REQUIRED_VMS_LIST[@]:-"None"}"
                echo "Misbehaving Virtual Machines: ${FAILED_VMS_LIST[@]:-"None"}"

                if [[ ! -z ${SCHEDULED_BACKUPS_LIST[@]} && -z ${SHUTDOWN_REQUIRED_VMS_LIST[@]} && -z ${FAILED_VMS_LIST[@]} ]]; then

                    echo ""
                    echo "All Virtual Machines Running Under Incremental Backups!"

                elif [[ ! -z ${SHUTDOWN_REQUIRED_VMS_LIST[@]} || ! -z ${FAILED_VMS_LIST[@]} ]]; then

                    user_action_message="USER ACTION IS REQUIRED!"

                    echo ""
                    echo $user_action_message

                    if [[ ! -z ${SHUTDOWN_REQUIRED_VMS_LIST[@]} ]]; then

                        shutdown_required_message="As env variable 'VM_ALLOW_POWERCYCLE' is not set, a manual Shut Down is required for VM(s): ${SHUTDOWN_REQUIRED_VMS_LIST[@]} 'As Soon As Possible'. This is required prior to perform backup integrity checks and cannot procced without that. Once shut down, the program will perform the checks and will restart involved VM(s) automatically."

                                                echo "WARNING: $shutdown_required_message"
                        [[ $(os_is_unraid) == yes ]] && unraid_notify "warning" "VM-Babysitter" "$user_action_message" "$shutdown_required_message"
                    fi

                    if [[ ! -z ${FAILED_VMS_LIST[@]} ]]; then

                        failed_vms_message="Unexpected failure during check/backup operation involving VM(s): ${FAILED_VMS_LIST[@]}. Human attention is required 'As Soon As Possible' to solve the situation. Any scheduled backup or further alert for involved VM(s) is suspended until this container had been restarted. Check $(basename $LOGFILE_PATH) in this host for detailed logs."


                        echo  "ERROR: $failed_vms_message"
                        [[ $(os_is_unraid) == yes ]] && unraid_notify "alert" "VM-Babysitter" "$user_action_message" "$failed_vms_message"
                    fi
                fi

                # 3.5 Marks ongoing check as finished and exports variables to be used on scheduled backups (entering in 'silent' mode):
                #------------------------------------------------------------------------------

                ONGOING_CHECK="false"

                # Clean formerly displayed ONGOING_CHECK_LIST:
                ONGOING_CHECK_LIST=()

               # Sed can't expand arrays correctly. Convert variables to be exported into strings.
               # This will be reverted at next block iteration, by sourcing such external variables as arrays:
                SCHEDULED_BACKUPS_LIST="${SCHEDULED_BACKUPS_LIST[@]}"
                FAILED_VMS_LIST="${FAILED_VMS_LIST[@]}"

                # Export the variables (CHECK_BACKUPS_LIST was unset at the end of its run)
                sed -i \
                -e "s/CHECK_BACKUPS_LIST=.*/CHECK_BACKUPS_LIST=()/" \
                -e "s/FAILED_VMS_LIST=.*/FAILED_VMS_LIST=($FAILED_VMS_LIST)/" \
                -e "s/SCHEDULED_BACKUPS_LIST=.*/SCHEDULED_BACKUPS_LIST=($SCHEDULED_BACKUPS_LIST)/" \
                -e "s/ONGOING_CHECK=.*/ONGOING_CHECK=\"$ONGOING_CHECK\"/" \
                $external_vars
            fi
        fi

        # 3.7 Restarts the loop from 3.1, until receives SIGTERM or SIGKILL from Docker
        #------------------------------------------------------------------------------
    done

else
    # Initial checks have proven non-recoverable errors:
    failed_initialize_message="Could not initialize due to errors! Check $(basename $LOGFILE_PATH) in this host for detailed logs."

    echo "ERROR: $failed_initialize_message"
    [[ $(os_is_unraid) == yes ]] && unraid_notify "alert" "VM-Babysitter" "Failed to start container" "$failed_initialize_message"

    stop_container
fi
